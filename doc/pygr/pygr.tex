\documentclass{howto}
\usepackage{distutils}

% TODO:
%   Fill in XXX comments

\title{Pygr: Docs Overview}


\input{boilerplate}

\author{Chris Lee}
\authoraddress{
\strong{UCLA, Department of Chemistry and Biochemistry}\\
	Email: \email{leec@chem.ucla.edu}
}

\makeindex

\begin{document}

\maketitle

\begin{abstract}
  \noindent
   A roadmap to the Pygr documentation.
\end{abstract}

%\begin{abstract}
%\noindent
%Abstract this!
%\end{abstract}


% The ugly "%begin{latexonly}" pseudo-environment supresses the table
% of contents for HTML generation.
%
%begin{latexonly}
\tableofcontents
%end{latexonly}


\section{Introductory Tutorial}
\label{intro}

Pygr is open source software designed to make it easy to do powerful sequence and
comparative genomics analyses, even with extremely large multi-genome alignments.


\subsection{Sequence / Alignment Tutorial}
\label{seq-align}

Sequences and alignments also can be modeled as graph structures in Pygr, providing the same consistent and simple framework for queries.

\subsubsection{Sequence Objects}
Pygr tries to provide a very "Pythonic" model for sequences.  This python interpreter session illustrates some simple features:

\begin{verbatim}
>>> from pygr.sequence import *
>>> s = Sequence('attatatgccactat','bobo') #create a sequence named bobo
>>> s # interpreter will print repr(s)
bobo[0:15]
>>> t = s[-8:] #python slice gives last 8 nt of s
>>> t # interpreter will print repr(t)
bobo[7:15]
>>> str(t) #string conversion just yields the sequence as a string 
'gccactat'
>>> rc = -s #get the reverse complement
>>> str(rc[:5]) #its first five letters
'atagt'
\end{verbatim}

Several points:
\begin{itemize}

\item
   Slices of a sequence object (e.g. s[1:10] or s[-8:]) are themselves sequence objects.

\item    
The string value of a sequence object (e.g. str(s)) is just the sequence itself (as a string).

\end{itemize}

\subsubsection{Comparative Genomics Query of Multigenome Alignments}

Many groups (e.g. David Haussler's group at UC Santa Cruz) have constructed alignments of multiple genomes.  These alignments are extremely useful and interesting, but so large that it is cumbersome to work with the dataset using conventional methods.  For example, for the 17-genome alignment you have to work simultaneously with the individual genome datasets for human, chimp, mouse, rat, dog, chicken, fugu and zebrafish etc., as well as the huge alignment itself.  Pygr makes this quite easy.  Here we illustrate an example of mapping a set of human exons, which has two splice sites
(\code{ss1} and \code{ss2}) bracketing a single exon (\code{exon}).
We use the alignment database to map each of these splice sites onto all the aligned
genomes, and to print the percent-identity and percent-aligned for each genome,
as well as the two nucleotides consituting the splice site itself.
It also prints the conservation of the two exonic region (between \code{ss1}
and \code{ss2}: 

\begin{verbatim}
import pygr.Data # FINDS DATA WHEREVER IT'S REGISTERED
msa = pygr.Data.Bio.MSA.UCSC.hg17_multiz17way() # SANTA CRUZ 17-GENOME ALIGNMENT
exons = pygr.Data.Leelab.ASAP2.hg17.exons() # ASAP2 HUMAN EXONS
idDict = ~(msa.seqDict) # INVERSE: MAPS SEQ --> STRING IDENTIFIER
def printConservation(id,label,site):
    for src,dest,edge in msa[site].edges(mergeMost=True):
        print '%d\t%s\t%s\t%s\t%s\t%s\t%2.1f\t%2.1f' \
              %(id,label,repr(src),src,idDict[dest],dest,
                100*edge.pIdentity(),100*edge.pAligned())
for id,exon in exons.iteritems():
    ival = exon.sequence # GET THE SEQUENCE INTERVAL FOR THIS EXON
    ss1 = ival.before()[-2:] # GET THE 2 NT SPLICE SITES
    ss2 = ival.after()[:2]
    cacheHint = msa[ss1+ss2] #CACHE THE COVERING INTERVALS FROM ss1 TO ss2
    printConservation(id,'ss1',ss1)
    printConservation(id,'ss2',ss2)
    printConservation(id,'exon',ival)
\end{verbatim}

A few notes:

\begin{itemize}

\item
Querying a large multi-genome alignment requires special interval indexing
algorithms (R-Tree or nested-list used in Pygr).  Pygr provides a high-performance
C implementation of a disk-based nested-list database that provides both
very fast interval overlap query times (sub-millisecond per query, compared with
10-30 seconds per query using MySQL multi-column indexing, and much faster
than Postgres R-Tree indexing), and a very small memory footprint
(e.g. 2.5 MB RSS in-memory, 8 MB VSZ virtual size,
for working with the UCSC 17 vertebrate
genome alignment and sequence databases).  For more information on the
nested-list algorithm and performance comparisons, see the published paper,
Alekseyenko and Lee, Bioinformatics 2007.

\item
The alignment database query is in the first line of \code{printConservation()}.
\code{msa} is the database; \code{site} is the interval query; and the
\method{edges} methods iterates over the results, returning a tuple for
each, consisting of a {\em source sequence} interval (i.e. an interval of
\code{site}), a {\em destination sequence} interval (i.e. an interval in
an aligned genome), and an {\em edge object} describing that alignment.
We are taking advantage of Pygr's group-by operator \code{mergeMost},
which will cause multiple intervals in a given sequence to be merged
into a single interval that constitutes their ``union''.  Thus,
for each aligned genome, the \code{edges} iterator will return a single
aligned interval.  The alignment edge object provides some useful 
conveniences, such as calculating the percent-identity between \code{src}
and \code{dest} automatically for you.  \method{pIdentity()} computes
the fraction of identical residues; \method{pAligned} computes the 
fraction of {\em aligned} residues (allowing you to see if there are 
big gaps or insertions in the alignment of this interval).  If we 
had wanted to inspect the detailed alignment letter by letter, we
would just iterate over the \member{letters} attribute instead of
the \method{edges} method. (See the \class{NLMSASlice} documentation for 
further information).

\item
Pygr provides convenient query options for specifying precisely how regions
of alignment should be ``grouped'' together (e.g. treat alignment intervals
separated by indels up to a certain size as being a {\em single} alignment
region) or filtered (e.g. require a certain level of conservation over some
minimum size of alignment region).  Here's an example:
\begin{verbatim}
results = msa[site].edges(maxgap=1,maxinsert=1,
                        minAlignSize=14,pIdentityMin=0.9)
\end{verbatim}
This example groups together any number of alignment intervals separated by indels
of at most one in length, and then filters these alignment regions to
just those (sub)regions that have at least 90\% sequence identity over
a region of at least 14 residues in length.

We can use this same idea to search for regions of ``deep conservation''.  Here
we search the UCSC alignment of 17 vertebrate genomes for regions of 90\% identity
or better that are at least 40 nt long, and then screen for a zone in which at
least nine different genomes all share this level of alignment with the human 
query:
\begin{verbatim}
>>> ival = nlmsa.seqDict['hg17.chr1'][7000:8000] # 1 kb REGION OF HUMAN CHROMOSOME 1
>>> for x,y,e in nlmsa[ival].edges(minAligned=9,minAlignSize=40,pIdentityMin=0.9):
...   print "%s\t%s\n%s\t%s\n" % (x,repr(x),y,(~(nlmsa.seqDict))[y])
...
GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAACAGCAGTAAAGAGCTGAC      danRer3.chr18

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAAGGAGCTGAC      dasNov1.scaffold_107966

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAAAGGAGCAAGGAGCTGAC      xenTro1.scaffold_1073

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAGGGAGCTGAG      galGal2.chr1

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAAGGAGCTGAC      panTro1.chr1

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAGGGAGCTGAC      bosTau2.chr5

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAATAGCAGCAACGAGCTGAC      canFam2.chr27

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAAGGAGCTGAG      monDom2.scaffold_31

GTGTTGAAGAGCAGCAAGGAGCTGAC      chr1[7480:7506]
GTGTTGAAGAGCAGCAAGGAGCTGAC      loxAfr1.scaffold_5603
\end{verbatim}
Each region of alignment was contained in a block of >=90\% identity and
over 40 nt long.  The region has been masked by the minAligned option to just
the portion in which at least nine different genomes are aligned to the human query.

\item
\code{src} and \code{dest} print the first two nucleotides
of the site in human and in the aligned genome.

\item
it's worth noting that the actual sequence string comparisons are being
done using a completely different database mechanism (either Pygr's
simple \code{pureseq} text format, or (before release 0.5) NCBI's \code{fastacmd}),
not the \code{cnestedlist} database.  Basically, each genome is being queried
as a separate sequence database, represented in Pygr by the
\class{BlastDB} class.  Pygr makes this complex set of multi-database
operations more or less transparent to the user.
For further information, see the \class{BlastDB} documentation.  

\item ASAP2.hg17.exons is an annotation database; each object it
contains (\code{exon}) is an annotation object.  To get the actual
sequence interval corresponding to this annotation, we simply request
the annotation object's \member{sequence} attribute.

\item
Note: \code{exon.sequence} must itself be a slice of a sequence in our alignment, 
or the alignment query \code{msa[site]} will raise an KeyError informing
the user that the sequence \code{site} is not in the alignment.

\item
One interesting operation here is the use of interval
addition to obtain the ``union'' of two intervals, e.g. \code{ss1+ss2}.
This obtains a single interval that covers both of the input intervals.

\item
When the print  statement requests str() representations of these sequence objects, Pygr uses fseek() to extract just the right piece of the corresponding chromosomes from the 17 BLAST databases representing all the different genomes.

\item
Given the high speed of the NLMSA alignment query, it turns out that the
operation of reading sequence strings from the sequence databases (in this
case, for printing them in printConservation() and calculating the percent identity
in pIdentity()) is the rate-limiting step for this analysis.  I.e. this analysis
spends far more time waiting for disk I/O to read a particular piece of sequence 
than it does running the NLMSA alignment queries.  To solve this problem, Pygr
provides a mechanism for intelligent caching of sequence data.  Whenever you
perform a query (e.g. \code{msa[site]}), it infers that you are likely to look
at the sequence intervals that are contained within this slice of the alignment
(i.e. within the region aligned to \code{site}).  It sets ``caching hints'' on the
associated sequence databases, recording for each aligned sequence
the covering interval coordinates (i.e. the smallest interval that fully contains
all portions of the sequence that are aligned to \code{site}).  These caching hints
do not themselves trigger reading of sequence string data from the databases.  Only
when user code actually requests sequence strings that fall within these covering
intervals, the sequence database object will load not the requested interval, but
the entire covering interval, which is then cached.  Thereafter, all sequence
string requests that fall within the covering interval are simply immediately sliced
from the cached sequence string, completely avoiding any need to read from disk.
This greatly accelerates sequence analysis with very large multigenome alignments
and sequence databases.  

In this case, to enforce the most efficient caching possible, we simply performed
a query that contains all three sites of interest (ss1, ss2, and exon).  By performing
this query first, and holding onto the query result, we ensure that Pygr will
use the same cache for all three subsequent queries contained in it.  As soon
as we release the reference to this query result (i.e. in the example above,
whenever the variable \code{cacheHint} is deleted or over-written with a new value,
freeing Python to garbage-collect the original query result), the associated 
cache hint information will also be cleared.

\end{itemize}

(Actually, because of Pygr's caching / optimizations, considerably more is going on than indicated in this simplified sketch.  But you get the idea: Pygr makes it relatively effortless to work with a variety of disparate (and large) resources in an integrated way.)

Here is some example output:
\begin{verbatim}
NEED TO UPDATE THESE RESULTS
1       Mm.99996        ss1     hg17    50.0    100.0   AG      GG
1       Mm.99996        ss1     canFam1 50.0    100.0   AG      GG
1       Mm.99996        ss1     panTro1 50.0    100.0   AG      GG
1       Mm.99996        ss1     rn3     100.0   100.0   AG      AG
1       Mm.99996        ss2     hg17    100.0   100.0   AG      AG
1       Mm.99996        ss2     canFam1 100.0   100.0   AG      AG
1       Mm.99996        ss2     panTro1 100.0   100.0   AG      AG
1       Mm.99996        ss2     rn3     100.0   100.0   AG      AG
1       Mm.99996        ss3     hg17    100.0   100.0   GT      GT
1       Mm.99996        ss3     canFam1 100.0   100.0   GT      GT
1       Mm.99996        ss3     panTro1 100.0   100.0   GT      GT
1       Mm.99996        ss3     rn3     100.0   100.0   GT      GT
1       Mm.99996        e1      hg17    78.9    100.0   AG      GG
1       Mm.99996        e1      canFam1 84.2    100.0   AG      GG
1       Mm.99996        e1      panTro1 77.6    100.0   AG      GG
1       Mm.99996        e1      rn3     97.4    98.7    AG      AG
1       Mm.99996        e2      hg17    91.6    99.1    CC      CC
1       Mm.99996        e2      canFam1 88.8    99.1    CC      CC
1       Mm.99996        e2      panTro1 91.6    99.1    CC      CC
1       Mm.99996        e2      rn3     97.2    100.0   CC      CC
\end{verbatim}

\subsubsection{Working with Sequences from Databases}

Pygr provides a variety of "back-end" implementations of sequence objects, ranging from sequences stored in a relational database table, or a BLAST database, to sequences created by the user in Python (as above).  All of these provide the same consistent interface, and in general try to be efficient.  For example, Pygr sequence objects are just "placeholders" that record what sequence interval you're working with, but if the back-end is an external database, the sequence object itself does not store the sequence, and creating new sequence objects (e.g. taking slices of the object as above) will not require anything to be done on the actual sequence itself (such as copying a portion of it).  Pygr only obtains sequence information when you actually ask for it (e.g. by taking the string value str(s) of a sequence object), and normally only obtains just the portion that you ask for (i.e. str(s[1000000:1000100]) only obtains 100nt of sequence, even if s is a 100 megabase sequence.  By contrast str(s)[1000000:1000100] would force it to obtain the whole sequence from the database, then slice out just the 100 nt you selected). 

Here's an example of working with sequences from a BLAST database:

\begin{verbatim}
NEED TO UPDATE THESE RESULTS
>>> from pygr.seqdb import *
>>> db = BlastDB('sp') # open BLAST database associated with FASTA file 'sp'
>>> s = db['CYGB_HUMAN'][90:150] # get a sequence by ID, and take a slice
>>> str(s)
'TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA'
>>> m = db.blast(s) # get alignment to all BLAST hits in db
>>> for src,dest,edge in m[s].edges(): # print out the alignment edges
...     print src,repr(src),'\n',dest,repr(dest),edge.pIdentity(),'\n'
... 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPP CYGB_HUMAN[90:145] 
TLVENLRDADKLNTIFNQMGKSHALRHKVDPVYFKILAGVILEVLVEAFPQCFSP CYGB_BRARE[87:142] 72

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150]
 120

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPMYFKILSGVILEVIAEEFANDFPVETQKA CYGB_MOUSE[90:150]
112 
...
\end{verbatim}

This example introduces the use of a Pygr alignment object to store the mapping of s onto homologous sequences in db, obtained from BLAST.  Here's what Pygr actually does:

\begin{itemize}

\item
We can construct a BlastDB object from any FASTA formatted sequence file.  A BlastDB object is just a placeholder that acts as a convenient interface to the BLAST database.  For example, it acts as a Python dictionary mapping sequence IDs to the associated sequence objects (i.e. if 'CYGB_HUMAN' is a sequence ID in sp, then db['CYGB_HUMAN'] is the sequence object for that sequence.  This object is instantiated from class BlastSequence, which provides an interface to the BLAST database.

\item 
When you work with such sequence objects, slicing etc. happens in the usual way, creating new sequence objects.

\item  
Only when you ask for actual sequence (by taking str(s)) does it obtain a sequence string from the database.  This is done using fseek() system call to obtain just the selected slice.  So you can efficiently obtain a substring of a sequence, even if that sequence is an entire chromosome.

\item
When you first create the BlastDB object, it looks for existing BLAST database files associated with the FASTA file 'sp'.  If present, it uses them.  If not, it will create them automatically if the user actually tries to run a BLAST query.  Pygr builds BLAST database files using the NCBI program formatdb (Pygr figures out whether the sequences are nucleotide or protein, and gives formatdb the appropriate command line options).  

\item 
When you invoke the db.blast() method on a sequence object, it obtains the actual string of the object, and uses it to run a BLAST search.  It determines the type (nucleotide or protein) of the sequence object, and uses the appropriate search method (in this case blastp).  You can pass optional arguments for controlling BLAST.  It then reads the results into a Pygr multiple sequence alignment object, which stores the alignments as sets of matched intervals.  Specifically, it is a graph, whose nodes are sequence intervals (i.e. sequence objects that typically represent only part of a sequence), and whose edges represent an alignment between a pair of intervals.  To illustrate this, we ran a for-loop over all the "edge relations" in this graph, and printed them out.  This is a tuple of 3 values: \code{src} and \code{dest} are the two aligned sequence intervals, and \code{edge} provides a convenient interface to information about their relationship (e.g. \%identity, etc.).  

\item
Note: print converts its arguments to strings (i.e. calls str() on them), so we used \code{repr(src)} to get a "string representation" of each sequence interval.  When print calls str() on individualBlastSequence interval objects returned by the BLAST search, they invoke fseek() to obtain the specific sequence slice representing that interval.
\end{itemize}

\subsection{Simplifying the Challenges of Working with Complex Datasets}
\subsubsection{pygr.Data: a Namespace for Transparently Importing Data}
One challenge in bioinformatics is the complexity of managing many diverse
data resources.  For example, running a large job on a heterogeneous cluster
of computers is complicated by the fact that individual computers often can't
access a given data resource in the same way (i.e. the file path may be different),
and some machines may not have direct access at all to certain resources.

Pygr provides a systematic solution to this problem: creating a consistent
namespace for data.  A given resource is given a unique name that then becomes
its universal handle for accessing it, no matter where you are (just as Python's
\code{import} command provides a consistent name for accessing a given code
resource, regardless of where you are).  For example, say we want to add the
hg17 (release 17 of the human genome sequence) as ``Bio.Seq.Genome.HUMAN.hg17''
(the choice of name is arbitrary, but it's best to choose a good convention and follow
it consistently):

\begin{verbatim}
from pygr import seqdb
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
hg17 = seqdb.BlastDB('hg17')
hg17.__doc__ = 'human genome sequence draft 17' # REQUIRED!
pygr.Data.Bio.Seq.Genome.HUMAN.hg17 = hg17 # SAVE AS THIS NAME
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
Note that you {\em must} call the function \code{pygr.Data.save()} to 
complete the transaction and save all pending data resources
(i.e. all those added since your last \code{pygr.Data.save()} or
\code{pygr.Data.rollback()}).  In particular, if you have added
data to pygr.Data during a given Python interpreter session, you
should always call \code{pygr.Data.save()} or
\code{pygr.Data.rollback()} prior to exiting from that session.

In any subsequent Python session, we can now access it directly by its
pygr.Data name:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
hg17=pygr.Data.Bio.Seq.Genome.HUMAN.hg17() # FIND THE RESOURCE
\end{verbatim}
The call syntax (\code{hg17()}) emphasizes that this acts like a Python
constructor: it constructs a Python object for us (in this case, the
desired seqdb.BlastDB object representing this genome database).
Note that we did {\em not} even have to know how to construct the hg17 
object, e.g. what Python class to use (seqdb.BlastDB), or even to import
the necessary modules for constructing it.  \code{pygr.Data} uses the
power of Python pickling to figure out automatically what to import.
pygr.Data looks at the environment variable PYGRDATAPATH to get a list 
of local and remote resource databases in which to look up any resource name
that you try to load.  For example, in the shell you might set:
\begin{verbatim}
setenv PYGRDATAPATH ~,.,/usr/local/pygr,mysql:PYGRDATA.index,http://leelab.mbi.ucla.edu:5000
\end{verbatim}
This is a comma-separated string (since colon ':' appears inside URLs).
In this case it tells pygr.Data to look for resource databases (in order):
\code{\$HOME/.pygr_data}; \code{./.pygr_data}; \code{/usr/local/pygr/.pygr_data};
the MySQL table PYGRDATA.index (using your
MySQL .my.cnf file to determine the MySQL host and authentication);
and the XMLRPC server running on leelab.mbi.ucla.edu on port 5000.

pygr.Data is smart about figuring out data resource dependencies.
For example, you could just save a 17-genome alignment in a single step
as follows:
\begin{verbatim}
from pygr import cnestedlist
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
nlmsa = cnestedlist.NLMSA('/loaner/ucsc17')
nlmsa.__doc__ = 'UCSC 17way multiz alignment, rooted on hg17'
pygr.Data.Bio.MSA.UCSC.hg17_multiz17way = nlmsa
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
This works, even though using this 17-genome alignment (behind the
scenes) involves accessing 17 BlastDB sequence databases (one for each
of the genomes in the alignment).  Because the alignment object (NLMSA)
references the 17 BlastDB databases, pygr.Data automatically saves information
about how to access them too.

However, it would be a lot smarter to give those databases pygr.Data resource
names too.  Let's do that:
\begin{verbatim}
from pygr import cnestedlist
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
nlmsa = cnestedlist.NLMSA('/loaner/ucsc17')
for id,genome in nlmsa.seqDict.prefixDict.items(): # 1st SAVE THE GENOMES
    genome.__doc__ = 'genome sequence '+id
    pygr.Data.getResource.addResource('Bio.Seq.Genome.'+id,genome)
nlmsa.__doc__ = 'UCSC 17way multiz alignment, rooted on hg17'
pygr.Data.MSA.Bio.UCSC.hg17_multiz17way = nlmsa # NOW SAVE THE ALIGNMENT
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}

This has several advantages.  First, we can now access other genome databases
using pygr.Data too:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
mm7 = pygr.Data.Bio.Seq.Genome.mm7() # GET THE MOUSE GENOME
\end{verbatim}
But more importantly, when we try to load the ucsc17 alignment on
another machine, if the genome databases are not in the same directory
as on our original machine, the first method above would fail, whereas in
the second approach pygr.Data now will automatically scan all its resource databases to
figure out how to load each of the genomes on that machine.

NOTE: Python pickling is not secure.  In particular, you should not unpickle
data provided by someone else unless you trust the data not to contain 
attempted security exploits.  Because Python unpickling has access to \code{import},
it has the potential to access system calls and execute malicious code on your
computer.

\subsubsection{pygr.Data.schema: a Simple Framework For Managing Database Schemas}
{\em Schema} refers to any relationship between two or more collections of
data.  It captures the structure of relationships that define these particular
kinds of data.  For example ``a genome has genes, and genes have exons'', or 
``an exon is connected to another exon by a splice''.  In pygr.Data we can
store such schema information as easily as:
\begin{verbatim}
splicegraph.__doc__ = 'graph of exon:splice:exon relations in human genes'
pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph = splicegraph # ADD A NEW RESOURCE
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.splicegraph = \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
This example assumes that 
\begin{itemize}
\item \code{splicegraph} is a graph whose nodes are exons, and whose
edges are splices connecting a pair of exons.  Specifically,
\code{splicegraph[exon1][exon2]=splice1} means \code{splice1} is a 
splice object (from the container \code{splices}) that connects
\code{exon1} and \code{exon2} (both from the container \code{exons}).

\item An exon can have one or more ``outgoing'' splices connecting it
to subsequent exons, as well as one or more ``incoming'' splices from
previous exons.  Thus this relation of exon to exon is a Many-to-Many
mapping (e.g. as distinguished from a One-to-One mapping, where each
exon must have exactly one such relationship with another exon).

\item Because pygr.Data now knows the schema for splicegraph, it
will automatically reconstruct these relationships for any user who
accesses these data from pygr.Data.  Specifically, if a user
retrieves \code{pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph},
the \code{sourceDB}, \code{targetDB}, \code{edgeDB} attributes on
the returned object will automatically be set to point to the 
corresponding pygr.Data resources representing \code{exons} and \code{splices}
respectively.  \code{splicegraph} does not need to do anything to 
remember these relationships; pygr.Data.schema remembers and applies
this information for you automatically.  Note that when you access
\code{splicegraph}, neither \code{exons} nor \code{splices} will be 
actually loaded unless you do something that specifically tries to
read these data (e.g. \code{for exon in splicegraph} will read
\code{exons} but not \code{splices}).

\item The easiest way for users to work with a schema is to translate
it into object-oriented behavior.  I.e. instead of remembering that
when we have \code{exons} we can use \code{splicegraph} to find its
\code{splices} via code like 
\begin{verbatim}
for exon,splice in splicegraph[exon0].items():
   do something...
\end{verbatim}
most people would find it easier to remember that every \code{exon}
has a \code{next} attribute that gives its splices to subsequent exons
via code like
\begin{verbatim}
for exon,splice in exon0.next.items():
   do something...
\end{verbatim}
Based on the schema statement we gave it,
pygr.Data.schema will automatically create the attributes \code{next},
\code{previous} on any exon item from the container \code{exons},
according to the schema.  I.e. \code{exon.next} will be equivalent to
\code{splicegraph[exon]}.  Note that as long as the object \code{exon0}
came from the pygr.Data resource, the user {\em would not have to do anything}
to be able to use the \code{next} attribute.  On the basis of the saved
schema information, pygr.Data will construct this attribute automatically,
and will automatically load the resources \code{splicegraph} and \code{splices}
if the user tries to actually use the \code{next} attribute.
\end{itemize}

\subsubsection{pygr.Data Sharing Over a Network via XMLRPC}
Sometimes individual compute nodes may not have sufficient disk space to
store all the data resources (for example, just the single UCSC hg17 17-genome alignment and 
associated genome databases takes about 200 GB).  Yet it would be useful
to run compute-intensive analyses on those machines accessing such data.
pygr.Data makes that easy.  The default setting of PYGRDATAPATH (if you
do not set it yourself) is 
\begin{verbatim}~,.,http://biodb2.bioinformatics.ucla.edu:5000\end{verbatim}
respectively your HOME directory, current directory, and the XMLRPC
server provided at UCLA as a service to pygr users.  Thus you can
simply import pygr.Data and start accessing data.  Try this:
\begin{verbatim}
>>> import pygr.Data
>>> pygr.Data.dir('Bio')
['Bio.MSA.UCSC.canFam2_multiz4way',...]
>>> msa = pygr.Data.Bio.MSA.UCSC.hg17_multiz17way()
>>> chr1 = msa.seqDict['hg17.chr1']
>>> ival = chr1[4000:4400]
>>> myslice = msa[ival]
>>> for s1,s2,e in myslice.edges():
...     print '%s\n%s\n' %(s1,s2)
...
AAGGGCCA
AAGGGCCA
\end{verbatim}
This provides a convenient way to begin trying out pygr and working
with comparative genomics data, but clearly is not efficient for analysis
of large amounts of data, which must be transmitted to you by the server
via XMLRPC, since potentially many users must share the access to the 
biodb2.bioinformatics.ucla.edu server.

To setup your own XMLRPC client-server using pygr.Data,
first create an XMLRPC server on a machine that
has access to the data:

\begin{verbatim}
import pygr.Data
nlmsa = pygr.Data.Bio.MSA.UCSC.hg17_multiz17way() # GET OUR NLMSA AND SEQ DBs
server = pygr.Data.getResource.newServer('nlmsa_server') # SERVE ALL LOADED DATA
server.register() # TELL PYGRDATA INDEX SERVER WHAT RESOURCES WE'RE SERVING
server.serve_forever() # START THE SERVICE...
\end{verbatim}

This example code looks for a pygr.Data XMLRPC server in your PYGRDATAPATH,
and registers our resources to that index.  Now any machine that can access
your servers can access the alignment as easily as:
\begin{verbatim}
import pygr.Data
nlmsa = pygr.Data.Bio.MSA.UCSC.hg17_multiz17way() # GET THE NLMSA AND SEQ DBs
\end{verbatim}
Alignment queries and sequence strings will be obtained via XMLRPC 
queries over the network.  Note that if any of the sequence databases
{\em are} available locally (on this machine), Pygr will automatically use that 
in preference to obtaining it over the network (based on your PYGRDATAPATH
settings).  However, if a particular resource is not available locally,
Pygr will transparently get access to it from the server we created,
using XMLRPC.

\subsubsection{download=True Mode}
Pygr.Data provides powerful automation for allowing you to have
both the convenience of obtaining resources automatically from
remote servers, but also the performance of local resources
stored on your computer(s).  If you specify the optional
\var{download=True} argument, pygr.Data will try to find a 
server that will allow download of the entire dataset, and
will then download and initialize the resource for you --
completely automatically.
\begin{verbatim}
nlmsa = pygr.Data.Bio.MSA.UCSC.dm2_multiz9way(download=True)
\end{verbatim}
The location in which downloads and constructed index files
will be stored is controlled by environment variables
PYGRDATADOWNLOAD and PYGRDATABUILDDIR.  If these variables are
not set, data files are simply stored in current directory.

If the resource you requested with download=True has resource
dependencies, they will also be downloaded and built automatically,
if you do not already have a local copy of a given resource.  In general,
if you place your local resource databases before remote resource
servers in your PYGRDATAPATH, download=True will always default to
any local resource that you already have, rather than downloading
a new copy of it.

\subsubsection{pygr.Data Layers}
Based on your PYGRDATAPATH, pygr.Data provides a number of named {\em layers}
that give abstract names for where you want to read or store your pygr.Data info.
For example, if you wanted to store a resource specifically in the resource
database in your current directory, you could type:
\begin{verbatim}
pygr.Data.here.Bio.MSA.UCSC.hg17_multiz17way = nlmsa # SAVE THE NLMSA AND SEQ DBs
\end{verbatim}
\begin{itemize}
\item The abstract pygr.Data layer \code{here} refers to the first entry in your
PYGRDATAPATH that starts with ``.'' (dot).  For other layer names, see
the reference documentation.  This might be useful for prototyping or
testing a new resource, without yet adding it to your long-term resource
database.

\item Similarly, the pygr.Data layer
\code{my} is the first entry that begins with your home directory
(i.e. ~ (tilde), ``/home/yourname'' or whatever your home directory is).

\item the pygr.Data layer \code{system} is the first entry that
begins with an absolute path and is not within your home directory.

\item the pygr.Data layer \code{subdir} is the first entry that
begins with a relative path (ie. does not fit any of the preceding
definitions).

\item Every pygr.Data resource database server (XMLRPC or MySQL) has
a ``layer name'' that will be automatically loaded to your pygr.Data module
when you import it.  For example, to delete this particular resource rule
from our lab's central resource database (called ``leelab'', because it is
not accessible outside our lab):
\begin{verbatim}
del pygr.Data.leelab.Bio.MSA.UCSC.hg17_multiz17way # DELETE THIS RESOURCE RULE
\end{verbatim}
\end{itemize}


\subsubsection{Collection, Mapping, Graph, SQLTable and SQLGraph classes}
One of the main challenges in persistent storage (e.g. keeping a database
on disk) of Python objects is how to store their inter-relations
in an efficient and transparent way.  For example, in a database
application we want to be able to load just one object at a time
(rather than being forced to load all the objects from the database into memory)
even though each object may have references to many other objects
(and we obviously want these references to work transparently for the
user).  The standard database answer is to associate a unique identifier
(e.g. an integer) with each object in a specific collection, and
to store references in the database in terms of these identifiers.
This gives the database a flexible way to refer to objects (by their unique
identifiers) that we have not yet actually loaded into memory.  

pygr.Data provides classes that make it very easy for you to store your
data in this way.  
\begin{itemize}
\item Its \class{Collection} class acts like a dictionary
whose keys are the unique identifiers you've chosen for your objects,
and whose values are the associated objects.  This provides the essential
association between Python objects and unique identifiers that allows
us to store inter-relationships persistently in a database by simply
storing them in terms of their unique identifiers.

\item The \class{Mapping} class
acts like a dictionary that maps objects of a given collection to
arbitrary objects of a second collection.  However, because internally
it stores only identifiers, the \class{Mapping} class can be stored
persistently, for example to a disk database.  

\item Indeed, you can make both of
these classes be stored as a database on disk, simply by passing a \var{filename}
argument that specifies the file in which the database should be stored.
If you do not provide a \var{filename}, a normal (in-memory) Python dictionary
is used.  

\item Alternatively you can use the \class{SQLTable} classes that
provide a dict-like interface to data from an SQL database server
such as MySQL, that is analogous to the \class{Collection} class.

\item The \class{Graph} class provides a general extension of the
mapping concept to represent a {\em graph of nodes connected by edges}.
The most obvious difference vs. the \class{Mapping} class is that 
the \class{Graph} class associates an {\em edge} object with each
node-to-node mapping relationship, which is highly useful for many
bioinformatics problems.  To see example uses of pygr graphs, see
section 1.5 below.  Like \class{Mapping}, \class{Graph} can store its graph data
in memory in a Python dict, or on disk using a BerkeleyDB file.

\item Alternatively you can use the \class{SQLGraph} classes that
provide an interface to store graph data in an SQL database server
such as MySQL, that provides an SQL database version of the functionality
provided by the \class{Graph} or \class{Mapping} classes.

\item All of these classes can be saved as resources in pygr.Data, making
it very easy for you to capture entire datasets of complex bioinformatics
data in pygr.Data.

\item It's important to distinguish that these classes divide into
{\em primary data} (e.g. \class{Collection}, \class{SQLTable}), versus
{\em relations} between data (e.g. \class{Mapping}, \class{Graph},
\class{SQLGraph}).  The latter should be given pygr.Data.schema information,
so that pygr.Data can automatically construct the appropriate data inter-relations
for any user of these data.

\end{itemize}
Here's a simple example of using a pygr \class{Collection}:
\begin{verbatim}
ens_genes = Collection(filename='genes.db',mode='c' # CREATE NEW DATABASE
                       itemClass=Transcript)
for gene_id,gene_data in geneList:
    gene = Transcript(gene_id,gene_data,ens_genes)
    ens_genes[gene_id] = gene # STORE IN OUR DATABASE
\end{verbatim}

\class{Mapping} enables you to store a relationship between one collection
and another collection in a way that is easily stored as a database.  For
example, assuming that \var{ens_genes} is a collection of genes,
and \var{exon_db} is a collection of exons, we can store the mapping from
a gene to its exons as follows:

\begin{verbatim}
gene_exons = Mapping(ens_genes, exon_db, multiValue=True, 
                     inverseAttr='gene_id',filename='gene_exons.db',mode='c')
for exon in exon_db:
    gene = ens_genes[exon.gene_id] # GET ITS GENE
    exons = gene_exons.get(gene, []) # GET ITS LIST OF EXONS, OR AN EMPTY LIST
    exons.append(exon) # ADD OUR EXON TO ITS LIST
    gene_exons[gene] = exons # SAVE EXPANDED EXON MAPPING LIST
\end{verbatim}
The optional \var{multiValue} flag indicates that this is a one-to-many
mapping (i.e. each gene maps to a {\em list} of exons.  Again, we used the 
\var{filename} variable to make pygr store our mapping on disk using a Python
\module{shelve} (BerkeleyDB file).

The \class{Collection}, \class{Mapping} and \class{Graph} classes provide
general and flexible storage options for storing data and graphs.  These classes
can be accessed from the \module{pygr.Data} or \module{pygr.mapping} modules.
For further details, see the \module{pygr.mapping} module documentation.
The \class{SQLTable} and \class{SQLGraph} classes in the \module{pygr.sqlgraph}
module provide analogous interfaces for storing data and graphs in an SQL
database server (such as MySQL).

Here's an example of creating an \class{SQLGraph} representing
the splices connecting pairs of exons, using data stored in an
existing database table:
\begin{verbatim}
splicegraph = sqlgraph.SQLGraphClustered('PYGRDB_JAN06.splicegraph_hg17',
                                         source_id='left_exon_form_id',
                                         target_id='right_exon_form_id',edge_id='splice_id',
                                         sourceDB=exons,targetDB=exons,edgeDB=splices,
                                         clusterKey='cluster_id')
pygr.Data.Bio.ASAP2.hg17.splicegraph = splicegraph
pygr.Data.schema.Bio.ASAP2.hg17.splicegraph = \
    pygr.Data.ManyToManyRelation(exons,exons,splices,
                                 bindAttrs=('next','previous','exons'))
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
This variant of \class{SQLGraph} is optimized for typical usage patterns,
by loading data in clusters (rather than each individual splice one by one).
Since the key that we provided for the clustering ('cluster_id') is the 
gene identifier, this means that looking at any splice will have the effect
of loading all splices for that gene.  This makes sense, because only exons
that are in the same gene can have splices to each other.  This makes
communication with the SQL server efficient, but only loads data that
is likely to be used next by the user.

\subsection{Storing Alignments}
\subsubsection{Alignment Basics}

Pygr multiple alignment objects can be treated as mappings of sequence intervals onto sequence intervals.  Here is a very simple example, showing basic operations for constructing an alignment:

\begin{verbatim}
>>> from pygr import cnestedlist
>>> m2 = cnestedlist.NLMSA('myo',mode='memory') # an empty alignment, in-memory
>>> m2 += s # add sequence s to the alignment
>>> ival = s[100:160] # AN INTERVAL OF s
>>> m2[ival] += db['MYG_CHICK'][83:143] # add an edge mapping interval s -> an interval of MYG_CHICK
>>> m2[ival] += db['MYG_CANFA'][45:105] # add an additional edge
>>> m2.build() # done constructing the alignment.  Initialize for query.
>>> for s2 in m2[ival[:10]]: # get aligned seqs for the first 10 letters of ival...
...     print repr(s2)
...
MYG_CHICK[83:93]
MYG_CANFA[45:55]
\end{verbatim}
In this case we used in-memory storage of the alignment.  

\subsubsection{Storing an All-vs-All BLAST Alignment}
However, for really large
alignments (e.g. an all vs. all BLAST analysis) we may prefer to store the alignment
on-disk.  In pygr, all we have to do is change the mode flag to 'w' (implying {\em write}
a file):
\begin{verbatim}
from pygr import cnestedlist,seqdb
msa = cnestedlist.NLMSA('all_vs_all',mode='w',bidirectional=False) # ON-DISK
sp = seqdb.BlastDB('sp') # OPEN SWISSPROT DATABASE
for id,s in sp.iteritems(): # FOR EVERY SEQUENCE IN SWISSPROT
    sp.blast(s,msa,expmax=1e-10) # GET STRONG HOMOLOGS, SAVE ALIGNMENT IN msa
msa.build(saveSeqDict=True) # BUILD & SAVE ALIGNMENT + SEQUENCE INDEXES
# msa READY TO QUERY NOW...
\end{verbatim}
Again you can see how pygr makes it quite simple to do a large analysis
and create a powerful resource (an all-vs-all alignment database).
A couple of points deserve comment:

\begin{itemize}

\item
The in-memory and on-disk NLMSA alignment storages have exactly the same
interface.  You can work with alignments from small to large to gimungous
using the same consistent set of tools.  Moreover the performance will be
fast across the whole range of scales, because the NLMSA storage and query
algorithms scale very well (O(logN)).

\item
Because of the \code{mode='w'} flag, NLMSA will create a set of alignment
index files called 'all_vs_all'.

\item 
\code{bidirectional=False}: whenever you store an alignment relationship
$S \rightarrow T$, this can either be {\em unidirectional} or {\em bidirectional}.
Unidirectional means only $S \rightarrow T$ is stored; bidirectional means
both $S \rightarrow T$ and $T \rightarrow S$ are stored (i.e. you can both query
with S (and get T), and query with T (and get S).  In general, you want 
a unidirectional alignment storage when {\em directionality matters}.  For
example, in a BLAST all vs. all search the alignment of S and T that you get
when you blast S against the database (finding T, among others) may well be
different from the alignment of S and T that you get when you blast T against
the database (finding S, among others).  If you stored the all-vs-all alignment
using bidirectional storage, querying \code{msa} with S would get TWO alignments 
to T: one from the $S \rightarrow T$ BLAST search results, and one from the
$T \rightarrow S$ BLAST search results.  This simply reflects the fact that 
the all vs all BLAST stored two alignments of S and T into \code{msa}.
What this highlights is that BLAST is not a true multiple sequence alignment
algorithm (among other things, it is not symmetric: you can get different
mappings in one direction vs. the other).  

In general, bidirectional storage
mainly makes sense for true multiple sequence alignments (which are guaranteed
to be symmetric).

\item
Supplying the BlastDB.blast() method with an alignment object makes it store
its results into that alignment, rather than creating its own alignment holder
for us.  In this way we can make it store many different BLAST searches into
a single alignment database.

\item
To make the NLMSA algorithm scalable, pygr defers construction of the alignment
indexes until the alignment is complete.  We trigger this by calling its build()
method.  At this point we now have an alignment database stored on disk, which
we can open at any time later and query with the high-speed nested list algorithm
as illustrated in the examples in previous sections.

\end{itemize}


\subsubsection{Building an Alignment Database from MAF files}
It may be helpful to see how a large multi-genome alignment database
is created in Pygr.  It is quite straightforward.
UCSC has defined a new file format for large multigenome alignment,
called MAF.  Pygr provides high-performance utilities for reading
MAF alignment files and building a disk-based NLMSA alignment database.
(These utilities are written in C for performance).  Here's an
example of building an alignment database from scratch using a
set of MAF files stored in a directory called \code{maf/}:

\begin{verbatim}
import os
from pygr import cnestedlist,seqdb

genomes = {'hg17':'hg17','mm5':'mm5', 'rn3':'rn3', 'canFam1':'cf1', 
           'danRer1':'dr1', 'fr1':'fr1','galGal2':'gg2', 'panTro1':'pt1'}
for k,v in genomes.items(): # PREFIX DICTIONARY FOR UNION OF GENOMES
    genomes[k] = seqdb.BlastDB(v) # USE v AS FILENAME FOR FASTA FILE
genomeUnion=seqdb.PrefixUnionDict(genomes) # CREATE UNION OF THESE DBs
# CREATE NLMSA DATABASE ucsc8 ON DISK, FROM MAF FILES IN maf/
msa = cnestedlist.NLMSA('ucsc8','w',genomeUnion,os.listdir('maf'))
msa.build(saveSeqDict=True) # BUILD & SAVE ALIGNMENT + SEQUENCE INDEXES
\end{verbatim}

The only real work here is due to the fact that UCSC's MAF files
use a {\em prefix.suffix} notation for identifying specific sequences,
where {\em prefix} gives the name of the genome, and {\em suffix}
gives the identifier of the sequence in that genome database.
Here we use Pygr's \class{PrefixUnionDict} class to wrap the 
set of genome databases in a dict-like interface that accepts
string keys of the form {\em prefix.suffix} and returns the
right sequence object from the right genome database.  As an
added twist, the genome names in the MAF files match the
filenames of the associated genome databases in most cases, but
not all, so we have to create an initial dictionary giving the
correct mapping.  Actually building the NLMSA requires just one
line, but actually a number of steps are happening behind the
scenes:
\begin{itemize}
\item If you have never opened \class{BlastDB} objects for these genome
databases before, \class{BlastDB} will initialize each one.  This means
two things.  First, it builds an index of all the sequences and their 
lengths.  This is essential for combining the
large numbers of sequences in these databases into 
``unified'' coordinate systems in the NLMSA (otherwise there would
have to be a separate database file for each individual sequence).
Second, it saves the sequences to a simple indexed file format that
allows Pygr to retrieve individual sequence fragments quickly and
efficiently.  We got tired of NCBI \code{fastacmd}'s horrible
memory requirements and slow speed, so we implemented fast sequence
indexing.

\item \class{NLMSA} reads each MAF file and divides the interval
alignment data into one or more coordinate systems created 
on-the-fly (for efficient memory usage, NLMSA uses \class{int}
coordinates (32-bit), which has a maximum size of approximately
2 billion.  This is too small even for a single genome like human;
\class{NLMSA} automatically splits the database into as many
coordinate systems are needed to represent the alignment.
Each coordinate system has its own database file on disk.

\item After it has finished reading the MAF data, \class{NLMSA}
begins to build the database indexes for each coordinate 
system.  Computationally, this operation is equivalent to
a {\em sort} (N log N complexity).  Once the indexes are built, the database is
ready for use.
\end{itemize}
	
\subsubsection{Example: Mapping an entire gene set onto a new genome version}
To illustrate how Pygr can perform a big task with a little code, here is an example that maps a set of gene sequences onto a new version of the genome, using megablast to do the mapping, and a relational database to store the results.  Moreover, since mapping 80,000 gene clusters takes a fair amount of time, the calculation is parallelized to run over a large number of compute nodes simultaneously:

\begin{verbatim}
import pygr.Data
from pygr.apps.leelabdb import * # this accesses our databases
from pygr import coordinator     # this provides parallelization support

def map_clusters(server,dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "CLIENT FUNCTION: map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome = pygr.Data.Bio.Seq.Genome.HUMAN.hg17()
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, 
    protein, clusterExons,clusterSplices) = getSpliceGraphFromDB(spliceCalcs[dbname])
    # NOW MAP CLUSTER SEQUENCES ONE BY ONE TO OUR NEW genome
    for cluster_id in server:
        g = genomic_seq[cluster_id] # GET THE OLD GENOMIC SEQUENCE FOR THIS CLUSTER
        m = genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE result_table USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR TO KEEP ERROR TRAPPING 
		         # HAPPY

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "SERVER FUNCTION: serve up cluster_id one by one to as many clients as you want"
    cursor = getUserCursor(dbname)
    t = SQLTable(source_table,cursor)
    for id in t:
        yield id # HAND OUT ONE CLUSTER ID TO A CLIENT

if __name__=='__main__': # AUTOMATICALLY RUN EITHER THE CLIENT OR SERVER FUNCTION
    coordinator.start_client_or_server(map_clusters,serve_clusters,[],__file__)
\end{verbatim}

First, let's just focus on the map_clusters() function, which illustrates how the mapping of each gene is generated and saved.  Let's examine the data piece by piece:
\begin{itemize}

\item
genome: a BLAST database storing our hg17 genome sequence

\item
genomic_seq: another sequence database (which in this case happens to be stored in a relational database), mapping each cluster ID to a piece of the old genomic sequence version containing that specific gene.

\item   
cluster_id: a cluster ID for us to process.

\item
g: the actual sequence object associated with this cluster_id

\item
m: the mapping of g onto genome, as generated by megablast after first running RepeatMasker on g, using the RepeatMasker options passed as rmOpts.  Note that only the top hit will be saved (maximum number of hits to save maxseq=1), and only if it has at least 98\% identity.  This alignment is then saved to a relational database table using createTableFromRepr().

\end{itemize}
This code will run in parallel over as many compute nodes as you have free, using Pygr's coordinator module.  The parallelization model for this particular task is simple: a single iterator (server) dispensing task IDs to many clients. 

\begin{itemize}

\item
server: the serve_clusters() function is trivial: all it does is connect to a specific database table (source_table) and iterate over all its primary keys, yielding them one by one.

\item    
client: the map_clusters() function expects an iterator as its first argument, which must give it a sequence of task IDs (cluster_id in this script).  This iterator is actually using an XMLRPC request to the server to get the next task ID, but that is done transparently by the coordinator.Processor() class.  The map_clusters() function is modeled as a generator: that is, it first does some initial setup (loading the database schema for example), then it runs its actual task loop, yielding each completed task ID. This enables coordinator.Processor to run map_clusters() within an error-trapping try: except: clause that catches and reports all errors to the central coordinator.Coordinator instance, and also to implement some intelligent error handling policies (like robustly preventing rare individual errors from causing an entire Processor() to crash, but detecting when consistent patterns of errors occur on a particular Processor, and automatically shutting down that Processor.

\item 
start_client_or_server(): this line automatically starts up the correct function (depending on whether this process is running as client or server).  To make a long story short, all you have to do is run the script once (as a server), and it will automatically start clients for you on free compute nodes (using ssh-agent), with reasonable load-balancing and queuing policies.  For details, see the coordinator module docs.
\end{itemize}


\subsection{Sequence Annotation Databases}
\subsubsection{What is a pygr sequence annotation?}
{\em Annotation} -- information bound to specific intervals of a genome
or sequence -- is an essential concept in bioinformatics.  We would like to
be able to store and query annotations naturally as part of working with
multi-genome alignments, as a standard operation in comparative genomics.
Pygr makes this easy:
\begin{verbatim}
for alignedRegion in msa[myRegion]: # FIND ALIGNMENT IN OTHER GENOMES
  for ival in alignedRegion.exons: # SEE IF THIS CONTAINS ANY ANNOTATED EXONS
    if ival.orientation>0: # ENSURE ANNOTATION IS ON SAME STRAND AS alignedRegion
      print 'exon\tID:%d\tSEQ:%s' % (ival.id,str(ival.sequence)) # PRINT ITS SEQUENCE
      for exon2,splice in ival.next.items(): # LOOK AT ALTERNATIVE SPLICING OF THIS EXON
        do something...
\end{verbatim}
\begin{itemize}
\item In the above code, we assumed that there exists a mapping of any genomic
sequence region (\code{alignedRegion}) to exon annotations.  This mapping
is bound by pygr.Data.schema to the sequence object's \code{exons} attribute.
In a moment we will see how to construct such a mapping.

\item An annotation is an interval (i.e. it has length, and can be sliced,
or negated to get the opposite strand) that has bound attributes giving
biological information about the annotation.  It acts like a little coordinate
system, i.e. \code{annotation[0]} is the first position in the annotation;
\code{annotation[-10:]} is the last ten positions of the annotation etc.
Any subslice of the annotation gives its coordinates and orientation
relative to the original annotation.  In the example above, we used this
to check whether the annotation slice \code{ival} (which is returned 
in the same orientation as \code{alignedRegion}) is on the same strand
(orientation = 1) as the original exon annotation,
or the opposite strand (orientation = -1).

\item An annotation object or slice is associated with the corresponding
sequence interval of the sequence to which it is bound.  To obtain that
sequence interval, simply request the annotation's \member{sequence}
attribute.  Use this if you want to get its sequence string via \code{str},
for example.

\item An annotation object or slice always has an
\member{annotationType} attribute giving a string identifier for 
its annotation type.

\item For any annotation slice, its \code{pathForward} attribute
points to its parent annotation
object.  For example, in the case above, \code{alignedRegion} might not contain
the whole exon, in which case \code{ival} would be just the part of the exon
contained in \code{alignedRegion}, and \code{ival.pathForward} would be the complete
exon.  Note that for nucleotide sequence annotations, orientation matters.
In this example, we restricted our analysis to exons that are on the same
strand as \code{alignedRegion}.

\item In addition, pygr also marks it with 
two attributes that identify it as belonging to its annotation database:
\code{id} gives its unique identifier (primary key) in that database,
and \code{db} points to the annotation database object itself.

\item Because pygr can see that \code{ival} is part of the exons annotation database,
it can apply schema information automatically to it.  In this particular case,
it applies the splicegraph schema to it (see example from pygr.Data.schema
tutorial above), so we can find out what exons it splices to via its \code{next}
attribute.
\end{itemize}


\subsubsection{Constructing an Annotation Database}
Suppose you had a set of annotations \code{sliceDB} each consisting of a sequence ID,
start, and stop coordinates.  We can easily construct an annotation database
from this:
\begin{verbatim}
from pygr import seqdb,cnestedlist
annoDB = seqdb.AnnotationDB(sliceDB,genome) # CREATE THE ANNOTATION DB
nlmsa = cnestedlist.NLMSA('exonAnnot','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                          pairwiseMode=True,bidirectional=False)
for a in annoDB.itervalues(): # SAVE ALL ANNOTATION INTERVALS
  nlmsa.addAnnotation(a) # ADD ALIGNMENT BETWEEN ival AND ann INTERVALS
nlmsa.build() # WRITE INDEXES FOR THE ALIGNMENT
annoDB.__doc__ = 'exon annotation on the human genome'
pygr.Data.Bio.Genomics.ASAP2.exons = annoDB # ADD AS A PYGR.DATA RESOURCE
nlmsa.__doc__ = 'map human genome regions to contained exons'
pygr.Data.Bio.Genomics.ASAP2.exonmap = nlmsa # NOW SAVE MAPPING AND SCHEMA
pygr.Data.schema.Bio.Genomics.ASAP2.exonmap = \
      pygr.Data.ManyToManyRelation(genome,annoDB,bindAttrs=('exons'))
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
\begin{itemize}
\item NLMSA provides an efficient, high-performance way to store and
query huge annotation databases.  The mapping is stored on disk but is
accessed with high-speed indexing.

\item More importantly, however, a pygr.Data user need never even be
aware that an NLMSA is being used to provide this mapping.  As far as
users are concerned, all they need to know is that any sequence object from \code{genome}
has an \code{exons} attribute that automatically gives a list of exon
annotations contained within that sequence.  I.e. as in the previous 
example, you would simply access it via:
\begin{verbatim}
for exon in someRegion.exons:
  do something...
\end{verbatim}

\item The \code{ManyToManyRelation} indicates that \code{nlmsa} should
be interpreted as being a many-to-many relation from items of \code{genome}
to items of \code{annoDB}.  It also creates an \code{exons} attribute on
items of \code{genome} that translates to \code{g.exons=nlmsa[g]}.

\item In the above example, we assumed that \code{genome} was obtained
from pygr.Data (and thus a pygr.Data resource ID).  If not, we would first
have to add it, just as we did for \code{annoDB}.

\item Note that we only bound an attribute (\code{exons}, to the 
\code{genome} items) for the forward mapping (from \code{genome} to \code{annoDB}).
We did not even store the reverse mapping in \code{nlmsa}, because
it is completely trivial.  (i.e. an annotation from \code{annoDB} is itself
the interval of \code{genome} that it maps to).  This was set by
the \code{bidirectional=False} option to the \code{NLMSA}.

\item The \code{pairwiseMode} option indicates that this is a pairwise
(sequence to sequence) alignment, not a true multiple sequence alignment
(which requires its own coordinate system, called an LPO; see the reference
docs on NLMSA for more information).  This option could have been omitted;
pygr would have figured it out automatically from the fact that we saved 
direct alignments of sequence interval pairs to \code{nlmsa}.  NLMSA does not
permit mixing pairwise and true MSA alignment formats in a single NLMSA.
\end{itemize}

\subsubsection{Constructing an Annotation Mapping using Megablast}
What if someone provided you with a set of ``exon annotations'' in the form
of short sequences representing the exons, rather than actual genomic
coordinates?  Again, pygr makes this mapping extremely easy to save:
\begin{verbatim}
from pygr import seqdb,cnestedlist
annoDB = seqdb.AnnotationDB(None,genome,'exon', # CREATE THE ANNOTATION DB
                            filename='exonAnnot',mode='c') # STORE ON DISK
nlmsa = cnestedlist.NLMSA('exonMap','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                          pairwiseMode=True,bidirectional=False)
for id,s in exonSeqs.items(): # SAVE ALL ANNOTATION INTERVALS
  for ann in annoDB.add_homology(s,'megablast',id=id,maxseq=1,minIdentity=98,maxLoss=2):
    nlmsa.addAnnotation(ann)
nlmsa.build() # WRITE INDEXES FOR THE ALIGNMENT
annoDB.close() # SAVE ALL OUR ANNOTATION DATA TO DISK
annoDB.__doc__ = 'exon annotation on the human genome'
pygr.Data.Bio.Genomics.ASAP2.exons = annoDB # ADD AS A PYGR.DATA RESOURCE
nlmsa.__doc__ = 'map human genome regions to contained exons'
pygr.Data.Bio.Genomics.ASAP2.exonmap = nlmsa # NOW SAVE MAPPING AND SCHEMA
pygr.Data.schema.Bio.Genomics.ASAP2.exonmap = \
      pygr.Data.ManyToManyRelation(genome,annoDB,bindAttrs=('exons',))
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}

\begin{itemize}
\item This example assumes \code{exonSeqs} is a dictionary of exon IDs and sequence
strings.

\item By passing \code{None} as the \var{sliceDB} argument, we force the
AnnotationDB to create a new dictionary for us.  By passing the \var{filename}
argument, we make it create a Python shelve disk file to store the dictionary.

\item The \method{add_homology}() method takes a sequence or string argument,
and performs a homology search against \code{genome}.  This requires that
our \code{genome} provide a method attribute matching our search name
('megablast'), which must return an alignment object.  For a \class{BlastDB}
object we could use either its \method{blast} or \method{megablast} methods.
Since we have provided an \var{id}, it will be used as
the id for the annotation.  The remaining arguments are passed to the
homology search and filtering functions; see the \class{BlastDB} and
\method{NLMSASlice.keys} documentation for full details of the options you
can use.  These specific arguments indicate that only the top hit should
be processed (maxseq=1), that it must have at least 98\% identity to the
query, and that no more than 2 nucleotides can be missing relative to the 
original query.  \method{add_homology}() returns a list of the resulting
annotation(s) for this search, which are added to the alignment as usual.

\item Because we requested creation of a disk file to store the annotation.sliceDB,
we must call the annotationDB's close() method, when we are done, to 
save all of the annotation data to disk.  Otherwise, the Python shelve file might be
left in an incomplete state.

\item We save pygr.Data resource and schema information as before.

\end{itemize}

\subsection{Using Pygr as a Graph Database}
The real power of Pygr is that it provides a simple model for viewing
all data as {\em graph databases}-- in which all data are represented
as nodes and connections between nodes (edges), and queries are formulated
as a specific pattern of connections to find --
in a very Pythonic style.  To illustrate the simplicity and power
of the graph database approach, Pygr has a strong emphasis
on bioinformatics applications ranging from genome-wide analysis of
alternative splicing patterns, to comparative genomics queries of
multi-genome alignment data.

The following introductory examples show how to use Pygr for graph queries, sequence searching and alignment queries, annotation queries, and multigenome alignment queries.


\subsubsection{Example: Simple graph query}
Why would you want to use Pygr?  Interesting data often consists of specific graph structures, and these relationships are much easier to describe as graphs than they are in SQL.  For example, the simplest and most common form of alternative splicing is exon-skipping, where an exon is either skipped or included (see slide 15 of the ISMB slides for a picture).  This can be defined immediately as a graph in which three nodes (exons 1, 2, 3) are joined by edges either as 1-2-3 or 1-3.  Unfortunately, writing an SQL query for this simple pattern requires a 6-way JOIN (argh).

\begin{verbatim}
SELECT * FROM exons t1, exons t2, exons t3, splices t4, splices t5, splices t6 
WHERE t1.cluster_id=t4.cluster_id AND t1.gen_end=t4.gen_start 
  AND t4.cluster_id=t2.cluster_id AND t4.gen_end=t2.gen_start 
  AND t2.cluster_id=t5.cluster_id AND t2.gen_end=t5.gen_start 
  AND t5.cluster_id=t3.cluster_id AND t5.gen_end=t3.gen_start 
  AND t1.cluster_id=t6.cluster_id AND t1.gen_end=t6.gen_start 
  AND t6.cluster_id=t3.cluster_id AND t6.gen_end=t3.gen_start;
\end{verbatim}

Such a six-way JOIN is painfully slow in a relational database; in general such queries just aren't practical.  More fundamentally, the relational schema is forced to represent the graph relation with combinations of foreign keys and other data that the user really should not have to remember.  All the user should know is that there is a specific relation, e.g. from this exon, the "next" exon is X, and the relation joining them is splice Y.

In Pygr, writing the query is just a matter of writing down the graph (edges from 1 to 2, 1 to 3, and 2 to 3, but no special "edge information"):

\begin{verbatim}
queryGraph = {1:{2:None,3:None},2:{3:None},3:{}}
\end{verbatim}

We can now execute the query using the GraphQuery class:

\begin{verbatim}
results = [dict(m) for m in GraphQuery(spliceGraph,queryGraph)]
\end{verbatim}

This is more or less equivalent to writing a bunch of for-loops for iterating over the possible closures:

\begin{verbatim}
results = []
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in spliceGraph[e1]: # NEXT EXON
        for e3 in spliceGraph[e2]: # NEXT EXON
            if e3 in spliceGraph[e1]: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

It is often convenient to bind an object attribute to a graph, so that you can use either the graph syntax or a traditional object attribute and mean exactly the same thing.  In the splice graph example, we bind the exon.next attribute to the spliceGraph, so the above for-loops can also be written:

\begin{verbatim}
results = []
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in e1.next: # NEXT EXON
        for e3 in e2.next: # NEXT EXON
            if e3 in e1.next: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

Another interesting query in the alternative splicing field is the so-called U12-adapter exon query (see slide 21 of the ISMB slides):

\begin{verbatim}
queryGraph = {0:{1:dict(dataGraph=alt5Graph),
                 2:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},
              1:{3:None},
              2:{3:None},
              3:{}}
\end{verbatim}

Here we use edge information in the query graph to add a few constraints:

\begin{itemize}
\item
the dataGraph argument tells the query to search for exon 1 from exon 0 using a different graph (alt5Graph).

\item    
the filter argument provides a function that returns True only if the edge between exon 0 and exon 2 is of type U11/U12.  Therefore the query will only match sp
lice graphs that have a U12 splice between this pair of exons.

\end{itemize}

Note that the query graph "nodes" (in this example, the integers 0, 1, 2, 3) are
quite arbitrary.  We could have used strings, or other kinds of objects instead.

Now if we want to see the results right away, we use the mapping returned by GraphQuery to look at individual nodes and edges of the dataGraph that matched our query:

\begin{verbatim}
for m in GraphQuery(spliceGraph,queryGraph):
    print m[1].id,m[0,2].id # PRINT EXON ID FOR EXON 1,
                            # SPLICE ID FOR SPLICE 0 -> 2
\end{verbatim}

The match is returned by GraphQuery as a mapping from nodes and edges of the query graph to nodes and edges of the data graph.  Edges are specified simply as tuples of the nodes you want to get the edge for (in this example 0,2).
Constructing a Graph
How was the spliceGraph created in the first place?  Let's say we have an initial list of tuples giving connections between exon objects and splice objects, where each tuple consists of a pair of exons connected by a splice.

\begin{verbatim}
for exon1,exon2,splice in spliceConnections: 
    spliceGraph += exon1 # add exon1 as a node in the graph
    spliceGraph += exon2 # if already a node in the graph, does nothing...
    exon1.next[exon2] = splice # add an edge, with splice as the edgeinfo
\end{verbatim}

The last operation makes use of the binding of exon.next to spliceGraph, and is equivalent to

\begin{verbatim}
spliceGraph[exon1][exon2] = splice
\end{verbatim}

If we didn't want to save the edge information, we could use the simpler syntax

\begin{verbatim}
spliceGraph[exon1] += exon2 # equivalent to exon1.next+=exon2
\end{verbatim}

This "short" form is equivalent to saving None as the edge information.


\subsubsection{Alignment Query as a Graph Database Query}

Pygr can do much more sophisticated analyses than this fairly easily.  
Just to give a taste of how to use these capabilities, we will illustrate
one example of a standard Pygr model: querying Pygr data by drawing a ``query
graph'' showing the connections we want to find, and running its GraphQuery()
engine.  Since Pygr alignments follow the same interface as any Pygr graph, we can query them using the standard GraphQuery class.  Let's say we have a Python script load_alignments.py that loads two alignments:

\begin{itemize}

\item
mRNA_swiss: an alignment of mRNA sequences to homologous SwissProt sequences;

\item
swiss_features: an alignment of SwissProt sequences onto annotation objects. 

\end{itemize}
To find out how the known SwissProt annotations map on to our mRNA sequences requires a join, which can be formulated as a simple Pygr graph, consisting of a mapping of an mRNA sequence interval (node 1), onto a SwissProt sequence interval (node 2), onto a feature annotation (node 3):

\begin{verbatim}
>>> from load_alignments import * # load the alignments
>>> from pygr.graphquery import *      # import the graph query code
# draw a graph using a dict.  Note: edge 2->3 must come from swiss_features
>>> queryGraph = {1:{2:None},2:{3:dict(dataGraph=swiss_features)},3:{}} 
# run the query and save the mappings
>>> l = [dict(d) for d in GraphQuery(mRNA_swiss,queryGraph)] 
>>> len(l) # how many annotations mapped onto our mRNA sequences?
4703
\end{verbatim}

We assumed that mRNA_swiss would be passed as the default dataGraph, and specified directly that edge 2->3 should be looked up in swiss_features.  We then captured all the results from the GraphQuery iterator using a Python list comprehension.  Note that since the iterator returns each result in the same container (mapping object), if we want to save all the individual results we have to copy each one to a new mapping (dict) object, as illustrated in this example.
Storing Alignments in a Relational Database


\subsubsection{Example: a MySQL Database OBSOLETE}
Here's an example of working with sequences from a relational database:

\begin{verbatim}
>>> import MySQLdb # standard module for accessing MySQL, now get a cursor...
>>> rdb = MySQLdb.connect(db='HUMAN_SPLICE_03',read_default_file=os.environ['HOME'
]+'/.my.cnf')
>>> t = SQLTable('genomic_cluster_JUN03',rdb.cursor()) #interface to a table of
 sequences
>>> from pygr.seqdb import *   # pygr module for working with sequences from databases
>>> t.objclass(DNASQLSequence) #use this class as "row objects"
>>> s2 = t['Hs.1162'] # get a specific sequence object by ID
>>> str(s2[1000:1050]) # this will only get 50 nt of the genomic sequence from 
MySQL
'acctgggtgatgaaataaatttttacgccaaatcccgatgacacacaatt'
\end{verbatim}

(Note: in this example we used MySQLdb.connect()'s ability to read database 
server and user authentication information directly from the standard ~/.my.cnf file normally used by the MySQL client).

\subsubsection{More examples}
\label{more-exam}

Additional examples of how to use pygr can be found in the tests/ directory within the pygr distribution package.




\section{Module Documentation}
\label{module-doc}

The following subsections provide details about how to use specific
modules of Pygr functionality. 

\subsection{Installation}
\label{install}
Installing pygr is quite simple:
\begin{verbatim}
tar -xzvf pygr-0.3.tar.gz 
cd pygr
python setup.py install 
\end{verbatim}

Once the test framework has completed successfully, the setup script
will install pygr into python's respective site-packages directory. 

If you don't want to install pygr into your system-wide site-packages,
replace the "python setup.py install" command with
\begin{verbatim}
python setup.py build
\end{verbatim}
This will build pygr but not install it in site-packages.

Pygr contains several modules imported as follows:
\begin{verbatim}
from pygr import seqdb # IMPORT SEQUENCE DATABASE MODULE
\end{verbatim}

If you did not install pygr in your system-wide site-packages, you 
must set your PYTHONPATH to the location of your pygr build.
For example, if your top-level pygr source directory is PYGRDIR then
you'd type something like:
\begin{verbatim}
setenv PYTHONPATH PYGRDIR/build/lib.linux-i686-2.3
\end{verbatim}
where the last directory name depends on your specific architecture.


\subsection{sequence Module}
\label{sequence}

{\em Base classes for representing sequences and sequence intervals.}


\subsubsection{Overview}
Pygr provides one base class representing both sequences and sequence intervals (SeqPath),
from which all sequence classes are derived (Sequence, SQLSequence, BlastSequence etc.).
In this section we document both the features of the base class, and ways to extend or
customize it by creating your own subclasses derived from SeqPath.  The IntervalTransform
class represents a coordinate system mapping from one interval of a sequence, onto 
another interval of the same or a different sequence.

\subsubsection{SeqPath}
This class provides the basic capabilities of a sliceable sequence or sequence interval,
widely used in Pygr.  It tries to provide core operations on sequences in a highly
Pythonic way:

\begin{itemize}

\item    
{\em Python Sequence}: of course, SeqPath behaves like a Python sequence. i.e.
the length of a \class{SeqPath} \var{s} is just \code{len(s)}, 
and you iterate over the ``letters'' in it using \code{for l in s:}  
(Note, the individual letters produced by this iterator
will themselves be \class{SeqPath} objects (by default, of length 1)).  
And all the slicing
operations defined for Python Sequences also apply to 
\class{SeqPath} (see below).

\item    
{\em Slicing}: \class{SeqPath} is designed to represent a slice 
(subinterval) of a sequence.
Like the Python builtin \class{slice} class, it has \member{start}, 
\member{stop}, and \member{step} attributes that indicate 
the interval beginning, end, and ``stride''.
Moreover, it is itself sliceable in the usual pythonic way, 
i.e. \code{s[start:stop]},
where \var{start} and \var{stop} are in the local coordinate system of \var{s} 
(i.e. \code{s[0]} is the first letter of the interval represented by 
\var{s}). Note that \class{SeqPath}
follows the Python slicing coordinate conventions of positive integers as
forward coordinates (i.e. counting from the interval start) and negative integers
as reverse coordinates (i.e. counting from the interval end).

\item    
{\em String value}: to obtain the actual sequence string representation
of a \class{SeqPath}, just use the Python builtin \code{str(s)}.  
Note that in most cases
a SeqPath object does not itself store the sequence string associated with it
but obtains it from somewhere else when the user requests it.

\item
{\em comparison and containment}: \class{SeqPath}
implements the interval-ordering
and interval-containment relations using the standard Python order operators
and containment operators. i.e. s<t iff s.start<t.start, and s in t iff
t.start<=s.start and s.stop<=t.stop.

\item
{\em orientation}: SeqPath carefully represents relationships between intervals
on opposite strands of a double-stranded nucleotide sequence.  A SeqPath object
knows whether it is an interval on the forward or reverse strand.  Pygr provides
a number of operations for manipulating and comparing intervals of different
orientations.  For example, \code{-s} yields the interval of the opposite strand that
is base-paired to interval s (i.e. this is not just the reverse-complement of \var{s}
in the string 'atgc' $\rightarrow$ 'gcat' sense, but is specifically the SeqPath
object representing the coordinate
interval on the opposite strand that is base-paired to \var{s}).

\item
{\em schema}: a SeqPath object knows ``what sequence'' it is an interval of;
it is not just a (start,stop) coordinate pair, but is actually bound to a specific
parent sequence object.  Specifically, s.path is the parent sequence object of
which s is a subinterval; s.path will itself be an instance of SeqPath, and its path
attribute will simply be itself.  All SeqPath objects are descended from such ``top-level''
SeqPath objects.  Note that when you have sequence intervals from both forward
and reverse strands of a sequence, all of the forward strand intervals will share
the same path attribute (your original top-level sequence object representing
the whole sequence in forward orientation), while all the reverse strand intervals
will reference another top-level SeqPath created automatically to represent the
reverse strand.

\item
{\em graph structure}: a SeqPath object itself acts as a graph, whose nodes are
the individual letters of the sequence, and whose edges represent the link 
from each letter to the next (if any).  Thus standard graph query works on
SeqPath objects, through the usual interfaces:

\begin{verbatim}
for l in s: # GET EACH LETTER OF THE SEQUENCE
    c=str(l)

edge = s[l1][l2] # GET EDGE INFORMATION FOR l1 --> l2

for l1,l2,edge in s.edges(): # GET ALL l1 --> l2 EDGES
    do_something(l1,l2,e)

# DUMB GRAPH QUERY TO FIND 'AG' SUBSTRINGS IN SEQUENCE s
for d in GraphQuery(s,{0:{1:dict(filter=lambda fromNode,toNode:
                                 str(fromNode)=='A' and str(toNode)=='G')},
                       1:{}})
    l1,l2,edge = d[0],d[1],d[0,1]
\end{verbatim}  

For more information about edges, see the LetterEdge class.

\item
{\em Mutable Sequences}: Just as the Python builtin list class implements
``mutable sequence'' objects that can be resized, SeqPath objects can be
resized and changed, without breaking existing subinterval objects that
are ``part of'' the resized SeqPath object.  In particular, just as a list
can be resized by extending its ``stop'' coordinate to a higher value, a SeqPath can
be resized by extending its stop coordinate to a higher value.  Indeed,
you can even create a SeqPath for a particular sequence without knowing that
sequence's length (computing the length of a genome sequence might take a long
time, if all you want to do is create a sequence object to represent that
sequence).  You can do this by passing {\em None} as the stop (or start)
coordinate.  In that case, SeqPath will automatically determine its own
length at a later time iff a specific user operation makes it absolutely 
necessary to know this length.

\item
{\em intersection, union, difference}: SeqPath uses the Python *, + and - 
operators to implement interval intersection, union, and difference
operations respectively.

\end{itemize}

\begin{funcdesc}{before}{} 
  This method returns the entire sequence interval preceding this interval.
  For example, if \code{exon} is an interval of genomic sequence, then
  \code{exon.before()[-2:]} is its acceptor splice site (i.e. the 2 nt immediately
  before \code{exon}).
\end{funcdesc}

\begin{funcdesc}{after}{} 
  This method returns the entire sequence interval following this interval.
  For example, if \code{exon} is an interval of genomic sequence, then
  \code{exon.after()[:2]} is its donor splice site, (i.e. the 2 nt immediately
  after \code{exon}).
\end{funcdesc}


\subsubsection{Sequence class}

\begin{funcdesc}{sequence.Sequence}{s, id}
  The Sequence class provides a SeqPath flavor that stores a sequence string
  {\em s} and identifier {\em id} for this sequence.

\begin{verbatim}
from pygr import sequence
seq = sequence.Sequence('GPTPCDLMETQ','FOOG_HUMAN')
\end{verbatim}
\end{funcdesc}


\begin{funcdesc}{update}{s}
  You can change the actual string sequence to a new string {\em s}
  using the {\em update} method:

\begin{verbatim}
seq.update('TKRRPLEDKMNEPS')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{seqtype}{}
  returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}


\begin{funcdesc}{reverse_complement}{s}
  returns the reverse complement of the sequence string s.
\end{funcdesc}


\begin{itemize}
\item
\member{id}: the \member{id} attribute stores the sequence's identifier.

\end{itemize}

\subsubsection{Coordinate System}
SeqPath follows Python slicing conventions (i.e. 0-based indexing, positive indexes
count forward from start, negative indexes count backwards from the sequence
end, and always {\em s.start<s.stop}).

Each SeqPath object has a number of attributes giving information about its
``location'':

\begin{itemize}

\item    
\member{orientation}: +1 if on the forward strand, or -1 if on the reverse strand.

\item
\member{path}: the top-level sequence object that this interval is part of, or self
if this object is its top-level (i.e. not a slice of a larger sequence).  Note that
all forward intervals share the same path attribute, but reverse strand intervals
all have a path attribute that represents the entire reverse strand.

\item
\member{pathForward}: same as {\em path}, but always the forward strand sequence.

\item
\member{start}: start coordinate of the interval.  NB: SeqPath stores coordinates
relative to the start of the {\em forward} strand.  This is necessary for allowing
resizing of the top-level SeqPath; if coordinates were relative to the end of the
sequence, they would have to be recomputed every time the length of the sequence 
changed.  The main consequence of this is that coordinates for forward intervals
are always positive, whereas coordinates for reverse intervals are always 
negative (i.e. following the Python convention
that negative coordinates count backwards
from the end, and the fact that the end of the reverse strand corresponds to 
the start of the forward strand). NB2: if the SeqPath was originally created with
{\em start=None}, requesting its start attribute will force it to compute its start
coordinate, typically requiring a computation of the sequence length.  In this
case, the start attribute will computed automatically by SeqPath.__getattr__().

\item
\member{stop}: end coordinate of the interval.  The above comments for {\em start}
apply to {\em stop}.  Note that for reverse intervals, a {\em stop} value of 0
means the end of the reverse strand (i.e. -1 is the last nucleotide of the 
reverse strand, and 0 is one beyond the last nucleotide of the reverse strand).

\item
\member{_abs_interval}: a tuple giving the ({\em start,stop}) coordinates of the 
interval on the forward strand corresponding to this interval (i.e. for a 
forward interval, itself, or for a reverse interval, the interval that base-pairs
to it).

\end{itemize}


\subsubsection{Extending and Customizing}
There are several methods and attributes you can override to extend or customize
the behavior of your own SeqPath-derived classes.  Typically you will derive
either from the Sequence class, or in some cases from the SeqPath class.

\begin{funcdesc}{strslice}{start, stop, useCache=True} 
  called to get the string
  sequence of the interval ({\em start, stop}).  You can provide your own strslice()
  method to customize how sequence is stored and accessed.  For example,
  \method{SQLSequence.strslice()} gets the sequence via a SQL query, and 
  \method{BlastSequence.strslice()} obtains it using the 
  \code{fastacmd -L start,stop} 
  UNIX shell command from the NCBI toolkit.
  The optional \var{useCache} argument controls whether your \method{strslice} method
  should attempt to get the sequence slice from its database cache (if any),
  or, if false, only directly from its back-end storage (in the usual way
  described above).
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  called to compute the length of the sequence.  You can
  customize this to provide an efficient length method for your particular
  sequence storage.  e.g. \class{SQLSequence} obtains it via a SQL query; 
  \class{BlastSequence} obtains it from a precomputed length index.
  The default \method{Sequence.__len__()} method computes it from 
  \code{len(self.seq)}, assuming that the sequence can be accessed
  from the \member{seq} attribute.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{slice_obj}
  if you want to monitor or intercept slicing
  requests on your sequence, you can do so by providing your own getitem method.
  See \class{seqdb.BlastSequenceCache} class for an example.
  If the sequence object has a \code{db} attribute, and that database object
  it points to has an \code{itemSliceClass} attribute, \code{SeqPath.__getitem__}
  will use that class to construct the subinterval object.  Similarly,
  if the sequence object has an \code{annot} attribute, and that annotation
  object has a \code{db} attribute, again the \code{itemSliceClass} attribute
  of that database will be used as the class to construct the subinterval
  object.  Otherwise it will
  use \code{SeqPath} itself as the class for constructing the subinterval object.

  Note: this \code{itemSliceClass} behavior applies not only to 
  sequence slices obtained via \method{__getitem__}, but also from all other
  methods that return sequence slices, such as the following list:
  \method{before}, \method{after}, \method{__mul__}, \method{__neg__}.
  \method{__add__}, \method{__iadd__}.
\end{funcdesc}

\begin{funcdesc}{__mul__}{other}
  get the sequence interval intersection of \var{self} and \var{other}.
\end{funcdesc}

\begin{funcdesc}{__neg__}{}
  get the sequence interval representing the opposite strand of \var{self} 
  i.e. the slice whose string value is the reverse complement of the string
  value of \var{self}.
\end{funcdesc}

\begin{funcdesc}{__add__}{other}
  get the sequence interval union of \var{self} and \var{other}, i.e.
  the smallest sequence interval that contains both of them.
\end{funcdesc}


\begin{funcdesc}{__getattr__}{attr}
  if you subclass a \class{SeqPath}-derived class and supply a \method{__getattr__}
  method for your subclass, it {\em must} call the parent class's 
  \method{__getattr__}.  This is essential for ``delayed evaluation'' of
  \member{start} and \member{stop} attributes, which are generated automatically
  by \class{SeqPath}'s \method{__getattr__}.  If your subclass inherits from
  more than one parent class, check whether {\em both} parents supply a 
  \method{__getattr__}, in which case your subclass must supply a
  \method{__getattr__} that explicitly calls both of them.  Failing to do so
  will lead to strange bugs.
\end{funcdesc}

\begin{itemize}
\item
\member{seq}: the \method{Sequence.strslice()} method assumes that 
the actual sequence is stored
on the \member{seq} attribute.  You could customize this behavior by 
making the \member{seq} attribute a property that is computed on the fly
by some method of your own.

\end{itemize}


\subsubsection{IntervalTransform}
This class provides a mapping transform between the coordinate
systems of a pair of intervals.

\begin{verbatim}
xform = IntervalTransform(srcPath,destPath)
d2 = xform(s2) # MAPS s2 FROM srcPath coords to destPath coord system
d3 = xform[s2] # CLIPS s2 TO NOT EXTEND OUTSIDE srcPath, THEN XFORMS
s3 = xform.reverse(d3) # MAP BACK TO srcPath COORD SYSTEM
\end{verbatim}

\subsubsection{Seq2SeqEdge}
This class represents a segment of alignment between two sequences.
It is a temporary object created in association with a MSASlice
object (see Alignment Module below).

\begin{funcdesc}{__init__}{msaSlice, targetPath, sourcePath=None}
  Create a Seq2SeqEdge for the targetPath, on the specified alignment
  slice.  If sourcePath is None, it will be calculated automatically
  by calling the slice's methods.
\end{funcdesc}

\begin{funcdesc}{__iter__}{sourceOnly=True, **kwargs}
  iterate over source intervals within this segment of alignment.
  \var{kwargs} will be passed on to the \var{msaSlice}'s 
  \method{groupByIntervals} and \method{groupBySequences} methods.
\end{funcdesc}

\begin{funcdesc}{items}{**kwargs}
  same as \method{__iter__}, but gets tuples of (source_interval,target_interval).
\end{funcdesc}

\begin{funcdesc}{pIdentity}{mode=max,trapOverflow=True}
  Compute the percent identity between the source and target sequence
  intervals in this segment of the alignment.  \var{mode} controls
  the method used for determining the denominator based on the lengths of
  the two aligned sequence intervals.  \var{trapOverflow} controls
  whether overflow (due to multiple mappings of the query sequence to 
  {\em different} regions of the alignment) is trapped as an error.
  To turn off such error trapping, set \var{trapOverflow=False}.
\end{funcdesc}

\begin{funcdesc}{pAligned}{mode=max,trapOverflow=True}
  Compute the percent alignment between the source and target sequence
  intervals in this segment of the alignment, i.e. the fraction of
  residues that are actually aligned as opposed to gaps / insertions,
  in the two intervals.
\end{funcdesc}

\begin{funcdesc}{conservedSegment}{pIdentityMin=.9,minAlignSize=1,mode=max}
  Return the longest alignment interval (possibly including gaps) with
  a \%identity fraction higher than \var{pIdentityMin}.  If there is no
  such interval, or the longest such interval
  is shorter than \var{minAlignSize}, it returns \var{None}.  The interval
  is returned as a tuple of integers \code{(srcStart,srcEnd,destStart,destEnd)}.
\end{funcdesc}

{\em Warning}: if your query sequence has multiple mappings in the alignment
(i.e. it is aligned to two or more different regions in the alignment),
\method{pIdentity}() and \method{pAligned}() may return fractions larger
than 1.0.  This is because the query sequence may align to a given
target sequence via {\em more} than one region in the alignment.  If you
encounter this problem, you can iterate through the individual mappings
yourself (by calling the \method{iter}(), \method{items}() or
\method{edges}() iterator methods for your alignment slice object), 
and calculating the percentage identity or alignment (via your own algorithm)
individually for each specific mapping.  For more
background on this problem, see ``Multiple Mappings'', below.

Note that the presence of multiple mappings is {\em not} a Pygr bug,
but simply reflects the alignment data loaded into Pygr.  \class{Seq2SeqEdge}
should be able to avoid this problem mostly, beginning with release 0.6.
(It tries to screen out hits not consistent with the specific region-region
mapping stored with this edge).

\subsubsection{SeqFilterDict}
This dict-like class provides a simple way for masking a set of sequences
to specific intervals.  It stores a specific interval for each
sequence.  Subsequent look-up using a sequence interval as a key will
return the intersection between that interval and the stored interval
for that sequence in the dictionary.  If there is no overlap, it
raises \code{KeyError}.

\begin{verbatim}
d = SeqFilterDict(seqIntervalList)
overlap = d[ival] # RETURNS INTERSECTION OF ival AND STORED IVAL, OR KeyError
\end{verbatim}

You can pass a list of intervals to store to the class constructor (as 
shown above).  You can also add a single interval using the syntax
\code{d[saveInterval]=saveInterval}.  (This syntax reflects the actual
mapping that the dictionary will perform if later called with the
same interval).

\subsubsection{LetterEdge}
This class represents an edge from origin -> target node.

\begin{funcdesc}{__iter__}{}
  iterate over seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{iteritems}{}
  generate origin, target seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{seq}
  return origin,target seqpos for sequence \var{seq};
  raise \code{KeyError} if not in this edge
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: returns its sequences that traverse this edge
\end{itemize}


\subsubsection{Functions}
The sequence module also provides convenience functions:

\begin{funcdesc}{guess_seqtype}{s}
  based on the letter composition of
  the string {\em s}, returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}

\begin{funcdesc}{absoluteSlice}{seq, start, stop}
  returns the sequence interval of top-level sequence object associated
  with \var{seq}, interpreting \var{start} and \var{stop} according to
  the Pygr convention: a pair of positive values represents an interval
  on the forward strand; a pair of negative values represents an
  interval on the reverse strand (see Coordinate System, above).
  Note: if {\em seq} is itself a subinterval, then the {\em start,stop}
  coordinates are interpreted relative to its parent sequence, i.e.
  \code{seq.pathForward[start:stop]}.
\end{funcdesc}


\begin{funcdesc}{relativeSlice}{seq, start, stop}
  returns the sequence interval of \var{seq}, interpreting
  \var{start} and \var{stop} according to
  the Pygr convention: a pair of positive values represents an interval
  on the forward strand; a pair of negative values represents an
  interval on the reverse strand (see Coordinate System, above).
  Note: if {\em seq} is itself a subinterval, then the {\em start,stop}
  coordinates are interpreted relative to {\em seq} itself, i.e.
  \code{seq[start:stop]}.
\end{funcdesc}




\subsection{Alignment Module}
\label{seqdb}

{\em Pygr interface to sequence alignment, and scalable storage for multigenome alignments}


\subsubsection{Overview}

Pygr provides a general model for interfacing with any kind of sequence alignment,
and also a uniquely scalable storage system for working with huge multiple sequence
alignments such as multigenome alignments.  Specifically, it lets you work with
an alignment both in the traditional Row-Column model (each row is a sequence, each
column is a set of individual letters from different sequences, that are aligned;
we will refer to this as the RC-MSA model), and also
as a graph structure (known as a Partial Order Alignment, which we will refer to as
the PO-MSA model).  This supports ``traditional'' alignment analysis, as well
as graph-algorithms, and even graph query of alignments.

This model has a few basic classes:
\begin{itemize}
\item    
\class{MSA}: this class represents an entire alignment.  It acts as a graph whose
nodes are sequences (or sequence intervals) that are aligned, and whose edges 
represent specific alignment relationships between specific pairs of sequences 
(or intervals).  Specifically, it acts as a dictionary whose keys are SeqPath
objects, and whose values are MSASlice objects (representing an alignment segment
associated with a specific SeqPath, see below for details).  For example, to find
out what's aligned to some sequence interval s1:
\begin{verbatim}
for s2 in msa[s1]: # GET ALL INTERVALS s2 ALIGNED TO s1 IN msa
    do_something(s1,s2)
\end{verbatim}

In addition, its {\em letters} attribute acts as a graph interface
to the Partial Order alignment (PO-MSA) representation of the alignment.  I.e.
it is a graph whose nodes each represent a set of individual letters from 
different sequences, that are aligned to each other, and whose edges connect
pairs of nodes that are ``adjacent'' to each other in at least one sequence.
Specifically, it acts as a dictionary whose keys are MSANode objects (see below),
and whose edges are LetterEdge objects (see previous section).
\begin{verbatim}
for node in msa.letters: # GET ALL ALIGNMENT ``COLUMNS'' IN msa
    for l in node: # GET ALL INDIVIDUAL SEQ LETTERS ALIGNED HERE
        say_something(node,l)
\end{verbatim}


\item    
\class{MSASlice}: this class represents a segment of alignment associated with
a specific sequence interval (s1).  It acts as dictionary whose keys are sequence
intervals s2 aligned to s1, and whose values are MSASeqEdge objects
that represent the alignment relationship between s1 $\rightarrow$ s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with s1, and the edges that interconnect them.

\begin{verbatim}
myslice = msa[s1]: # GET SLICE ALIGNED TO s1 IN msa
    for node in myslice.letters:  # GET ALL ALIGNMENT ``COLUMNS'' FOR s1
        for l1,l2,e in node.edges(): # GET INDIVIDUAL LETTERS ALIGNED TO l1 OF s1
	    whatever(l1,l2,e)
\end{verbatim}

This class also has a {\em regions} method that generates all the alignment
interval relationships in this slice according to ``grouping'' criteria such
as maximum permissible gap length, etc.  (i.e. any region of alignment containing
no gaps larger than a specified size would be returned as a single region, 
whereas any gap larger than the specified size would split it into two separate
regions).  This provides a general interface for group-by operations in alignment
query.

\item    
\class{MSASeqEdge}: this class represents a relationship between a pair of 
sequence intervals s1 and s2 (SeqPath objects).  It provides a mapping between
subintervals of s1 $\rightarrow$ s2.  I.e. it acts as a dictionary 
that accepts subintervals of s1 as keys, and maps them to aligned
subintervals of s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with them, and the edges that interconnect the nodes.

\item    
\class{MSANode}: this class represents a specific ``column'' in the alignment
that aligns a set of individual letters from different sequences.  This
corresponds to a node in the PO-MSA representation of the alignment.
It acts as a dictionary whose keys are sequence intervals (typically only
one letter long) aligned in this column, and whose values are MSASeqEdges
representing the alignment of that letter to the column (see above).

\end{itemize}

\subsubsection{NestedList Storage}
Pygr provides a highly scalable storage mechanism for working with
multi-genome alignments.  One fundamental challenge in working with
very large alignments is the {\em interval overlap query} problem: 
to obtain a portion of an alignment (defined by some interval of
interest) requires finding all interval elements in the ``alignment
database'' that overlap the query interval.  Since the intervals
can be indexed by start (or end) position, one can typically find the
first overlapping element in $O(\log N)$ time, where $N$ is the total
number of intervals in the database.  The problem is that since
standard index structures cannot index both {\em start} and {\em end}, 
to obtain {\em all} intervals that overlap the query interval, one must scan
forwards (or backwards) from that point.  Furthermore, one cannot stop
at the first non-overlapping interval; there might be an extremely long 
interval at the very beginning of the index, that extends to overlap 
the query interval.  In this case, one would have to scan the entire
database ($O(N)$ time) to guarantee that all overlapping intervals are
found.

The {\em nested list} data structure solves this problem, by moving
any interval in the database that is {\em contained} in another interval
out of the top-level interval list, into the {\em sublist} of the
parent interval.  Based on this, one can prove that one can stop
the scanning operation at the first non-overlapping interval (i.e.
the overlapping intervals in any list form a single contiguous block).
Overall, this reduces the query time to $O(\log N + n)$, where $n$ is
the number of intervals in the database that actually overlap the 
query (i.e. results to return).  Moreover, the nested list data structure
can be implemented very well both in computer memory (RAM) or as indexed
disk files.  Pygr's disk-based cnestedlist database can complete
a typical interval query of the 26GB UCSC 8 genome alignment in
about 60 microseconds, compared with 10-30 seconds per query using
MySQL.

\subsubsection{Multiple Mappings: a Warning}
Multi-genome alignments take traditional models of alignment to an
entirely different scale, and inevitably many of the assumptions of
standard row-column multiple sequence alignment are broken (e.g. 
no inversions; no cycles; etc.).  One major issue that users should be
aware of in UCSC multi-genome alignments is the possibility of 
{\em multiple mappings}, in which a given query sequence interval is
mapped to two or more different regions of the alignment (and thus potentially
to two or more different locations in a given target genome).  Currently,
UCSC multi-genome alignment are typically based on a single 
{\em reference genome}, to which all other genomes are aligned.  While
a given region of the reference genome might be guaranteed to have
a unique mapping in the UCSC multi-genome alignment, {\em other} genomes
do not appear to have any such guarantee: a region in any of those genome
can have multiple mappings.  This is problematic for several reasons:
\begin{itemize}

\item It introduces ambiguity in the alignment: you don't know which of the
multiple hits is considered to be the ``right'' alignment; the UCSC alignment
file does not tell you.

\item There is no scoring information to resolve this ambiguity.  In a way,
this situation is even worse than the common situation we previously faced
in search for alignment mappings using BLAST, because (unlike BLAST) the 
MAF alignment does not give a score that indicates which mapping is best.
(We haven't seen such scoring information; if it can be recovered for these
alignment files, we'd be love to know about that...).

\item It can cause ``buggy'' results in calculations based on the alignment.
For example, Pygr's \method{pIdentity}() and \method{pAligned}() computations
can give values larger than one when a query region has multiple hits.  This
is not, strictly speaking, a Pygr bug: the query region is mapped by the MAF
file to the same target region {\em multiple} times, resulting in multiple
overlaps.

\end{itemize}

If you encounter multiple mappings, you can always iterate over them one
by one, and perform your own computations for each one.  However, to avoid them
altogether, you can restrict your queries to the reference genome for this specific
alignment (UCSC offers different versions of each alignment set, each based on 
a different reference genome).

\subsubsection{NLMSA}
Top-level object representing an entire multiple sequence alignment, 
stored using a set of disk-based nested list interval databases.
The alignment is stored as an interval representation of a 
{\em linearized partial order} (LPO), using {\em nested list}
databases.  This has several elements:

\begin{itemize}

\item
{\em PO-MSA}: Conceptually, the alignment is represented as a partial order alignment
(PO-MSA), in which aligned sequence intervals are fused together as a single
``node'' in the alignment graph; two nodes are connected by an edge if and only
if they are adjacent in at least one of the sequences aligned to them
(i.e. if residue {\em i} of that sequence is in the first node, and 
residue {\em i+1} is in the second node, then there is a directed edge
from the first PO-MSA node to the second node).

\item
{\em LPO}: This alignment graph is {\em partially ordered}.  Let's define an
ordering relation {\em ``i<j''} to mean ``there exists a path
of directed edges from {\em i} to {\em j}''.  For two 
letters {\em i} and {\em j} in a sequence, {\em i<j XOR j<i} (i.e. all
nodes have an ordering relationship).  By contrast, if two nodes in the LPO
represent insertions in different sequences, then NOT {\em i<j} AND NOT {\em j<i}.
Thus there can be some nodes in the LPO that have no ordering relationship
with respect to each other.  It is still possible to map the PO-MSA onto 
a linear coordinate system (i.e. to ``linearize'' the partial order): as long
as the graph contains no cycles, we can map the nodes {\em i,j,k,...} of the graph
onto a linear coordinate system {\em x,y,z,...} such that for any pair of
nodes {\em i,j} mapped to coordinates {\em x<y}, we assert NOT {\em j<i}.  This is
called the {\em linearized partial order} (LPO). This maps the PO-MSA onto
a standard Row-Column MSA format, where the LPO coordinate (just an integer
sequence 0,1,2...) can be considered the index value of each alignment column.

\item
{\em nested list}: The actual alignment data are stored in the form of
({\em start,stop}) pairs representing aligned intervals.  Since this representation
uses intervals, not individual letters, it takes no more memory to store
an alignment of two 100 kb regions than it does to align two individual letters.
This is important for scalable storage (and query) of large multi-genome
alignments.  (Each alignment interval takes 24 bytes: five \class{int} for
the {\em (start,stop)} pairs and target sequence ID, plus one \class{int}
for the sublist ID).
These interval databases are stored using nested lists.  Specifically, 
the alignment is stored as 1) a mapping of each aligned sequence interval
onto an LPO coordinate interval; 2) a reverse mapping of each LPO interval onto
all the sequence intervals that are aligned there.  To find the alignment of
a sequence interval onto the other sequences in the alignment, that interval
is first mapped onto the LPO, and from there mapped back to intervals in the
other sequences.  A nested list database is stored for {\em each} of these 
mappings (i.e. for an alignment of {\em N} sequences, there will be {\em N+1}
nested list databases to store the MSA).  Furthermore, if the size of the LPO
coordinate system (i.e. number of columns in its RC-MSA format)
grows larger than the range representable by \class{int} (typically $2^{31}$  = 2 GB),
the LPO will have to be split into separate nested list databases of a size
smaller than the maximum range representable by \class{int}.  This is necessary
for handling alignments of large genomes (e.g. the human genome is approximately 3 GB).
Pygr takes care of all this for you automatically.  Note, as an entirely separate 
issue, that Pygr's cnestedlist
module uses the \class{long long} data type for file offsets and
the \function{fseeko()} POSIX interface for large file support (i.e. 64-bit
file sizes), which is supported by current versions of Linux, Mac OS X, etc;
otherwise, check if your filesystem supports this.

\end{itemize}

This functionality is encapsulated in the NLMSA class, which has a number of methods
and attributes.

Construction Methods:

\begin{funcdesc}{NLMSA}{pathstem='', mode='r', seqDict=None, mafFiles=None, axtFiles=None, maxOpenFiles=1024, maxlen=None, nPad=1000000, maxint=41666666, trypath=None, bidirectional=True, pairwiseMode= -1, bidirectionalRule=nlmsa_utils.prune_self_mappings, maxLPOcoord=None}
  Constructor for the class.  \var{pathstem} specifies a path and filename prefix for
  the NLMSA files (since multiple files are used to store one NLMSA, it will automatically add a
  number of suffixes automatically to open the necessary set of files for the NLMSA).
  \var{mode} is either ``r'' to open an existing NLMSA (from the \var{pathstem} disk files); 
  ``w'' to create a new one (which will be saved to the \var{pathstem} disk files);
  or ``memory'' to create a new in-memory NLMSA (i.e. stored in your computer's RAM
  instead of using files on your hard disk).  Obviously, this limits you to 
  the amount of RAM in your computer, but will make the NLMSA much, much faster.

  \var{seqDict} specifies a dictionary which maps sequence names to actual sequence
  objects representing those sequences.  If \var{seqDict} is None, the constructor
  will call \method{nlmsa_utils.read_seq_dict}() to try to obtain it from files
  associated with the NLMSA.  It first looks for a file \var{pathstem}\code{.seqDictP} 
  that is simply a pickle of the \var{seqDict} data.  If this is not found, it
  next looks for a file \var{pathstem}\code{.seqDict} that is a \class{prefixUnionDict}
  header file for opening all the sequence database files for you automatically.
  This header file will itself specify a list of sequence database files; the
  \var{trypath} option, if provided, specifies a list of directories in which to look for these
  sequence database files.

  The \var{bidirectional} option indicates whether you wish the NLMSA to
  save each input alignment relationship A:B in {\em both} possible directions
  (i.e. nlmsa[A] will yield B, and nlmsa[B] will yield A).  In general, the
  \var{bidirectional=True} mode is most appropriate for true multiple sequence
  alignments, i.e. where it is guaranteed that for a given pair of sequences A,B
  each interval of A maps to a unique interval in B, which in turn maps back
  to the same interval of A (and {\em only} that interval in A).  There are
  many possible scenarios where you might prefer \var{bidirectional=False} mode:
\begin{itemize}
\item When you WANT your alignment to have a specific directionality.  For example,
if \code{nlmsa} is a mapping of the human genome sequence onto the mouse genomic
sequence, then \code{nlmsa[s]} should only yield a result if \code{s} is a human
genome sequence interval; a mouse genome sequence interval should raise a \code{KeyError}.

\item When the input alignment data themselves give each A:B relationship in
both directions (i.e. the input data include both an A:B mapping and also a
B:A mapping).  Since the input data contain both directions of each mapping,
there is no need for the constructor code to save each input alignment
bidirectionally.  In this case \var{bidirectional=True} mode would cause duplicate
mappings to be saved (i.e. the A:B mapping would be saved twice, and the B:A mapping
would also be saved twice) and thus alignment queries would yield duplicated results.
In such a case, \var{bidirectional=False} prevents this problem.

\item A common example of this issue is when the
input alignment data may contain multiple, inconsistent alignments of
a given pair of sequences.  For example, a BLAST all-vs-all will return TWO alignments
of A,B: one when A is blasted against the database (finding B), and another when 
B is blasted against the database (finding A).  These two alignments could be different!
In this case, a \var{bidirectional=True} alignment would return BOTH alignments
(i.e. \code{nlmsa[A]} will return TWO alignments of B, which might be identical...
or might be significantly different).  This is undesirable behavior.  Instead,
use \var{bidirectional=False} so that \code{nlmsa[A]} will simply return the 
alignments that were found when A was blasted against the database.

\item In general, using \var{bidirectional=True} can yield multiple, potentially
inconsistent results when the input data are not a true multiple-sequence alignment
(e.g. BLAST alignment data is strictly pairwise, not a true multiple-sequence alignment).
\end{itemize}

  \var{pairwiseMode=True} indicates a PAIRWISE sequence alignment, in which
  the stored alignment relationships each consist of a pair of sequence intervals
  that are aligned.  Note: this pairwise format can store the alignment of {\em any}
  number of sequences, but the key point is that the individual alignment relations
  are pairwise, sequence-to-sequence.  The opposite model (\var{pairwiseMode=False})
  indicates a true MULTIPLE sequence alignment, in which the stored alignment
  relationships each consist of an integer coordinate interval (the alignment's internal
  coordinate system, for technical reasons called the ``LPO'') and a sequence
  interval that is aligned to it.  Under normal circumstances, you will not need
  to specify a value for the \var{pairwiseMode} option; the NLMSA will infer
  the correct setting automatically based on the input data.  Note: the pairwise format
  (\var{pairwiseMode=True}) and multiple alignment format (\var{pairwiseMode=False})
  cannot be mixed in a single NLMSA.  It must be either one format or the other.

  \var{mafFiles} can be used to specify a list of
  filenames containing a multiple sequence alignment in the UCSC MAF format,
  for saving as a new NLMSA (i.e. \var{mode='w'}).
  Note that this automatically sets \var{pairwiseMode}=False.  After the MAF
  data are read, it will automatically call the build() method to construct
  the alignment index files.

  \var{axtFiles} can be used to specify a list of
  filenames containing a set of pairwise alignments in UCSC axtNet format,
  for saving as a new NLMSA (i.e. \var{mode='w'}).
  Note that this automatically sets \var{pairwiseMode}=True.  After the axtNet
  data are read, it will automatically call the build() method to construct
  the alignment index files.

  \var{bidirectionalRule} allows the user to provide a function that has
  complete control over the desired \var{bidirectional} setting to use for
  each possible pair of sequence databases.  Currently, this is only used
  for \var{axtFiles} reading; the default method (\method{nlmsa_utils.prune_self_mappings})
  filters out duplicate mappings for a sequence database onto itself
  (since these are provided in both forward and reverse directions in the axtNet
  file), but stores mappings for one sequence database to another 
  bidirectionally (since the axtNet files give such mappings in only one direction
  normally).  To implement your own bidirectionalRule function, see
  \method{nlmsa_utils.prune_self_mappings}() as an example.

  \var{maxlen} specifies the maximum coordinate
  value for a union or LPO coordinate system.  Its default value is 2GB, to prevent \class{int} overflow.
  Using a smaller value can be useful, to 1) limit the size of the LPO in memory
  during initial construction, and 2) to limit the size of LPO database files on disk
  (if for example, your file system does not support files above some maximum size).
  During initial construction of the NLMSA (from MAF files or user-specified interval
  alignments), the algorithm performs a one-pass sort of the LPO intervals.  Thus,
  this set of intervals is briefly held in RAM for this sort.  If you have insufficient
  RAM, the construction step may raise a MemoryError.  If so, you can avoid this problem
  by using a smaller \var{maxlen} value.

  The \var{maxint} option provides another way of limiting the size of LPO
  databases.  It specifies the maximum number of intervals to store per LPO database.
  Since each interval takes 24 bytes, the default setting limits each LPO to
  a total size of 1 GB.  Note that the current NLMSA construction algorithm
  requires loading each database index into memory as one-time operation
  during construction.  If your NLMSA build fails due to running out of memory,
  simply reduce this value.

  The \var{nPad} option sets the maximum number of LPO coordinate systems
  (specifically, the offset for the start of real sequence IDs in the NLMSA
  sequence index).  You are unlikely to need to change this default value.

  \var{maxOpenFiles} limits the open file descriptors the NLMSA will use.
  {\em This option is no longer of much importance.  In versions prior to pygr 0.5,
  however, it was important because each sequence in the alignment had its
  own index file (in v.0.5 and later this problem is solved by unionization;
  for details see below)}.  Since
  each sequence has a separate nested list database file, a large multi-genome alignment
  (with each genome containing 20 different chromosomes, say) can rapidly open a large
  number of file descriptors.  Note: NLMSA only opens a given sequence's nested list database
  when one of your queries actually requires access to that sequence; it then
  keeps that file descriptor open to make subsequent queries to it fast.  If the number
  of open file descriptors would exceed \var{maxOpenFiles}, it will close other open
  database files, which may slow down query performance (due to having to open and close
  databases repeatedly to process queries).

\end{funcdesc}


\begin{funcdesc}{__iadd__}{sequence}
  As part of constructing an alignment, adds \var{sequence} to the alignment graph,
  so that you can subsequently save specific alignments of intervals of
  \var{sequence}, using code like \code{nlmsa[s]+=s2}, where \code{s} is
  an interval of \var{sequence} and \code{s2} is some other sequence interval.
  If \var{sequence} had not been added to the alignment, this later operation
  will raise a \code{KeyError}.
\end{funcdesc}

\begin{funcdesc}{addAnnotation}{annotation}
  adds an alignment relationship to \var{annotation} from its underlying
  sequence interval.  Note: to use this, the NLMSA must have been created with the
  {\em pairwiseMode=True} option.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{seqInterval}
  prepare to store an alignment relationship for the sequence interval \var{seqInterval},
  i.e. get a BuildMSASlice object representing \var{seqInterval}, to which you can
  then add other sequence intervals to align them.  I.e. \code{nlmsa[s1]+=s2}
  saves the alignment of intervals s1 and s2.
  You can also use a regular Python {\em slice} object using integer indices
  ie. \code{nlmsa[1:45]}, in which case, it indicates that 
  region of the LPO coordinate system.
  If the sequence containing
  interval \var{s2} is not already in the NLMSA, it will be added for you automatically
  (i.e. creating the necessary indexing, nested list database files, etc.).  In this
  case, the sequence must supply a unique string identifier, which will be used
  on subsequent attempts to open the NLMSA database, to match the individual sequence
  nested-list databases against corresponding sequence objects (using {\em seqDict},
  see above).
\end{funcdesc}


\begin{funcdesc}{build}{buildInPlace=True,saveSeqDict=False,verbose=True}
  to construct the final nested list databases,
  after all the desired alignment intervals have been saved (using the
  \method{iadd/getitem} above).  This method
  simply calls the build() method on all the constituent NLMSASequence objects
  in this alignment.  NOTE: you do not need to call \method{build}() if
  you provided a \var{mafFiles} constructor argument, since that automatically
  calls \method{build}().

  \var{buildInPlace=False} forces it to use an older NLMSA construction method
  (higher memory usage, but more tested).  The new in-place construction method
  (made the default in release 0.7) is described in the Alekseyenko \& Lee 2007
  paper published in {\em Bioinformatics}.

  \var{saveSeqDict=True} forces it to write the NLMSA's seqDict (dictionary
  of sequences that are included in the alignment) to disk.  This is unnecessary
  if you intend to store the NLMSA in pygr.Data, as pygr.Data will automatically
  save the NLMSA's seqDict as part of that process.  However, if you plan on
  re-opening the NLMSA directly from disk, you should save the seqDict 
  to disk by passing this option, or by directly calling the NLMSA's
  save_seq_dict() method.

  \var{verbose} controls whether the method will print explanatory 
  messages to stderr about the saveSeqDict=False mode.
  To suppress printing of these messages, use \var{verbose=False}.
\end{funcdesc}

\begin{funcdesc}{save_seq_dict}{}
  Forces saving of the NLMSA's seqDict to a disk file named 'FILESTEM.seqDictP'
  (where FILESTEM is the base path to your NLMSA files).  This is unnecessary
  if you intend to store the NLMSA in pygr.Data, as pygr.Data will automatically
  save the NLMSA's seqDict as part of that process.  The seqDictP file format
  is a pygr.Data-aware pickle; that is, references to any pygr.Data resources
  will simply be saved by their pygr.Data IDs, and loaded in the usual
  pygr.Data way.
\end{funcdesc}



Alignment Usage Methods:

\begin{funcdesc}{__getitem__}{s1}
  get the alignment slice for the sequence interval \var{s1},
  i.e. get an NLMSASlice object representing the set of intervals aligned to \var{s1}.
  You can also use a regular Python {\em slice} object using integer indices
  ie. \code{nlmsa[1:45]}, in which case, it gets the NLMSA slice corresponding to that 
  region of the LPO coordinate system.
\end{funcdesc}

\begin{funcdesc}{doSlice}{s1}
  If you subclass NLMSA and provide a \method{doSlice} method, the NLMSA will
  call your \method{doSlice(seq)} method to find alignment results for \code{seq},
  instead of querying its stored alignment data.  You can thus use this 
  to provide an NLMSA interface around virtually any source of alignment information
  that you have.  To see an example, see the \class{xnestedlist.NLMSAClient} class.  
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: This attribute provides a dictionary of the sequences in
the NLMSA, whose keys are top-level sequence objects, and whose values are
the associated NLMSASequence object for each sequence.  Ordinarily you will have
no need to access the NLMSASequence object directly; only do so if you know what
you're doing (details below).  This dictionary is of type NLMSASeqDict (see below).

\end{itemize}

\subsubsection{dump_textfile, textfile_to_binaries}
These two functions enable you to dump a constructed NLMSA binary database
to a platform-independent text format, and to restore an NLMSA binary database
from this text format.  This can be useful for 
\begin{itemize}
\item speeding up the process of installing an NLMSA database on multiple
machines.  Since the restore operation does not involve a build step, it 
can be substantially faster than building the NLMSA separately on each machine.

\item moving an NLMSA database from one machine to a machine with a different
binary architecture.  Since the binary database format depends on platform-specific
details (e.g. big-endian vs. little-endian integer representation), it is not
compatible between different architectures.

\item using an NLMSA database on a machine that has insufficient RAM memory
to perform the binary database build.  You can build the NLMSA binary database
on another machine with sufficient RAM, dump it to text, then restore it on
the desired machine where you wish to be able to use it.

\item using the text format to ``package'' an NLMSA database for distribution
on the Internet.  Users need only to obtain a single file and run a single command
to restore the NLMSA database.  Users only need sufficient disk space to hold
the NLMSA; they do not need large amounts of RAM (because they will not have to
perform a ``build'' step).

\end{itemize}

\begin{funcdesc}{dump_textfile}{pathstem,outfilename=None,verbose=True}
  Dumps a text representation of an existing NLMSA binary database.
  \var{pathstem} must be the path to the NLMSA.  For
  example if you have an NLMSA database index file \code{/loaner/hg17_NLMSA/hg17_msa.idDict}
  (and many other index files with different suffixes),
  then you would supply a \var{pathstem} value of \code{/loaner/hg17_NLMSA/hg17_msa}.

  \var{outfilename} gives the path for the output text file into which the
  NLMSA database will be dumped.  If None, it will default to \var{pathstem} with a
  \code{.txt} suffix added.

  Setting \var{verbose=False} will prevent printing of warning messages
  to stderr (for details about possible warnings, see below).

  Note: \method{dump_textfile} attempts to save information about the seqDict 
  (or, alternatively, the PrefixUnionDict dictionary of multiple sequence
  databases), using their pygr.Data IDs if possible.
  Specifically, for a PrefixUnionDict (i.e. multiple sequence databases in
  one NLMSA), it saves a dictionary of the prefixes
  for each sequence database in the NLMSA, with its pygr.Data ID if it has one.
  Assigning a pygr.Data ID to each sequence database has the great advantage that
  the reconstruction method \method{textfile_to_binaries}() can simply request
  pygr.Data for these IDs on the destination machine, automatically.  By contrast,
  if a sequence database has no pygr.Data ID, the user will have to supply that
  sequence database manually on the destination machine.  In this case, 
  \method{dump_textfile} will print a warning message to stderr explaining
  what the user must do.  This provides yet another reason why it's a good idea
  to assign a pygr.Data ID to any sequence database that is a well-defined,
  commonly used public resource.
\end{funcdesc}


\begin{funcdesc}{textfile_to_binaries}{filename,seqDict=None,prefixDict=None}
  Creates an NLMSA binary database from input text file \var{filename}.
  The NLMSA binary database will be created in the current directory,
  and will be given the same name as it originally had prior to being dumped to text.
  Since no build is required, this function does not require significant amounts
  of RAM memory.

  Handling of sequence databases: \method{textfile_to_binaries} will attempt to 
  obtain any needed sequence databases using their pygr.Data ID if assigned.
  If you obtain a \class{PygrDataNotFoundError}, this simply means that one
  of the pygr.Data IDs was not found in any of your pygr.Data resource
  databases.  In this case, you must either add it to one of your resource
  databases, or add a resource database that does contain it to your PYGRDATAPATH,
  then re-run \method{textfile_to_binaries}.

  On the other hand, if any of the needed sequence databases were NOT assigned
  a pygr.Data ID, then you will have to provide that sequence database(s)
  manually to the \method{textfile_to_binaries}() function, either via
  its \var{seqDict} argument (if the NLMSA contains only one sequence database),
  or via its \var{prefixDict} argument (if the NLMSA contains multiple sequence
  databases).  If you do not
  do so, an appropriate error will be raised, explaining what you need to do.
  The \var{prefixDict} argument must be a dictionary whose keys match 
  individual sequence database prefixes in the original NLMSA PrefixUnionDict,
  and whose associated values are the appropriate sequence database to use
  for each specified prefix.  You only need to provide those sequence databases
  that \method{textfile_to_binaries}() is unable to obtain from pygr.Data.
  When in doubt, just run \method{textfile_to_binaries}() without the \var{prefixDict}
  argument, and it will raise an error message listing the prefixes that you
  need to provide.
\end{funcdesc}



\subsubsection{xnestedlist.NLMSAServer, xnestedlist.NLMSAClient}
These two classes, provided by the separate \module{xnestedlist} module,
provide an XMLRPC client-server mechanism for querying NLMSA databases
over a network.  

\class{NLMSAServer} is constructed exactly the same as a normal \class{NLMSA};
it {\em is} a normal NLMSA with just two methods added for serving XMLRPC client
requests.  See the \class{coordinator.XMLRPCServerBase} reference 
documentation below for details about starting an XMLRPC server.

\class{NLMSAClient} provides a read-only client interface for querying
data in a remote \class{NLMSAServer}.  It takes two extra arguments for
its constructor: \var{url}, the URL for the XMLRPC server; \var{name},
the name of the NLMSAServer server object in the XMLRPC server's dictionary.
For example, to use an NLMSA stored on a remote XMLRPC server,
assuming that \code{myPrefixUnion} stores a dictionary of all the
sequence databases used by that NLMSA alignment, would just be:
\begin{verbatim}
from pygr import xnestedlist
nlmsa = xnestedlist.NLMSAClient(url='http://leelab.mbi.ucla.edu:5000',
                                name='ucsc17',seqDict=myPrefixUnion)
\end{verbatim}


\subsubsection{NLMSASlice}
A temporary object created on-the-fly to represent (an interface to provide 
information about) the portion of the alignment associated with a specific
sequence interval.  This is the main class for querying information about
alignments, and provides a number of useful methods for getting 
detailed information about alignment relationships.

In addition, the NLMSASlice is the basic unit of {\em sequence caching}
control, by which you can ensure that pygr alignment analysis accesses
sequence databases in the most efficient way.  Here's how it works:
\begin{itemize}
\item When you perform an NLMSA query by creating an NLMSASlice, it assembles
a list of covering intervals for all sequences in this part of the alignment
(i.e. for each sequence, the smallest interval that contains all of its
aligned intervals in this NLMSASlice).  

\item NLMSASlice then attempts to call the \code{cacheHint} method for each
sequence database object containing the relevant sequences (if this method 
exists; if it doesn't, this step is skipped).  It passes the \code{cacheHint} method 
the covering interval information for the aligned sequence, and a reference to 
itself (the NLMSASlice object) as the {\em owner} of this cache hint.

\item If any operation subsequently attempts to access the actual sequence
for any interval that is contained within this covering interval, the sequence
database will instead load the entire covering interval, which it stores in 
its cache, associated with the specified {\em owner}.  It then returns the
appropriate subinterval of sequence requested, as usual.

\item Any subsequent requests for sequence strings that fall within this 
covering interval will simply be obtained from this cache, instead of 
retrieving the sequence from disk files.

\item This cache information is retained until the {\em owner} (in this case,
the original NLMSASlice) is deleted (by Python garbage collecting).  Thus, to
control sequence caching, all you have to do is hold on to the NLMSASlice as
long as you want to work with its associated sequence intervals.  As soon as 
you drop it, its associated cache information will also be automatically deleted,
freeing up memory.
\end{itemize}

An NLMSASlice acts like a dictionary whose keys are
sequence intervals that are aligned to this region, and whose values are
\class{Seq2SeqEdge} objects providing detailed information about the alignment of
the target interval (key) to the source interval (the sequence interval
used to create the NLMSASlice in the first place).  You can use this
dictionary interface in several ways:


\begin{funcdesc}{__iter__}{}
  iterates over all sequence intervals that have
  a 1:1 mapping (i.e. a block of alignment containing no indels) to
  all or part of the source interval.
\end{funcdesc}


\begin{funcdesc}{keys}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeMost=False, mergeAll=False, maxsize=500000000, minAlignSize=None, maxAlignSize=None, pIdentityMin=None, ivalMethod=None, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0., seqMethod=None, **kwargs}
  Provides a more general interface than {\em iter()}, with two types of 
  group-by capabilities, ``group-by'' operations on the alignment intervals
  contained within this slice (``horizontal'' grouping), 
  and on the sets of sequences aligned
  to this slice (``vertical'' grouping).

  1. ``group-by'' operations on the alignment intervals
  contained within this slice.  It allows the user to supply
  various parameters for controlling when alignment intervals will be
  merged or split in the results that it returns.

  \var{mergeAll}
  forces it to combine intervals of a given sequence irrespective
  of the size of gaps or inserts separating them.

  \var{mergeMost}
  forces it to combine intervals of a given sequence, within reason
  (but don't merge a whole chromosome if you get one interval from one end
  and one interval from the other end:
  \var{maxgap=maxinsert=10000, mininsert=-10, maxsize=50000}).

  \var{maxgap} sets the
  maximum gap size for merging two adjacent intervals.  If the target sequence
  for the two alignment intervals has a gap longer than \var{maxgap} 
  letters between the two alignment intervals, they will be returned as
  separate intervals; otherwise they will be merged as a single alignment
  region.

  \var{maxinsert} sets the maximum length of insert in the target
  sequence that allows to adjacent intervals to be merged as a single alignment
  region in the results.

  \var{mininsert} is specifically for handling
  alignments that may have small ``cycles'' (due to slight inconsistencies
  in the reported alignment intervals, for example, if a portion of sequence
  can align at both the end of one interval or at the beginning of another, and
  the intervals are actually added to the NLMSA that way, then the {\em start}
  of the second interval will actually be {\em before} the {\em stop} of 
  the first interval; this corresponds to a negative insert value).  A
  \var{mininsert} value of zero (the default), prevents any such interval
  pairs from being merged.  Giving a negative \var{mininsert} value will allow
  interval pairs whose insert value is greater than or equal to this value, 
  to be merged.

  \var{maxsize}: upper bound on maximum size for interval merging.

  \var{filterSeqs}, if not None, should be a dict of sequences
  used to filter the group-by analysis; i.e. only alignment intervals 
  containing these sequences are considered in the analysis.  More
  specifically, \var{filterSeqs} can be used to mask the group-by analysis
  to a specific interval of a sequence, by having \var{filterSeqs}
  return only the intersection between the interval it is passed as a key,
  and the masking interval that it stores.  If there is no overlap, it
  must raise \code{KeyError}.  The \class{sequence.SeqFilterDict} class
  provides exactly this masking capability, i.e.
\begin{verbatim}
d = sequence.SeqFilterDict(someIntervals)
overlap = d[ival] # RETURNS INTERSECTION BETWEEN ival AND someIntervals, OR KeyError
\end{verbatim}
  \var{minAlignSize} if not None, sets a minimum size for filtering the resulting
  alignment regions.  Regions smaller than the specified size will be culled
  from the output.  

  \var{maxAlignSize} if not None, sets a maximum size for filtering the resulting
  alignment regions.  Regions larger than the specified size will be culled
  from the output.  

  \var{pIdentityMin} if not None, sets a minimum fractional sequence identity
  for filtering the resulting alignment regions.  Regions with lower levels
  of identity will be clipped from the output.  Specifically, within each
  region, the largest contiguous segment (possibly including indels, if
  permitted by \var{maxgap} and \var{maxinsert}) whose sequence identity is above the
  threshold will be returned (but only if it is larger than \var{minAlignSize}
  if set).  

  \var{ivalMethod},
  if not None, allows the user to provide a Python function that performs
  interval grouping.  Specifically it is called as
  \function{ivalMethod(l, ns,msaSlice=self, **kwargs)}, where \var{l} is the
  list of intervals for NLMSASequence \var{ns} within the current slice 
  \var{msaSlice}; all other args are passed as a dict in \var{kwargs}.

  2. merge groups of sequences using "vertical" group-by rules.
  \var{seqGroups}: a list of one or more lists of sequences to group.
  If None, the whole set of sequences will be treated as a single group.
  Each group will be analyzed separately, as follows:

  \var{sourceOnly}: output intervals will be reported giving only
  the corresponding interval on the source sequence; redundant
  output intervals (mapping to the same source interval) are
  culled.  Has the effect of giving a single interval traversal
  of each group.

  \var{indelCut}: for \var{sourceOnly} mode, do not merge separate 
  intervals that the groupByIntervals analysis separated due to an indel).

  \var{minAligned}: the minimum number of sequences that must be aligned to
  the source sequence for masking the output.  Regions below
  this threshold are masked out; no intervals will be reported
  in these regions.

  \var{pMinAligned}: the minimum fraction of sequences (out of the
  total in the group) that must be aligned to the source
  sequence for masking the output.

  \var{seqMethod}: you may supply your own function for grouping.
  Called as \function{seqMethod(bounds,seqs,**kwargs)}, where
  \var{bounds} is a sorted list of
  \var{(ipos,isStart,i,ns,isIndel,(start,end,targetStart,targetEnd))}
  and \var{seqs} is a list of sequences in the group.
  Must return a list of \var{(sourceIval,targetIval)}.  See the docs.

\end{funcdesc}


\begin{funcdesc}{iteritems}{**kwargs}
  same keys as {\em iter}, but for each provides the source interval
  to target interval mapping (\class{Seq2SeqEdge}).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{edges}{**kwargs}
  same interval mappings as {\em iteritems}, but for
  each provides a tuple of three objects:
  the source interval, the corresponding target interval,
  and the \class{Seq2SeqEdge} providing detailed
  information about the alignment between the source and target intervals
  (such as percent identity, etc.).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{__getitem__}{s1}
  treats \var{s1} as a key (target sequence
  interval), and returns an \class{Seq2SeqEdge} object providing detailed
  information about the alignment between this target interval
  and the source interval.
\end{funcdesc}


\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences that
  are aligned to the source interval.  {\em Note}: this is NOT necessarily 
  equal to the number of items that will be returned by the above iterators,
  since a single target sequence might have multiple 1:1 intervals of
  alignment to the source interval, due to indels.
\end{funcdesc}



In addition to these standard dictionary methods, NLMSASlice provides
several additional methods and attributes:


\begin{itemize}
\item
\member{letters}: this attribute provides an interface to 
the individual alignment columns (NLMSANode objects) containing the
source interval, in order from \var{start} to \var{stop}.  This provides
an easy way to obtain detailed information about the letter-to-letter
alignment of different sequences within this region of the alignment.
For details on the kinds of information you can obtain for each
alignment column, see NLMSANode, below.

It also provides a graph interface to subset of the partial order alignment 
graph corresponding to this slice.  For details, see NLMSASliceLetters, below.
\end{itemize}

\begin{funcdesc}{split}{**kwargs}
  this method provides a way to perform group-by operations on the slice;
  the output of split() is one or more NLMSASlice objects; if the
  group-by analysis results in no splitting of the current slice, then
  it is returned unchanged (i.e. the method just returns {\em self}).
  Uses same group-by arguments as \method{keys()}.
  For further details on group-by operations, see \method{keys()} above.
\end{funcdesc}

\begin{funcdesc}{regions}{**kwags}
  performs the same group-by analysis as {\em split()}, but replaces
  the source interval by the corresponding interval in the LPO.  The main
  practical consequence of this is that target sequence {\em inserts}
  are included in the resulting slice (because they are present in the LPO
  interval corresponding to the original source interval), whereas they
  were NOT included in the original slice (because they are not aligned
  to the source interval).  The main place where this matters is in graph
  traversal of the slice's {\em letters} attribute: whereas the nodes
  and edges corresponding to these inserts are not considered to be part
  of the {\em letters} graph for the original slice, they {\em are} part of the
  LPO slice.  Also, the ``source interval'' in any subsequent operations
  with the LPO slice will be LPO coordinates instead of subintervals of the
  original source sequence interval.
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}

\begin{funcdesc}{groupByIntervals}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeMost=False, maxsize=500000000, mergeAll=True, ivalMethod=None, pIdentityMin=None, minAlignSize=None, maxAlignSize=None,**kwargs}
  This method performs the interval grouping analysis for all the iterators
  described above.  Users will not need to call it directly.  Its arguments
  are described above (see \method{keys()}).  It returns a dictionary
  whose keys are sequences aligned to this slice (represented by their integer nlmsa_id),
  and whose values are
  the list of intervals produced by the group-by analysis for the corresponding
  sequence.  The values are tuples of the form
  \var{(source_start, source_stop, target_start, target_stop)}, showing the
  mapping of a source sequence interval onto a target sequence interval.
  This dictionary is the primary input to the \method{groupBySequences()}
  method below.
\end{funcdesc}

\begin{funcdesc}{filterIvalConservation}{seqIntervals,pIdentityMin=None,filterFun=None,**kwargs}
  This method is used by \method{groupByIntervals}() to filter the results
  using the specified \var{filterFun} filter function, which should either
  return \var{None} if the specified alignment region does not pass the filter,
  or return the filtered interval.  For an example
  filter function, see \method{conservationFilter}, which is used by default 
  in \method{filterIvalConservation}.  \var{seqIntervals} must be passed in
  the same format as expected by \method{groupBySequences}; it is modified in
  place by \method{filterIvalConservation}, which always returns \var{None}.
\end{funcdesc}

\begin{funcdesc}{conservationFilter}{seq,m,pIdentityMin=None,minAlignSize=None,maxAlignSize=None,**kwargs}
  Tests an alignment mapping \var{m} for the specified size and sequence 
  identity criteria.  Returns the (possibly clipped) interval \var{m} if
  the criteria are met, and \var{None} if the criteria are not met.  \var{m}
  is expected to be a tuple of integers \code{(srcStart,srcEnd,destStart,destEnd)}.
  \var{seq} must be the destination sequence object (sliceable by the destination
  interval coordinates).  The conservation criteria and clipping are performed
  using \method{Seq2SeqEdge.conservedSegment}().
\end{funcdesc}

\begin{funcdesc}{groupBySequences}{seqIntervals, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0., seqMethod=None, **kwargs}
  This method performs the sequence grouping analysis for all the iterators
  described above.  \var{seqIntervals} must be a dictionary of sequences
  and their associated list of intervals (produced by \method{groupByIntervals()}
  above).  It returns a list of output sequence intevals, which is either
  a list of source sequence intervals (\var{sourceOnly} mode), or a list
  of tuples of the form \var{(source_interval, target_interval)}.
\end{funcdesc}


\begin{funcdesc}{matchIntervals}{seq=None}
  this method returns the set of
  1:1 match intervals for the target sequence {\em seq} (or all
  aligned sequences, if {\em seq} is None), as a dictionary
  whose keys are target sequence intervals, and whose values are
  the corresponding source sequence intervals to which they are
  aligned.
\end{funcdesc}

\begin{funcdesc}{findSeqEnds}{seq}
  returns the largest possible interval of
  {\em seq} that is aligned to this slice, i.e. it merges all 
  alignment intervals in this slice containing {\em seq}, and
  returns the merged sequence interval based on the minimum {\em start}
  value and maximum {\em stop} value found.
\end{funcdesc}

\subsubsection{NLMSASliceLetters}
represents the {\em letters} graph of a specific NLMSASlice.  It is
a graph whose nodes are the NLMSANode objects in this slice, and whose
edges are sequence.LetterEdge objects. {\em Note}: currently the edge objects
are just returned as None -- please implement!

This graph has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the nodes in the slice, in order from left to right.
\end{funcdesc}

\begin{funcdesc}{items}{}
  also \method{iteritems()}. Generate the same set of nodes as above,
  as keys, but for each also returns a value representing its outgoing
  directed edges (see getitem, below).
\end{funcdesc}

\begin{funcdesc}{__getitem__}{node}
  gets a dictionary indicating all the outgoing
  directed edges from {\em node} to subsequence nodes, whose keys are
  the target nodes, and whose edges are the 
  \class{sequence.LetterEdge} objects representing each edge.
\end{funcdesc}

\subsubsection{NLMSANode}
A temporary object (created on-the-fly) 
representing a single letter ``column'' in the alignment.  It acts like
a container of the sequence letters aligned to the source sequence in
this column.  It has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the individual sequence letters 
  (as SeqPath intervals, presumably of length 1) that are aligned to 
  the source sequence, in this column of the alignment.
\end{funcdesc}

    
\begin{funcdesc}{edges}{}
  generates the same list of of target sequence letters as
  the iterator, but as a tuple of (target letter, source letter, edge).
  Currently, edge is just None.
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences aligned to
  the source interval, in this column.
\end{funcdesc}

Other, internal methods that regular users are unlikely to need:

\begin{funcdesc}{getSeqPos}{seq}
  returns the sequence interval of \var{seq}
  that is aligned to this column, or raises \code{KeyError} if it is not
  aligned here.
\end{funcdesc}


\begin{funcdesc}{getEdgeSeqs}{node2}
  returns a dictionary of sequences
  that traverse the edge directly from this node to {\em node2},
  i.e. if letter {\em i} of seq is aligned to this node, then
  letter letter {\em i+1} is aligned to {\em node2}.  The
  dictionary's keys are top-level sequence objects, and its
  value for each is the letter position index {\em i} as defined above.
\end{funcdesc}

\begin{funcdesc}{nodeEdges}{}
  returns a dictionary of the outgoing edges
  from this node, whose keys are target nodes, and whose values
  are the corresponding edge objects (of type sequence.LetterEdge).
\end{funcdesc}


\subsubsection{NLMSASequence}
You are unlikely to need to manipulate NLMSASequence objects directly;
they perform the back-end work for accessing the nested list disk storage
of the alignment of the associated sequence.

However, one thing you should know is that for a sequence to be stored
in a NLMSA, it needs to have a unique string identifier.
NLMSASequence obtains a string identifier for the sequence in one of the following
ways (in decreasing order of precedence): 1) the sequence ``object'' can itself just
be a Python string, in which case that string is used as the identifier. 2) otherwise,
the object should be a SeqPath instance.  If it has a {\em name} attribute, that will
be used as the identifier. 3) Otherwise, if it has a {\em id} attribute (which is present
by default on sequence.Sequence objects), that will be used.




\subsection{Seqdb Module}
\label{seqdb}

{\em Pygr interface to sequence databases stored in FASTA, BLAST or relational databases.}


\subsubsection{Overview}

The seqdb module provides a simple, consistent interface to sequence databases from a variety of different storage sources such as FASTA, BLAST and relational databases.  Sequence databases are modeled (like other Pygr container classes) as dictionaries, whose keys are sequence IDs and whose values are sequence objects.  Pygr sequence objects use the Python sequence protocol in all the ways you'd expect: a subinterval of a sequence object is just a Python slice (s[0:10]), which just returns a sequence object representing that interval; the reverse complement is just -s; the length of a sequence is just len(s); to obtain the actual string sequence of a sequence object is just str(s).  Pygr sequence objects work intelligently with different types of back-end storage (e.g. relational databases or BLAST databases) to efficiently access just the parts of sequence that are requested, only when an actual sequence string is needed.

\subsubsection{External Requirements}
This module makes use of several external programs:
\begin{itemize}
\item
{\em NCBI toolkit}: The BLAST database functionality in this module 
requires that the NCBI toolkit
be installed on your system.  Specifically, some functions will call the command line
programs \code{formatdb}, \code{fastacmd}, \code{blastall}, and \code{megablast}.

\item
{\em RepeatMasker}: the \method{BlastDB.megablast()} method calls the command line
program \code{RepeatMasker} to mask out repetitive sequences from seeding alignments,
but to allow extension of alignments into masked regions.

\item
{\em Python DB-API 2.0}: the \class{SQLTable} class, and dependent classes such as 
\class{SQLSequence} and \class{StoredPathMapping}, conform to the Python DB-API 2.0.
Typically you must supply a DB-API 2.0-compliant database cursor to the 
\class{SQLTable} constructor.  To do so, you must have some DB-API 2.0-compliant
module (such as \module{MySQLdb}) installed for connecting to a database server.
\end{itemize} 

If you are lacking one or more of these requirements, you can still install Pygr
and use all Pygr functionality that does not depend on the missing requirements.
If you try to use a function for which a requirement is missing, Pygr will raise
an appropriate exception (e.g. unable to run \code{blastall}).

\subsubsection{BlastDB}
Interface to an existing BLAST database or FASTA sequence file; in the latter case, it will automatically construct BLAST database files for you using the NCBI tool formatdb. Here's a simple example of opening a BLAST database and searching it for matches to a specific piece of sequence:

\begin{verbatim}
from pygr.sequence import *
from pygr.seqdb import *
db = BlastDB('sp') # OPEN SWISSPROT BLAST DB
s = Sequence(str(db['CYGB_HUMAN'][40:-40]),'boo')
m = db.blast(s) # DO BLAST SEARCH
myg = db['MYG_CHICK']
for i in m[s][myg]:
    print repr(i.srcPath),repr(i.destPath),i.blast_score,i.percent_id
\end{verbatim}

Let's go through this example line by line:

\begin{itemize}

\item    
construction of a BlastDB object: This looks for either a FASTA file with the path 'sp' or BLAST database formatted files based on this path (e.g. 'sp.psd' for protein sequences, or 'sp.nsd' for nucleotide sequences).

\item
db['CYGB_HUMAN'] obtains a sequence object representing the SwissProt sequence whose ID is CYGB_HUMAN.  The slice operation [40:-40] behaves just like normal Python slicing: it obtains a sequence object representing the subinterval omitting the first 40 letters and last 40 letters of the sequence.  The str() operation obtains the actual string representation of this subinterval.

\item
Sequence(letter_string, name) creates a new sequence object whose sequence is letter_string, and whose ID is name.

\item
Running the db.blast(s) method searches the BLAST database for homologies to s, using NCBI BLAST.  It chooses reasonable parameters based upon the sequence types of the database and supplied query.  However, you can specify extra parameter options if you wish.  It returns a Pygr sequence mapping (multiple alignment) that represents a standard Pygr graph of alignment relationships between s and the homologies that were found.

\item
The expression m[s][myg] obtains the "edge information" for the graph relationship between the two sequence nodes s and myg.  (if there was no edge in the m graph representing a relationship between these two sequences, this would produce a 
\code{KeyError}).  This edge information consists of a set of interval alignment relationships (described in detail below), which are printed out in this example.

\end{itemize}

Options for constructing a BlastDB:

\begin{funcdesc}{BlastDB}{filepath=None,skipSeqLenDict=False,ifile=None,idFilter=None,
                 blastReady=False,blastIndexPath=None,blastIndexDirs=None,**kwargs}
  Open a sequence file as a ``database'' object, giving the user access to its sequences,
  easy searching via \code{blast} or \code{megablast}, etc.
  \var{skipSeqLenDict} prevents construction of a sequence length index file 
  (which will be named ``{\em filepath}.\code{seqlen}'') and a fast
  sequence access file (which will be named ``{\em filepath}.\code{pureseq}'').
  Setting this option to \code{True} can be useful if you either wish to
  speed up initial opening of the BlastDB (note: construction of these indexes is
  only a one-time event) or avoid the extra disk space required by these indexes.
  {\em Explanation}: To facilitate the rapid creation of sequence objects (which requires the length of the sequence), it creates a sequence length index (as a Python shelve).  This enables it to avoid actually loading the sequence string into memory each time a sequence object is created; instead it just looks up the sequence length.  While this speeds up access to genomic sequence databases (where each sequence tends to be extremely long), this initial step may be slow for databases of short sequences.  Setting \var{skipSeqLenDict} option to \code{True}, will prevent construction of this sequence length index.
  \var{ifile} lets you open the database directly from a file object rather
  than a filename.  If you have a file object, you can pass it directly to BlastDB instead of a filepath.  NB: the BlastDB() constructor will close ifile when it is done reading from the file object.
  \var{idFilter} allows you to provide a function for re-mapping the FASTA sequence
  identifiers read from the sequence file.  This can be useful in the case of
  NCBI FASTA files, since NCBI often treats the sequence ID as a ``blob'' into
  which any number of database fields can be stuffed, rather than a true ID.
  \var{blastReady} option specifies whether BLAST index files should be automatically
  constructed (using \code{formatdb}).  Note, if you attempt to run the \code{blast}()
  method, it will automatically create the index files for you if they are missing.
  \var{blastIndexPath}, if not None, specifies the path to the BLAST index
  files for this database.  For example, if the BLAST index files are
  \code{/some/path/foo.psd} etc., then \var{blastIndexPath}\code{='/some/path/foo'}.
  \var{blastIndexDirs}, if not None, specifies a list of directories in which
  to search for and create BLAST index files.  Entries in the list can be
  either a string, or a function that takes no parameters and returns 
  a string path.  A string value ``FILEPATH'' instructs it to use the 
  filepath of the FASTA file associated with the BlastDB.
  The default value of this attribute on the \class{BlastDB} class is 
\begin{verbatim}
['FILEPATH',os.getcwd,os.path.expanduser,pygr.classutil.default_tmp_path]
\end{verbatim}
This corresponds to: self.filepath, current directory, the user's HOME
directory, and the default temporary directory used by the Python 
function \method{os.tempnam}().
\end{funcdesc}

Useful methods:

\begin{funcdesc}{iter}{}
  iterate over all IDs in the BLAST database.
\end{funcdesc}

\begin{funcdesc}{len}{}
  returns number of sequences in the BLAST database.
\end{funcdesc}

\begin{funcdesc}{blast}{seq, al=None, blastpath='blastall', blastprog=None, expmax=0.001,
maxseq=None, opts='', verbose=True}
  run a BLAST search on sequence object seq.
  \var{maxseq} will limit the number of returned hits to the best \var{maxseq} hits. 
  \var{al} if not None, must be an alignment object in which you want the results
  to be saved.  Note: in this case, the \method{blast} function will not automatically
  call the alignment's \method{build}() method; you will have to do that yourself.
  \var{blastpath} gives the command to run BLAST.
  \var{blastprog}, if not None, should be a string giving the name of the BLAST
  program variant you wish to run, e.g. 'blastp' or 'blastn' etc.  If None,
  this will be figured out automatically based on the sequence type of \var{seq}
  and of the sequences in this database.
  \var{expmax} should be a float value giving the largest ``expectation score''
  you wish to allow homology to be reported for.
  \var{opts} allows you to specify arbitrary command line arguments to the BLAST
  program, for controlling its search parameters.
  \var{verbose=False} allows you to switch off printing of explanatory messages to 
  stderr.
\end{funcdesc}

\begin{funcdesc}{megablast}{seq, al=None, blastpath='megablast', expmax=1e-20,
maxseq=None, minIdentity=None, maskOpts='-U T -F m',
rmPath='RepeatMasker', rmOpts='-xsmall', opts='', verbose=True}
  first performs repeat masking on the sequence by converting repeats to lowercase,
  then runs megablast with command line options to prevent seeding new alignments
  within repeats, but allowing extension of alignments into repeats.
  In addition to the blast options (described above),
  \var{minIdentity} should be a number (maximum value, 100)
  indicating the minimum percent identity for hits to be returned.
  \var{rmPath} gives the command to use to run RepeatMasker.
  \var{rmOpts} allows you to give command line options to RepeatMasker.
  The default setting causes RepeatMasker to mark repetitive regions in the
  query in lowercase, which then works in concert with the \var{maskOpts} option, next.
  \var{maskOpts} gives command line options for controlling the megablast program's
  masking behavior.  The default value prevents megablast from using repetitive
  sequence as a seed for starting a hit, but allows it to propagate a regular
  (non-repetitive hit) through a repetitive region.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  The invert operator (\textasciitilde, the ``tilde'' character) 
  enables reverse-mapping of sequence objects to their string ID.
\begin{verbatim}
id = (~db)[seq] # GET IDENTIFIER FOR THIS SEQUENCE FROM ITS DATABASE
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{formatdb}{filepath=None}
  Forces the BlastDB to construct new BLAST index files, either at the
  location specified by \var{filepath}, if not None, or in the first
  directory in the \member{blastIndexDirs} list where the index files
  can be succesfully built.  Index files are generated using the 
  \code{formatdb} program provided by NCBI, which must be in your
  PATH for this method to work.
\end{funcdesc}



Useful attributes:
\begin{itemize}

\item
\member{itemClass}: the object class to use for instantiating new sequence objects from this BLAST database.  You can set this to create customized sequence behaviors.  
This is used by \class{pygr.Data} to propagate correct attribute schemas to
items / slices from database containers managed by it.

\item
\member{itemSliceClass}: the object class to use for instantiating new sequence slice objects (i.e. subintervals of sequences from this BLAST database).  You can set this to create customized sequence behaviors.
This is used by \class{pygr.Data} to propagate correct attribute schemas to
items / slices from database containers managed by it.

\item
\member{filepath}: the location of the FASTA sequence file upon which
this \class{BlastDB} is based.

\item
\member{blastIndexPath}: if present, the location of the BLAST index files
associated with this \class{BlastDB}.  If not present, the location is assumed
to be the same as the FASTA file.

\item
\member{blastIndexDirs}: the list of directories in which to search for
or build BLAST index files for this \class{BlastDB}.  For details, see
the explanation for the constructor method, above.

\end{itemize}

\subsubsection{BlastDBXMLRPC}
A subclass of \class{BlastDB} that adds a couple methods needed to serve
the data to clients connecting over XMLRPC.  For example, to make an XMLRPC
server for a blast database, accessible on port 5020:
\begin{verbatim}
import coordinator
server = coordinator.XMLRPCServerBase(name,port=5020)
db = BlastDBXMLRPC('sp') # OPEN BlastDB AS USUAL, BUT WITH SUBCLASS
server['sp'] = db # ADD OUR DATABASE TO THE XMLRPC SERVER
server.serve_forever() # START SERVING XMLRPC REQUESTS, UNTIL KILLED.
\end{verbatim}

\subsubsection{XMLRPCSequenceDB}
Class for a client interface that accesses a Blast database over
XMLRPC (from the the \class{BlastDBXMLRPC} acting as the server).
\begin{funcdesc}{__init__}{url,name}
  \var{url} must be the URL (including port number) for accessing the 
  XMLRPC server; \var{name} must be the key of the BlastDBXMLRPC object
  in that server's dictionary (in the example above, it would be 'sp').
  Thus to access the server above (assuming it is running on leelab port 5020):
\begin{verbatim}
db = XMLRPCSequenceDB('http://leelab:5020','sp')
hbb = db['HBB_HUMAN'] # GET A SEQUENCE OBJECT FROM THE DATABASE...
\end{verbatim}
\end{funcdesc}
Currently, this class provides sequence access.  You can work with sequences
exactly as you would with a \class{BlastDB}, but cannot perform actual BLAST searches
(i.e. the \method{blast} and \method{megablast} methods don't work over XMLRPC).

\subsubsection{FileDBSequence}
The default class for sequence objects returned from BlastDB.  It provides efficient,
fast access to sequence slices (subsequences).  When the BlastDB is initially opened,
Pygr constructs a length and offset index that enables FileDBSequence to \code{seek()}
to the correct location for any substring of the sequence.  New in Pygr 0.4.

\subsubsection{BlastSequence}

This was previously the default class for sequence objects returned from BlastDB,
but has been deprecated because we found that NCBI \code{fastacmd} was much too slow
and consumed enormous amounts of memory.  \class{BlastSequence} relies on
\code{fastacmd} for ``fast'' access to individual sequence slices.  The advantage is
that it only requires BLAST database files (produced by Pygr using \code{formatdb}),
whereas the new \class{FileDBSequence} requires a specially indexed sequence file
(constructed by default by BlastDB), which may be a disadvantage if you are low
on disk space.

\class{BlastSequence} has several optimizations for working with BLAST databases:

\begin{itemize}
\item
it uses the NCBI tool fastacmd to retrieve sequence efficiently from a BLAST database, when your program requests an actual string of sequence text.  Moreover, for subintervals (slices) of the sequence, it uses fastacmd's -L option to request just the desired subinterval of the sequence, rather than the whole sequence.  This makes it efficient for requesting specific intervals of large genomic contigs.  Basically, just use Python slicing and str() methods on sequence objects, and subsequences will be obtained in an efficient manner.

\item 
the len() method is implemented using the seqLenDict, a precalculated index of the sequence lengths.  So again no sequence has to be read by Python.

\end{itemize}

\subsubsection{AnnotationDB}
This class provides a general interface for sequence annotation databases.
This interface follows several principles:
\begin{itemize}
\item An {\em annotation object} acts like a sliceable interval
(representing the region of sequence that it annotates)
with annotation attributes that provide further information or relations
for that annotation.  An annotation object always has three identifying attributes:
\member{db}, which gives the \class{AnnotationDB} object containing this
annotation;
\member{id}, which gives the unique identifier of this annotation within
its \class{AnnotationDB};
and \member{annotationType}, which gives a string identifier for the
type of annotation, e.g. ``exon''.
All slices derived from an annotation object retain its
\code{db}, \code{id} and \code{annotationType} attributes.  

\item An annotation will generally have additional attributes that
describe its specific biological information; for example,
a gene annotation might have a \member{symbol} attribute giving
its gene symbol.  These annotation-specific attributes are provided
by a \var{sliceDB}; see below for details. 

\item You can always obtain the actual sequence object corresponding
to an annotation object or slice, by simply requesting its
\member{sequence} attribute.

\item An annotation object can itself be sliced 
(e.g. \code{e[:10]} gets the slice representing
the first ten bases of the exon); such annotation slices can themselves
also be sliced.  More generally, an annotation is itself a coordinate
system that can be sliced, negated (only for nucleotide sequence 
annotations, to obtain the opposite strand), and have a length
(obtainable as usual via the builtin \code{len}() function).

\item Annotation objects provide a consistent interface
to the annotation coordinate system, based on the \class{SeqPath}
class.  Pretty much anything that you can do with \class{SeqPath}
you can also do with an annotation or annotation slice.
You can tell whether
an annotation is on the same strand (or opposite strand)
from the original annotation in the usual way, by checking
its \member{orientation} attribute, which is +1 for same strand
and -1 for opposite strand.  You can also obtain the entire
original annotation in the usual way, by accessing the \member{pathForward}
attribute of any annotation slice. 

\item One difference is that you cannot obtain the string value
(letters of the corresponding sequence) directly from an annotation
object or slice.  Instead, you must first obtain the corresponding
sequence slice, via its \member{sequence} attribute, to which
you can then apply the \method{str}() builtin function.

\item Because annotations obey the
coordinate system and slicing behaviors of sequence objects,
they can be aligned in an NLMSA sequence alignment just like any
sequence.  This provides a powerful and convenient way for
querying annotation databases.

\item The mapping of an annotation object to the sequence region it
represents is trivial, i.e. simply request its \member{sequence} attribute.
The reverse mapping (for any region of sequence, find the annotation(s) 
that map to that region) is best performed by creating an NLMSA alignment
object and saving the mapping as follows:
\begin{verbatim}
nlmsa = cnestedlist.NLMSA('myAnnotDB','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                          pairwiseMode=True,bidirectional=False)
for a in annoDB.itervalues(): # GET EACH ANNOTATION OBJ IN DATABASE
  nlmsa.addAnnotation(a) # SAVE ALIGNMENT OF ITS SEQ INTERVAL TO THIS ANNOTATION
nlmsa.build() # CREATE FINAL INDEXES FOR THE ALIGNMENT DATABASE
\end{verbatim}
Later you can get the list of annotations in some sequence interval \code{s}
as easily as 
\begin{verbatim}
for a in nlmsa[s]: # FIND ANNOTATIONS THAT MAP TO s
  # DO SOMETHING...
\end{verbatim}

\item Based on your pygr.Data schema, an annotation object may
have other attributes that connect it to other data.
For example, an object \code{e} representing an exon annotation
might have attributes that link it
to its {\em splice graph}.  \code{for e2,splice in e.next.items()} would iterate
through the list of exons it is connected to by a forward splice, etc.

\end{itemize}
\begin{funcdesc}{AnnotationDB}{sliceDB, seqDB, annotationType=None, itemClass=AnnotationSeq, itemSliceClass=AnnotationSlice, itemAttrDict=None, sliceAttrDict=dict(), filename=None, mode='r', maxCache=None}
  Constructs an annotation database using several arguments:

  \var{sliceDB}, a database that takes an annotation ID as a key, and returns
  a slice information object with attributes that give the sequence ID and start/stop
  coordinates of the sequence interval representing the annotation,
  and any other information about the annotation.  In general, any
  attribute on the slice information object, will also be accessible
  on the corresponding annotation object and slices derived from it.

  You can give \code{None} as the \var{sliceDB}, in which case the
  AnnotationDB will create one for you, either using an in-memory dictionary,
  or by opening a Python shelve file if you provide the \var{filename} argument;
  see below.

  \var{seqDB}, a sequence database that takes a sequence ID as a key, and
  returns a sequence object.

  \var{annotationType} should be a string identifier for the type of
  annotation.  This will be propagated to all annotation objects / slices
  derived from this annotation database.

  \var{itemClass}: the class to use for constructing an annotation object 
  to be returned from the AnnotationDB.__getitem__.  You can extend the
  behavior of annotation objects by subclassing \class{AnnotationSeq}.
  If the AnnotationDB participates in important schema relations,
  pygr.Data may add properties to the \var{itemClass} that implement
  its schema relations to other database containers.  (See the reference
  docs on \module{pygr.Data} below for details).

  \var{itemSliceClass}: the class to use for slices of annotation
  objects returned from the AnnotationDB.__getitem__.  You can extend the
  behavior of annotation objects by subclassing \class{AnnotationSlice}.
  If the AnnotationDB participates in important schema relations,
  pygr.Data may add properties to the \var{itemSliceClass} that implement
  its schema relations to other database containers.  (See the reference
  docs on \module{pygr.Data} below for details).

  \var{sliceAttrDict}, a dictionary providing the attribute name aliases
  for attributes on annotation objects to access attributes or tuple values
  in the sliceInfo objects.  The minimal required attributes are the
  sequence ID, start and stop coordinates in each object returned from \var{sliceDB}.
  For example,
\begin{verbatim}
sliceAttrDict = dict(id='chromosome',start='gen_start',stop='gen_stop')
\end{verbatim}
  would make it use \code{s.chromosome,s.gen_start,s.gen_stop} as the ID and interval
  coordinates for each slice information object \code{s}.  Note: the start,stop
  coordinates should follow the \class{SeqPath} sign convention, i.e. positive
  coordinates mean an interval on the positive strand, and negative coordinates
  mean an interval on the negative strand (i.e. the reverse complement of
  the positive strand.  See the reference documentation on \class{SeqPath} above
  for details).

  If the sliceAttrDict (or sliceInfo object directly) provides a \member{orientation}
  attribute, it will be used to be change positive intervals to negative intervals
  if the \member{orientation} attribute is negative.  This gives the user an alternative
  method to represent orientation: give all coordinates in positive orientation
  (positive integer values), and give an \member{orientation} attribute that
  is a negative value if the interval should be reversed (to negative orientation).

  If a sliceAttrDict value is an integer, then it will not be treated as an
  attribute name, but instead will be used as an index, treating the sliceInfo
  object as a tuple.  This makes it possible to use a \var{sliceDB} whose
  items are tuples.  Here's an example:
\begin{verbatim}
exon_db = AnnotationDB(exon_slices, db,
                       sliceAttrDict=dict(id=0, orientation=3, # GIVE ATTR INTERFACE TO 2PLE
                                          transcript_id=4, start=5, stop=6))
\end{verbatim}
  Additional tuples values beyond the required \member{id,start,stop}
  attributes may be used to provide additional informative attributes
  for the individual annotations.

  \var{filename}, if not None, indicates a Python shelve file to store the
  sliceDB info.  It will be opened according to the \var{mode} argument; 
  see the Python \module{shelve} docs for details.  Note: if you write data
  to an AnnotationDB stored using a shelve, you {\em must} call its
  \method{close}() method to ensure that all data is saved to the Python
  shelve file!

  \var{maxCache}, if not None, specifies the maximum number of annotation
  objects to keep in the cache.  For large databases, this is an important
  parameter for ensuring that the AnnotationDB will not consume too much
  memory (e.g. if you iterate over all or a large fraction of the annotations
  in the database).
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  Get the annotation object with primary key \var{id}.  This annotation object
  is both a sequence interval (representing the region of sequence that it
  annotates, e.g. for an exon, the region of genomic sequence that constitutes
  that exon), and also an annotation (i.e. it may have additional attributes
  from the slice information object, that give useful information about this
  annotation).
\end{funcdesc}

Note: to save new annotations to the AnnotationDB, use either of the following two
methods, instead of \method{__setitem__}, which is not permitted (because
there would be no way of guaranteeing that the annotation object provided
by the user could be stored persistently).
\begin{funcdesc}{new_annotation}{k,sliceInfo}
  Use this method to save new annotations to an \class{AnnotationDB},
  instead of using \code{annoDB[k] = v}, which is not permitted.
  Creates a new annotation with ID \var{k}, based on \var{sliceInfo},
  which must provide a sequence ID, start, stop, either by attribute
  names or integer indices (as specified by the sliceAttrDict), 
  and any addition attributes that we want to associate with this annotation.
  \var{sliceInfo} is saved in the \class{AnnotationDB}'s sliceDB.
  Returns an annotation object associated with \var{sliceInfo}.
\end{funcdesc}

\begin{funcdesc}{add_homology}{seq, search='blast', id=None, idFormat='\%s_\%d', autoIncrement=False, maxAnnot=999999, maxLoss=None, sliceInfo=None, **kwargs}
  Search for homology to \var{seq} in the sequence database self.seqDB
  using the named method specified by the \var{search} argument,
  and filtered using the NLMSASlice.keys() function, and store
  them as new annotations in the annotation database.  

  \var{seq} can be a string or sequence object or slice.

  \var{search} can be a string, in which case it will be treated as an
  attribute name for a method on self.seqDB to run the homology search.
  Alternatively, \var{search} must be a function that runs the homology search.
  Either way, the search function must take a sequence object as its
  first argument, and optional keyword arguments for controlling its
  search parameters.  Note: since both searching and filtering keyword
  arguments are passed as a single dictionary, the function should not
  die on unexpected keyword arguments.  The function must return an
  alignment object (e.g. NLMSA).

  \var{id} if not None, will be used as the annotation ID.  Otherwise,
  the \var{seq.id} will be used as the annotation ID.

  \var{idFormat} controls the generation of ID strings for cases where
  multiple hits pass the search and filter criteria.  It simply appends
  an integer counter to the id.

  \var{autoIncrement=True} forces it to generate its own integer IDs for
  each new annotation.

  \var{maxAnnot} specifies the maximum numbers of hits that will be
  processed for \var{seq}.  If the number of hits passing both search
  and filter criteria exceed this number, a \class{ValueError} will be raised.

  \var{maxLoss} if not None, must be an integer indicating the maximum
  number of residues that can be missing from the alignment to \var{seq}
  to be acceptable as an annotation.

  \var{sliceInfo} if not None, will be appended to the (id,start,stop)
  tuple that is saved for each annotation.  This enables you to add
  annotation attributes, by giving a sliceAttrDict setting to your AnnotationDB
  constructor that defines these additional attributes.  Note: \method{add_homology}()
  saves each annotation as a slice tuple to self.sliceDB, in the form:
  \code{(id,start,stop)+sliceInfo}.

  You can (and should) specify many additional arguments for controlling
  the homology search, and results filtering.  For the former, see the list
  of arguments for BlastDB.blast() and BlastDB.megablast().  For the latter,
  see the list of arguments for NLMSASlice.keys().

  \method{add_homology}() returns a list of the annotation objects 
  created as a result of the homology search.
\end{funcdesc}

For iteration over annotations in a very large annotation database, it is
important to understand how to control the caching of annotation objects.
We try to follow Python's iterators rules closely: \method{iteritems}()
and \method{itervalues}() simply iterate over the annotation, applying
the \var{maxCache} limit to the total number of annotations that will be
kept in cache at any one time.  
\begin{funcdesc}{iteritems}{}
\end{funcdesc}
\begin{funcdesc}{itervalues}{}
\end{funcdesc}
By contrast, \method{items}() and \method{values}()
force loading of all annotations in the entire database into cache, since
that is what these methods require.
\begin{funcdesc}{items}{}
\end{funcdesc}
\begin{funcdesc}{values}{}
\end{funcdesc}
Finally, \method{__iter__}() and \method{keys}() just obtain the 
list of annotation IDs, without loading anything into the cache.

\begin{funcdesc}{close}{}
  You must call this method to ensure that any data added to the AnnotationDB
  will be written to its Python shelve file on disk.
  This method is irrelevant, but harmless,
  if you are instead using an in-memory dictionary as storage.
\end{funcdesc}

\subsubsection{VirtualSeq}
This class provides an empty sequence object that
acts purely as a reference system.
Automatically elongates if slice extends beyond current stop.
This class avoids setting the {\em stop} attribute, taking advantage
of SeqPath's mechanism for allowing a sequence to grow in length.
\begin{verbatim}
s = VirtualSeq('FOOG_HUMAN')
len(s) # ONLY 1 LETTER LONG BY DEFAULT
s1 = s[100:215] # GET A SLICE OF THIS SEQUENCE
len(s) # NOW IT'S 215
\end{verbatim}

The associated VirtualSeqDB class provides a ``sequence database''
that returns a VirtualSeq object for every identifier requested of
it.  It acts like a Python dictionary:
\begin{verbatim}
db = VirtualSeqDB()
s = db['FOOG_HUMAN'] # ASK FOR A SEQUENCE BY ITS IDENTIFIER
s1 = s[100:215] # GET A SLICE OF THIS SEQUENCE
\end{verbatim}
For a given identifier it always returns the same VirtualSeq
object (i.e. the object returned from the first request for that identifier).
In other words, if the identifier was previously requested,
it returns the VirtualSeq for that identifier; if not, it 
creates a new one.
This can be convenient when performing operations that just
need a coordinate reference system, not actual sequence.


\subsubsection{PrefixUnionDict}
This class acts as a wrapper for a set of dictionaries, each
of which is assigned a specific string ``prefix''.  It provides
a dictionary interface that accepts string keys of the form
``prefix.suffix'', and returns d['suffix'] where {\em d} is
the dictionary associated with the corresponding prefix.  This
is useful for providing a unified ``database interface'' to a
set of multiple databases.
\begin{verbatim}
hg17 = BlastDB('/usr/tmp/ucsc_msa/hg17')
mm5 = BlastDB('/usr/tmp/ucsc_msa/mm5')
... # LOAD A BUNCH OF OTHER GENOMES TOO...
genomes = {'hg17':hg17,'mm5':mm5, 'rn3':rn3, 'canFam1':cf1, 'danRer1':dr1,
'fr1':fr1, 'galGal2':gg2, 'panTro1':pt1} # PREFIX DICTIONARY FOR THE UNION 
					 # OF ALL OUR GENOMES
genomeUnion = PrefixUnionDict(genomes)
ptChr7 = genomeUnion['panTro1.chr7'] # GET CHIMP CHROMOSOME 7

if 'panTro1.chr5' in genomeUnion: # CHECK IF THIS ID IN OUR UNION
    pass # DO SOMETHING...

s = -(ptChr7[1000:2000]) # GET A BIT OF THIS SEQUENCE
if s in genomeUnion: # THIS IS HOW TO CHECK IF s DERIVED FROM OUR UNION
    pass # DO SOMETHING... 
\end{verbatim}

It provides a \method{__contains__} method that tests whether
a given sequence object is derived from the \class{PrefixUnionDict}
(see example above).  Here are some additional methods:

\begin{funcdesc}{__init__}{prefixDict=None,separator='.',filename=None,dbClass=BlastDB}
  You can create a \class{PrefixUnionDict} either using
  a \var{prefixDict} (whose keys are string prefixes, and whose 
  values are sequence databases), or using a previously created
  header file \var{filename}.  
  Using the header file, the constructor will
  automatically open all the sequence databases for you.
  When opening from a header file, you can also specify a
  \var{dbClass} to be used for opening individual sequence databases
  listed in the header file; the default is \class{BlastDB}.
  The database class constructor must take a single argument,
  which is the filepath for opening the database.  The 
  \var{separator} character is used to form ``prefix.suffix''
  identifiers.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  The invert operator (\textasciitilde, the ``tilde'' character) 
  enables reverse-mapping of sequence objects to their string ID.
  This is the recommended way to get the ``fully qualified sequence ID'', i.e. with
  the appropriate prefix prepended. 
\begin{verbatim}
id = (~db)[seq] # GET PROPERLY PREFIXED-IDENTIFIER FOR THIS SEQUENCE
\end{verbatim}
  For a given sequence object \var{seq} derived from the union
  (or a slice of a sequence from the union), return a string identifier
  in the form of ``foo.bar''.  
\end{funcdesc}

\begin{funcdesc}{getName}{path}
  This method is deprecated; instead use the \method{__invert__} operator
  above.
\end{funcdesc}

\begin{funcdesc}{writeHeaderFile}{filename}
  THIS METHOD IS DEPRECATED, because it is restricted to
  assuming that all sequence dictionaries it contains
  are of a single class.  We recommend that you instead save
  it to pygr.Data, or pickle it directly using pygr.Data.dumps().

  Save a header file for this union, to reopen later.
  It saves the separator character, and a list of prefixes
  and filepaths to the various sequence databases (which
  must have a \member{filepath} attribute).  This header
  file can be used for later reopening the prefix-union
  in a single step.
\end{funcdesc}

\begin{funcdesc}{newMemberDict}{}
  Returns a new member dictionary for testing membership in
  the distinct prefix groups.  See \class{PrefixUnionMemberDict}.
\end{funcdesc}

\begin{funcdesc}{cacheHint}{owner,ivalDict}
  Communicates a set of caching hints to the appropriate member
  databases.  \var{ivalDict} must be a dictionary whose keys are
  sequence ID strings, and whose values are each a (start,stop) tuple
  for the associated covering interval coordinates to
  cache for each sequence.  \var{owner} should be a python object
  whose existence controls the lifetime of these cache hints.
  When \var{owner} is garbage-collected by Python (due to its 
  reference count going to zero), the member databases will clear
  these cache hints from their cache storage.

  On \class{PrefixUnionDict}, this method simply passes along
  the cache hints to the appropriate member databases by calling
  their \method{cacheHint} method, without itself doing anything
  to cache the information.
\end{funcdesc}



\subsubsection{PrefixUnionMemberDict}
Implements membership testing on distinct prefix groups.  Specifically,
you can bind a given prefix to a value
\begin{verbatim}
d['prefix1'] = value
\end{verbatim}
then test whether a given object \var{k} is a member of any of the
prefix groups in the dictionary:
\begin{verbatim}
v = d[k] # raises KeyError if k not a member of 'prefix1' or other prefix group in d
\end{verbatim}

\begin{funcdesc}{__init__}{puDict,default=None,attrMethod=lambda x:x.pathForward.db}
  \var{puDict} must be a \class{PrefixUnionDict}, whose prefix groups constitute the
  allowed possible key groups for this membership dictionary.  \var{default}
  provides a default value to apply to any key whose prefix has not been explicitly
  given a value in this dictionary.  If no \var{default} is set, this dictionary
  will raise a \code{KeyError} for any key whose prefix has not been 
  explicitly given a value in this dictionary.
  \var{attrMethod} specifies a method for obtaining
  the actual prefix container object from a given member key object.  The default
  \var{attrMethod} treats the key as a sequence object and tries to determine what
  database container it is from.
\end{funcdesc}

\begin{funcdesc}{possibleKeys}{}
  Returns an iterator for the key values (prefix strings) that are allowed for 
  this dictionary, obtained from the bound \class{PrefixUnionDict}.
\end{funcdesc}

\subsubsection{PrefixDictInverse}
Provides the interface to the inverse mapping of the \class{PrefixUnionDict}.
\begin{funcdesc}{__getitem__}{k}
  Returns the fully-qualified string ID for sequence object \var{k}.  
  Properly handles both sequence annotation object and regular sequence
  objects.
\end{funcdesc}

\subsubsection{PrefixDictInverseAdder}
Adds the capability of automatically adding new sequence databases to the
\class{PrefixUnionDict}, if needed.  This is implemented by extending
the standard \method{__getitem__} method:
\begin{funcdesc}{__getitem__}{k}
  Returns the fully-qualified string ID for sequence object \var{k}.  
  Properly handles both sequence annotation object and regular sequence
  objects.  If sequence object \var{k} is from a sequence database that
  is not in the \class{PrefixUnionDict}, it will be automatically added
  to the prefixUnion, if the prefixUnion has an \member{addAll} attribute
  set to \var{True}; if not, a \code{KeyError} is raised.
  This is used in the standard \class{NLMSA} write mode 'w'
  to allow users to add sequences to the alignment without having to 
  previously add the sequence databases containing those sequences, 
  to the prefixUnion for the NLMSA.
\end{funcdesc}



\subsubsection{BlastSequenceCache}

This class is deprecated; the \class{FileDBSequence} class and associated
database container caching mechanisms provide a more powerful mechanism
that is intended to replace \class{BlastSequenceCache}.

Implements a variant of BlastSequence designed to merge and cache requests for local intervals of sequence so that repeated accesses to these regions are bundled and cached for efficiency.  You work with sequence objects of this type normally, using Python slicing to obtain subintervals, and str() to get the sequence string for a subinterval.  But behind the scenes, it does two things:

\begin{itemize}
\item
all slicing operations are recorded, in the form of a cache of superintervals.  Overlapping or adjacent intervals are merged into a superinterval up to a maximum superinterval size (default 20000).  It will automatically create as many superintervals as needed to cover the requested subinterval slices.  Each superinterval is represented by an object of the FastacmdIntervalCache class.

\item 
when the sequence string of a subinterval is requested, the cache actually retrieves (and caches) the entire superinterval containing that subinterval.  Fastacmd only needs to be called once for this superinterval.  Subsequent subinterval string requests that fall within this cached superinterval are simply returned directly from the cache, without calling fastacmd.

\end{itemize}

\subsubsection{SQLSequence}

Implements a subclass inheriting from SQLRow and SequenceBase, to use a relational database table to obtain the actual sequence.  There are three minor variants DNASQLSequence, RNASQLSequence, ProteinSQLSequence (so that the sequence does not have to analyze itself to determine what kind of sequence it is).  Its constructor takes the same arguments as SQLRow(table, id), where table is the SQLTable object representing the table in which the sequence is stored, and id is the primary key of the row representing this sequence.  However, normally this class is simply passed to the Table object itself so that it will use it to instantiate new row objects whenever they are requested via its dictionary interface.  Here's a simple example:

\begin{verbatim}
class YiProteinSequence(ProteinSQLSequence): # CREATE A NEW SQL SEQUENCE CLASS
    def __len__(self): return self.protein_length  # USE LENGTH STORED IN DATABASE
protein = jun03[protein_seq_t] # protein IS OUR SQLTable OBJECT REPRESENTING PROTEIN SEQUENCE TABLE
protein.objclass(YiProteinSequence) # FORCE PROTEIN SEQ TABLE TO USE THIS TO INSTANTIATE ROW OBJECTS
pseq = protein['Hs.1162'] # GET PROTEIN SEQUENCE OBJECT FOR A SPECIFIC CLUSTER
\end{verbatim}

Let's go through this line by line:

\begin{itemize}

\item
we create a subclass of ProteinSQLSequence to show how Python makes it easy to create customized behaviors that can make database access more efficient.  Here we've simply added a __len__ method that uses the protein_length attribute obtained directly from the database, courtesy of SQLRow.__getattr__, which knows what columns exist in the database, and provides them transparently as object attributes.  (The ordinary SequenceBase __len__ method calculates it by obtaining the whole sequence string and calculating its length.  Clearly it's more efficient for the database to retrieve this number (stored as a column called protein_length) and return it, rather than making it send us the whole sequence).

\item
next we call the protein.objclass() method to inform the table object that it should use our new class for instantiating any row objects for this table.  It will call this class with the usual SQLRow contructor arguments (table, id).
\end{itemize}

\subsubsection{StoredPathMapping}

Note: This class is deprecated; the \class{NLMSA} alignment database class provides
a much more powerful interface that is intended to replace older mechanisms
such as \class{StoredPathMapping}.

A second major area in Pygr is representation and query of multiple sequence alignment databases in a way that is scalable to whole genomes.  We have previously showed (in our work on Partial Order Alignment) that graphs provide both a compact and algorithmically powerful way to store alignments.  Combining this with "interval alignment" makes it scalable and gives a simple interface.  In Pygr, alignments are just another kind of graph, whose nodes are sequence intervals, and edges are alignment relations.  This provides a general-purpose facility for working with sets of sequence intervals, sequence annotation databases, and multiple sequence alignments, all queryable via Pygr graph queries.  We have implemented different container subclasses to work with these data in memory or to work transparently with data stored in relational databases.  The consistency and simplicity of the Pygr framework makes it a good interface both to run external tools like BLAST, and to store or query the results in persistent storage like a MySQL database.

\begin{verbatim}
hg17 = BlastDB('/data/ucsc/hg17') # GET CONTAINER FOR HUMAN GENOME DATABASE
bcl2m = hg17['chr22'][16544303:16588541] # GET INTERVAL WITH BCL2L13 GENE
al = hg17.megablast(mouse_bcl2,maxseq=1) # GET REPEAT-MASKED MEGABLAST ALIGNMENT, ONLY TOP HIT
al[bcl2m[1000:1100] ] += mrna[210:310] #ADD ALIGNMENT OF A 100nt SEGMENT TO mrna SEGMENT
al.storeSQL('test.table',db_cursor) # STORE COMPLETE ALIGNMENT IN RELATIONAL DATABASE
for e in MAFStoredPathMapping(bcl2m,'ucsc_maf8',u).edges(): #GET ITS MULTIGENOMEALIGNMENTS
   print str(e.srcPath),str(e.destPath) # PRINT THE ACTUAL ALIGNED SEQUENCE INTERVALS
\end{verbatim}

Note: {\em We will be unifying all sequence alignment functionality under the 
NLMSA interface design sometime in the near future.  Specifically, the
\class{PathMapping} and related classes, while similar to \class{NLMSA},
will be replaced with interfaces that are identical to the \class{NLMSA}
interface.}

\subsubsection{SliceDB}
For most applications, \class{AnnotationDB} is a better choice than
this older class.
This class enables you to apply ``slicing information'' from 
one database to sequences from a second database.  For example,
you could have a database that lists genes as intervals (slices)
on genomic sequences stored in a BlastDB database.  The only
requirements are:
\begin{itemize}
\item
{\em slice database}: must accept a string identifier as a key,
and return a slice information object as a value.

\item
{\em slice information}: a slice information object must
have the following attributes: \member{name} gives the identifier
of the sequence containing the slice; \member{start} and \member{stop}
give the coordinates of the sequence interval (which should be positive
integers following standard
Python slice coordinate conventions); \member{ori} gives the sequence
orientation as an integer (1 for positive orientation, -1 for
negative orientation).

\item
{\em sequence database}: must accept a string identifier as a key,
and return a sliceable sequence object as a value.

\end{itemize}

Both databases should raise \code{KeyError} for bad identifiers.
The current \class{SliceDB} implementation caches sequence objects so
that subsequent calls for the same identifier will not require
repeating the database queries to the two databases.  To
remove a sequence object from the cache, just use
\code{del db[id]} as usual.

SliceDB inherits from the builtin Python \class{dict} class,
so all standard methods can be used.

\begin{verbatim}
db = SliceDB(sliceDB,seqDB) # CREATE OUR DATABASE
gene = db[cluster_id] # USE IT TO GET A GENE SEQUENCE...
\end{verbatim}



\subsubsection{Functions}
The seqdb module also provides several convenience functions:

\begin{funcdesc}{read_fasta}{ifile}
  a generator function
  that yields tuples of \var{id,title,seq} from \var{ifile}.  
\end{funcdesc}

\begin{funcdesc}{write_fasta}{ofile, s, chunk=60, id=None}
  writes the sequence \var{s}
  to the output file \var{ofile}, using \var{chunk} as the line width.
  \var{id} can provide an identifier to use instead of the default 
  \code{s.id}.
\end{funcdesc}

\subsection{pygr.Data Module}
\label{pygrData-module}
This module provides a simple but powerful interface for creating
a ``data namespace'' in which users can access complex datasets
by simply requesting the name chosen for a given dataset -- much
like Python's \code{import} mechanism enables users to access
a specified code resource by name, without worrying about where it
should be found or how to assemble its many parts.  For an introduction,
see the pygr.Data tutorial.

\subsubsection{What kinds of data can be saved in pygr.Data?}
There are a few basic principles you should be aware of:
\begin{itemize}
\item The object should be a database (container) or mapping (graph),
not an individual item of data.  pygr.Data is intended to provide
a name space for commonly used resources, i.e. an entire database,
which in turn enable you to access the items they contain.

\item The object must have a \member{__doc__} string that describes
what its contents are.

\item The object must be {\em picklable} using Python's \module{pickle}
module.  pygr.Data uses \module{pickle} both to save your object to 
a persistent storage (either a python \module{shelve}, MySQL database,
or XMLRPC server), and to analyze its {\em dependencies} on other
Python objects.  The default pickling procedure (save a dictionary of
your object's attributes) works fine for simple Python classes.
However, if your class accesses external data (i.e. data not actually
stored in its attributes), you will have to define \method{__getstate__}
and \method{__setstate__} methods that save and restore just the 
relevant information for it to be able to access the information
it needs (e.g. if your class reads a file, \method{__getstate__} must
save its filename).  If your class inherits from \class{dict}, you
will also have to define a \method{__reduce__} method.  For a simple
example, see the classes \class{seqdb.AnnotationDB,seqdb.SeqDBbase,seqdb.XMLRPCSequenceDB}, and the \module{pickle} module documentation.

\item pygr.Data provides a {\em namespace} for commonly used data resources.
Once you import pygr.Data, you can save resources into it just as you would into
any python namespace.  For example to save an alignment object \code{nlmsa}
as the resource ID ``Bio.MSA.UCSC.foo17'':
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
pygr.Data.Bio.MSA.UCSC.foo17 = nlmsa # NOW SAVE THE ALIGNMENT
pygr.Data.save() # SAVE ALL PENDING DATA TO THE RESOURCE DATABASE
\end{verbatim}
The crucial point is that this namespace is {\em persistent} between
Python interpreter sessions.  The actual data is not saved in the pygr.Data 
module file, but in {\em resource databases} either on your disk, in
a remote XMLRPC server, or in a MySQL database (for details see below).

\item If an object saved to pygr.Data depends on a given file,
you should use an absolute path to that file, instead of a relative path,
when originally constructing that object, prior to adding it to
pygr.Data.  Relative paths are obviously inadequate for future users of
pygr.Data to find the file, since they are likely to be working in
a different ``current directory''.  

\item For similar obvious reasons, you should ensure that such a 
``dependency file'' has security settings that make it readable 
to the set of users that you want to be able to access this pygr.Data
resource in the future.  Users who lack privileges to be able to 
read that file will be unable to access this specific pygr.Data resource.

\item To get a named resource from pygr.Data, you again just use this
namespace, but with a ``constructor syntax'', i.e. add a call at the end of
the resource name:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
nlmsa = pygr.Data.Bio.MSA.UCSC.foo17() # SYNTAX EMPHASIZES CONSTRUCTION OF INSTANCE
\end{verbatim}
The actual resource object is not obtained until you call the constructor.

\item pygr.Data also stores {\em schema information} for the resources.
These represent relationships between one resource and another resource
(or their contents).  For example
\begin{verbatim}
pygr.Data.schema.Bio.Annotation.ASAP2.hg17.splicegraph = \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}
indicates that the pygr.Data resource \code{Bio.Annotation.ASAP2.hg17.splicegraph}
is a many-to-many mapping of the pygr.Data resource \var{exons} onto itself,
with additional ``edge information'' for each exon-to-exon mapping
provided by the pygr.Data resource \var{splices}.  Furthermore, this mapping
is to be bound directly to items of \var{exons} (i.e. objects returned
from \code{exons.__getitem__}) as their \member{next} attribute (giving the
forward mapping), their \member{previous} attribute (giving the reverse
mapping), and the \member{exons} attribute on items of \var{splices}
(giving the mapping of the splice object to its pair of (source,target) exons
as a tuple).

\item when a user requests a resource that itself depends on other
resources, pygr.Data automatically loads them.  Thus users do not need
to know about the complex set of dependencies between data; all they
have to do ask is ask for the specific data resource they want,
and pygr.Data will take care of all the details behind the scenes.
For example, a database of exon annotations is not very useful without
also loading the genomic sequence database that these annotations
refer to.  Using pygr.Data, we can simply load the exon annotation
resource, and it will automatically get the genomic sequence data
for us.  Thus to get an exon's sequence all we have to do is:
\begin{verbatim}
exons = pygr.Data.Bio.Annotation.ASAP2.hg17.exons() # ANNOTATION DATABASE
str(exons[464].sequence) # GET THE SEQUENCE OF THIS SPECIFIC EXON
\end{verbatim}

\item It should be noted that at the moment there is only one name
(\code{Bio}) at the top-level of the pygr.Data module namespace (since currently
this is only being used for bioinformatics).  However it's 
trivial to add new names as \class{ResourcePath} objects to the pygr.Data
module.

\end{itemize}

\subsubsection{pygr.Data is transactional}

pygr.Data follows a {\em transactional} model: new resources
added to pygr.Data are not saved to the resource database until you
call \code{pygr.Data.save()}.  This has several benefits:

\begin{itemize}
\item Because of the transactional model,
within a single transaction, the {\em order} in which you
add resources to pygr.Data does not matter.  This is a crucial data
integrity requirement.  In a non-transactional model (where each
resource is saved the instant it is added), adding resources in the
wrong order will break data integrity.  Specifically, 
if object \var{B} depends on object \var{A}, 
but you saved \var{B} before \var{A}, then
\var{B} will not be aware of \var{A}'s resource ID (i.e. it has no way of
knowing that you plan on assigning \var{A} a resource ID some time
in the future).  This would break a crucial data integrity guarantee,
namely that if two objects \var{B} and \var{C} refer to the same
object \var{A} at the time they are saved, it is guaranteed that
when restored in the future they will still refer to the identical
Python object.  To provide this guarantee in a way that is
persistent across Python interpreter sessions, pygr.Data must 
store object references in terms of their unique pygr.Data IDs.
This is only possible if the object has been assigned a pygr.Data
ID (by having added it to pygr.Data in the usual way) before
you complete the transaction by calling \code{pygr.Data.save()}.

\item This also enables pygr.Data to provide a limited form of
{\em rollback}, i.e. the ability to cancel an entire set of
resource additions at any time before they are committed.
You can do this by calling \code{pygr.Data.rollback()}.

\item You can save a given group of pygr.Data resources as a transaction
to multiple pygr.Data resource databases, simply by calling
\code{pygr.Data.save}(\var{layer}) multiple times with different
pygr.Data \var{layer} names.

\item How do you decide what set of data forms a single transaction?
This follows a very simple rule: if an object \var{A} that you are adding
to pygr.Data depends on (i.e. {\em refers
to}) some other object \var{B} that you also
intend to add to pygr.Data, then \var{B} must either {\em already} have a pygr.Data ID,
or \var{B} must also be added to pygr.Data in the same transaction.

\item If you add resources to pygr.Data, you {\em must} call \code{pygr.Data.save()}
before your Python interpreter session exits.  Otherwise the transaction would
be left incomplete and would not be saved to the pygr.Data resource database.
Similarly, if for some reason you need to call \code{reload(pygr.Data)}, 
but there are pending pygr.Data additions of data or schema, you must
first call either \code{pygr.Data.save()} or \code{pygr.Data.rollback()}
to indicate whether you wish to save or dump these pending additions.

\end{itemize}

\subsubsection{pygr.Data Namespace Conventions}
At this point, we're still just making this up as we go along.
However, it is clearly advantageous to adopt some simple conventions
that make it easy for people to use the same name for a given data resource,
and to find what they're looking for.  We are adopting the following conventions:
\begin{itemize}
\item The general pattern is \var{Area.Category.Database.release}.  For example,
Swissprot release 42 is ``Bio.Seq.Swissprot.sp42''.  This is a very straightforward
pattern to follow for well-known databases.
\item In other cases, the dataset is not strictly speaking a well-known database,
but is instead an ``instance of a larger class of data''.  For example, genome
sequences.  In this case we follow the general pattern 
\var{Area.Category.Class.Instance.release}.  For example, the human genome draft
17 is ``Bio.Seq.Genome.HUMAN.hg17''.  
\item To identify specific genomes, we use the Uniprot / Swissprot 
controlled vocabulary for species names, e.g. ``HUMAN'' for human.  For more
information, see the Swissprot website
\url{http://www.expasy.org/cgi-bin/speclist}.
\item  Often a database may itself contain many different resources.  These
individual resource names are simply appended to the pygr.Data name, for example,
the ASAP database contains a resource called \code{exons}.  This would be 
accessed as ``Bio.Genomics.ASAP.asap2.exons''.  This pattern can be extended,
for as many layers are required to specify a unique resource in the database.
\item In cases where the original data provider does not assign a release name, 
we use the approximate release date as the release name (chosen appropriately
for the release frequency of the database).  e.g. ``jan06''.
\item Capitalization: we capitalize Area, Category, Database, Class and Instance
names.  Release names are chosen to match the name used by the original data 
provider, which are usually not capitalized.
\end{itemize}
Existing Area categories:
\begin{itemize}
\item Bio.Seq: currently, the main category in pygr.Data is sequence databases.
\item Bio.MSA: Another major category is multiple sequence alignments (e.g. genome alignments).
For example: ``Bio.MSA.UCSC.hg18_multiz28way''.
\item Bio.Annotation: category representing annotation information bound
to sequence intervals.
\item Bio.Expression: category representing gene expression analysis,
including microarray data.
\end{itemize}
You may obtain a directory of available resources available using
the \code{pygr.Data.dir}() function:
\begin{verbatim}
>>> pygr.Data.dir('Bio.Seq.Swiss')
['Bio.Seq.Swissprot.sp42']
\end{verbatim}
This returns the list of items beginning with the string
you provided.  Use its \code{asDict=True} argument to make it return a dictionary
of matches with detailed information such as their docstring descriptions.

We suggest that you follow these conventions and extend them as needed.
Please report new category names to us so we can add them to the list.

\subsubsection{How does pygr.Data access resource databases?}
The list of resource databases is read from the environment variable
PYGRDATAPATH.  If this variable is empty or missing, the default path
for pygr.Data to search is the user's home directory (\$HOME) and
current directory, in that order.  PYGRDATAPATH should be a comma separated list
of ``resource path'' strings, which must be one of the following:
\begin{itemize}
\item A directory path (e.g. /usr/local/pygrdata), in which pygr.Data should
look for (or, if none present, create) a database file called ``.pygr_data''.  
You can use the tilde character to indicate your home directory path.
These are accessed by pygr.Data using its \class{ResourceDBShelve} class.

\item a URL for accessing an XMLRPC server that is serving a pygr.Data
resource database index (previously started by you or someone else).
The URL must begin with ``http://''.
These are accessed by pygr.Data using its \class{ResourceDBClient} class.

\item a MySQL server, indicated by a path entry of the form 
``mysql:\var{DBNAME.TABLENAME} \var{dbinfo}'',
where \var{DBNAME} is the name of the database in your MySQL
server that contains the pygr.Data resource index,
and \var{TABLENAME} is the name of the table which contains this index.
\var{dbinfo} is optional.  If provided, it must be a whitespace separated 
list of arguments for connecting to the MySQL server, of the form
\var{host} \var{user} \var{passwd}.  You can provide one, two
or three of these optional arguments, always beginning with \var{host}.  
If no \var{dbinfo} is provided,
pygr.Data will get the host, user, and password information for connecting 
to the MySQL server as usual from your
.my.cnf configuration file in your home directory.
Such resource databases are accessed by pygr.Data using its 
\class{ResourceDBMySQL} class.
\end{itemize}

\subsubsection{download=True Mode}
When requesting a pygr.Data resource name, you can specify 
the optional argument \var{download=True}, which forces pygr.Data
to search for a resource that can be downloaded to your local
filesystem (instead of accessed via XMLRPC from a remote server).
\begin{itemize}
\item If you already have a local copy of the resource, that will be used.

\item If no local copy of the resource exists, and a downloadable
resource is found, it will be automatically downloaded and initialized
for you.  The result of the resource request will be the fully
initialized local copy of the resource, ready for use.  Of course,
downloading a very large dataset may take a long time, but
the download and processing is completely automatic.

\item If the downloaded resource itself depends on other 
resources that you do not have local copies of, they will also
be requested using the download=True mode, and so on, until
all resource dependencies are satisfied.  In this way, pygr.Data
can automatically obtain for you the complete set of local 
resources needed to work with a multi-genome alignment, for example:
\begin{verbatim}
nlmsa = pygr.Data.Bio.MSA.UCSC.dm2_multiz9way(download=True)
\end{verbatim}

\item After a resource has been successfully downloaded and
initialized, it will be automatically saved to your local pygr.Data resource
database (specifically, the first resource database in your
PYGRDATAPATH) for future usage.  Future requests for this
resource do not need to specify download=True, because the 
resource is now recorded in your local pygr.Data resource database
as being available locally.

\item To see what downloadable resources are available, pass the
download=True option to pygr.Data.dir().  Note: currently, this 
also lists resources that you have available locally.

\item A downloadable resource can be any URL that returns
a dataset usable in Pygr as a data resource.  Examples:
a FASTA sequence dataset (accessed in Pygr as a BlastDB);
an NLMSA textdump file (loaded in Pygr as an NLMSA using the
textdump_to_binaries() function).  The URL can be anything 
that can be downloaded using the Python \module{urllib}
module.

\item Pygr.Data searches its resource databases for records
of downloadable resources matching the requested name.  
Currently, only XMLRPC pygr.Data servers will return
lists of downloadable resources.  Note that the resource
database does not store the resource, and the resource will not
be directly downloaded from the resource database.  Instead,
the resource database simply stores a record indicating the location
(URL) for downloading the resource, and how to initialize it
automatically on your local computer.

\end{itemize}

\subsubsection{Adding Downloadable Resources to Pygr.Data}
Only a few steps are required to add a downloadable resource
to pygr.Data.  The main difference is that instead of saving an
actual resource, you are merely saving a pointer to download / 
initialize the resource, which will only be invoked when a user
requests that the resource be downloaded to their local computer.
\begin{itemize}
\item First, you need the URL for downloading a data file that 
Pygr could use as a resource.  Obvious examples include a FASTA
sequence database, or an NLMSA textdump.  Compressed or archived
data files are supported (for details, see the \module{downloader}
module documentation).

\item Next, create a \class{SourceURL} object with the desired URL:
\begin{verbatim}
from pygr.downloader import SourceURL
dfile = SourceURL('http://biodb.bioinformatics.ucla.edu/PYGRDATA/dm2_multiz9way.txt.gz')
\end{verbatim}
Note that this represents just a file, not an actual resource usable
in Pygr.  This is the difference between a textdump file, and a 
Pygr NLMSA object built from that textdump.

\item Just save the SourceURL to a local pygr.Data resource
database (i.e. shelve storage) in the usual way:
\begin{verbatim}
dfile.__doc__ = 'DM2 based nine genome alignment from UCSC in textfile  dump format'
pygr.Data.Bio.MSA.UCSC.dm2_multiz9way.txt = dfile
\end{verbatim}
Note that we added the suffix ".txt" to the usual resource name, because
this is just a textdump file instead of the actual resource that can be
used in Pygr.  Strictly speaking there is no need to save the textfile
directly to pygr.Data, but this improves modularity (e.g. there might
be multiple URLs from which we could download the same resource text file).

\item Finally, we create a rule for initializing the actual resource
object (in this case, NLMSA) from the downloaded text.  As an example,
the \class{NLMSABuilder} class saves the appropriate rule for 
initializing an NLMSA from a text file:
\begin{verbatim}
from pygr.nlmsa_utils import NLMSABuilder
nbuilder = NLMSABuilder(dfile)
nbuilder.__doc__ = 'DM2 based nine genome alignment from UCSC'
pygr.Data.Bio.MSA.UCSC.dm2_multiz9way = nbuilder
pygr.Data.save() 
\end{verbatim}
Note that we saved this as the actual resource representing the
dm2_multiz9way alignment, because that is what it will return
when unpickled by pygr.Data.

Here is another example, for downloading and initializing a
FASTA sequence database:
\begin{verbatim}
src = SourceURL('ftp://hgdownload.cse.ucsc.edu/goldenPath/droVir3/bigZips/droVir3.fa.gz')
src.__doc__ = 'D. virilis Genome (February 2006) FASTA file'
pygr.Data.Bio.Seq.Genome.DROVI.droVir3.fasta = src
from pygr.downloader import GenericBuilder
rsrc = GenericBuilder('BlastDB', src)
rsrc.__doc__ = 'D. virilis Genome (February 2006)'
pygr.Data.Bio.Seq.Genome.DROVI.droVir3 = rsrc
pygr.Data.save()
\end{verbatim}
Note that we used the \class{GenericBuilder} class, which acts as proxy
for the class we want to use for building the resource (\class{BlastDB}).
At this moment we do not actually want to make a BlastDB, we simply
want to save a rule for making a BlastDB when the user actually
requests that this resource be downloaded.  
Upon unpickling by pygr.Data, \class{GenericBuilder}
simply calls its target class with the exact list of arguments / 
keyword arguments it originally received.  When \var{src} is
unpickled by pygr.Data, it will be transformed into the local
filename where the FASTA file was downloaded to (after automatic gunzipping).
Since \class{BlastDB} just expects a filename as its first argument,
we provide \var{src} as the only additional argument to \class{GenericBuilder}.
Note that you specify the target class as a string; GenericBuilder
matches this against its list of accepted classes, to avoid creating
a security hole wide enough to drive a truck through!

\item Setting up an XMLRPC server to serve the downloadable 
resources you saved to your pygr.Data shelve database is easy.    
When you create the server object, just pass the optional 
\var{downloadDB} argument as follows.  It should give
the path to your shelve file containing this resource database:

\begin{verbatim}
import pygr.Data
nlmsa = pygr.Data.Bio.MSA.UCSC.hg17_multiz17way() # data to serve: NLMSA AND SEQ DBs
server = pygr.Data.getResource.newServer('nlmsa_server',
                  downloadDB='/your/path/to/the/shelve/.pygr_data',
                  withIndex=True)
server.serve_forever() # START THE SERVICE...
\end{verbatim}
You can also directly call the server method \method{read_download_db}(path)
to read a list of downloadable resources from a shelve specified by
the \var{path}.  Resources from the new file will be added to
the current list of downloadable resources.
Note however that the server object currently can only store one
download rule for a given resource name, so a duplicate rule for
a resource name already in its downloadDB index will overwrite the
previously existing rule.

\end{itemize}


\subsubsection{Convenience functions}
\begin{funcdesc}{save}{layer=None}
  Saves all pending pygr.Data additions to the resource database.
  If \var{layer} is not specified, each resource will be saved to the
  layer it was added to, or to the default layer if none was specified
  at the time of addition.  If \var{layer} is not None, it forces all
  pending data to be saved specifically to that layer.  You can call
  \code{pygr.Data.save()} multiple times with different \var{layer}
  values to make the same set of data (transaction) be saved to each
  of the specified resource databases.
\end{funcdesc}

\begin{funcdesc}{rollback}{}
  Dumps all pending pygr.Data additions (since the last \code{save()}
  or \code{rollback()}) without adding them to the resource database.
\end{funcdesc}

\begin{funcdesc}{list_pending}{}
  Returns a pair of two lists ([\var{data}],[\var{schema}]), where
  the first list shows newly added pygr.Data IDs that are currently pending,
  and the second list pygr.Data IDs that with newly added schema information
  pending.
\end{funcdesc}

\begin{funcdesc}{addResource}{id,obj,layer=None}
  Add \var{obj} to pygr.Data as resource ID \var{id}, specifically within
  abstract resource \var{layer} if provided.  Queues \var{obj} for addition to 
  the resource database, and marks it with its \member{_persistent_id}
  attribute, whose value is just \var{id}.  For a resource \var{id} 'A.Foo.Bar'
  this method is equivalent to the assignment statement
\begin{verbatim}
pygr.Data.A.Foo.Bar = obj
\end{verbatim}
  This method is provided mainly to enable writing code that automates
  saving of resources, e.g. via code like
\begin{verbatim}
for id,genome in nlmsa.seqDict.prefixDict.items(): # 1st SAVE THE GENOMES
    genome.__doc__ = 'draft genome sequence '+id
    addResource('Bio.Seq.Genome.'+id,genome)
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{deleteResource}{id,layer=None}
  Delete resource \var{id} from the resource database specified by
  \var{layer} if provided (or the default resource database otherwise).
  Also delete its associated schema information.
\end{funcdesc}

\begin{funcdesc}{addSchema}{name,schemaObj,layer=None}
  Add a schema object for the pygr.Data resource indicated by the
  string passed as \var{name}, to the specified \var{layer} if provided
  (or the default resource database otherwise).  For example:
\begin{verbatim}
addSchema('Bio.Genomics.ASAP2.hg17.geneExons',
          pygr.Data.OneToManyRelation(genes,exons,bindAttrs=('exons','gene')))
pygr.Data.save() # SAVE ALL PENDING DATA AND SCHEMA TO RESOURCE DATABASE
\end{verbatim}
\end{funcdesc}
Note that schema information, like pending data, is not saved to 
the resource database until you call \code{pygr.Data.save()}.

The pygr.Data module also provides a directory function for searching
for resource names that begin with a given stem, either in all
databases, or in a specific layer:
\begin{funcdesc}{dir}{prefix,layer=None,asDict=False}
  get list or dict of resources beginning with the specified string.
  If the optional \var{asDict} argument is True, then they are returned
  as a dictionary whose keys are resource names, and whose values are their
  descriptions (taken from the resource object's \member{__doc__} string).
  Otherwise they are returned as a list.
\end{funcdesc}

\begin{funcdesc}{newServer}{name,serverClasses=None,clientHost=None,withIndex=False, host=None, port=5000, excludeClasses=None, downloadDB=None, **kwargs}
  Create and return a new XMLRPC server to serve all pygr.Data resources 
  currently loaded in memory that are capable of XMLRPC client-server
  operation.  The server \var{name} will be used for 
  purposes of XMLRPC communication.  The \var{withIndex=True} option
  will cause the server to also act as a pygr.Data resource database
  accessible via XMLRPC (i.e. add its URL to your PYGRDATAPATH environment
  variable, to make its resources accessible to any Python script).
  In this case, the server will add itself as new pygr.Data layer
  \var{name}, for any Python script that accesses its resource index.

  Currently, newServer() can serve three types of data as remote
  XMLRPC services: \class{NLMSA}, \class{BlastDB}, and \class{AnnotationDB}.

  \var{serverClasses} allows you to specify a list of tuples of
  classes that can be served via XMLRPC.  Each tuple should consist of
  three values: \var{(dbClass,clientClass,serverClass)}, where 
  \var{dbClass} is a normal pygr class, \var{clientClass} is the 
  class to use for the XMLRPC client version of this data, and
  \var{serverClass} is the class to use for the XMLRPC server of
  this data.  If no value is provided to this option, the current
  default is 
\begin{verbatim}
[(seqdb.BlastDB,seqdb.XMLRPCSequenceDB,seqdb.BlastDBXMLRPC),
 (AnnotationDB,AnnotationClient,AnnotationServer),
 (cnestedlist.NLMSA,xnestedlist.NLMSAClient,xnestedlist.NLMSAServer)] 
\end{verbatim}
  The \var{clientHost} option allows you to override the hostname
  that clients will be instructed to connect to.  The default is simply
  the fully qualified hostname of your computer.  But if, for example,
  you wished to access your server by port-forwarding localhost port 5000
  to your server port via SSH, you could pass a \var{clientHost}='localhost'
  setting.

  \var{excludeClasses}, if not None, should be a list of classes that
  should be excluded from the new server.  If None, the default is
  [pygr.sqlgraph.SQLTableBase,pygr.sqlgraph.SQLGraphClustered], since
  such relational database resources are better accessed directly from
  the relational database server, rather than via the XMLRPC server as
  an intermediate step.

  \var{downloadDB}, if not None, should be a file path to a pygr.Data
  shelve file in which a set of downloadable resource records have been
  stored.  See the section "download=True Mode" above for more details.

  \var{host, port} arguments are passed to the \class{XMLRPCServerBase} constructor.
  For details see that section below.

  Once you create a server using this method, you start it using its
  \method{serve_forever()} method.  If the server does not provide its
  own index (i.e. \var{withIndex=False}), then you should first register
  it to your local resource database server (so that clients of that server
  will know about the new services your new server is providing), by
  calling its \method{register()} method.
\end{funcdesc}


\begin{funcdesc}{ResourceDBMySQL}{tablename,createLayer=LAYERNAME}
  Create a resource database in a MySQL database table.
  \var{tablename} is the table to use in the database, in the format
  ``\var{DBNAME.TABLENAME} \var{dbinfo}'', where \var{DBNAME} is the name of the
  database in the MySQL server, and \var{TABLENAME} is the name of
  the table in that database that you wish to use to store the
  resource database.  \var{dbinfo} is optional.
  If provided, it must be a whitespace separated 
  list of arguments for connecting to the MySQL server, of the form
  \var{host} \var{user} \var{passwd}.  You can provide one, two
  or three of these optional arguments.
  If no \var{dbinfo} is provided, host, port, user and password info are obtained
  from your .my.cnf config file as usual for the mysql client.

  To create a new table in the MySQL database (automatically initializing its schema),
  instead of assuming that it already exists, you must provide
  the \var{createLayer} argument, which is saved as the layer name
  of the new resource database.  If pygr.Data finds that it is unable
  to connect to a MySQL database table specified in your PYGRDATAPATH
  it will print a warning message, and ignore the offending database table.
  It will NOT silently create a database table for you in this case.
  The rationale is that whereas a misspelled directory name will result in
  an IOError (thus allowing pygr.Data to detect a bad directory name in PYGRDATAPATH),
  there would be no easy way for pygr.Data to tell whether you simply mistyped the name
  of an existing MySQL table, or whether you actually wanted to create a new MySQL table.

  Example: create a new resource database, give it the layer name ``leelab'',
  and register it in our list of resource databases.
\begin{verbatim}
rdb = pygr.Data.ResourceDBMySQL('pygrdata.index',createLayer='leelab')
\end{verbatim}
  Note that you must provide the \var{createLayer} argument, in order to 
  create a new resource database table.  \class{ResourceDBMySQL} will not
  automatically create a new table without this argument, simply because the
  \var{tablename} you provided does not exist.  In that case, it will
  raise an exception to alert you to the fact that either the correct table name
  was not given, or the table does not exist.
\end{funcdesc}

\begin{funcdesc}{dumps}{obj}
  Provides a pygr.Data-aware pickling service; that is, if
  during pickling of \var{obj} any references are encountered
  to objects that pygr.Data IDs, it will simply save the ID.
  Returns a string pickle of \var{obj}.
  Use pygr.Data.loads() to restore an object pickled using this function.
\end{funcdesc}

\begin{funcdesc}{loads}{data,cursor=None}
  Unpickles the string pickle contained in \var{data} in a pygr.Data-aware
  manner.  I.e. any references in the pickle of the form ``PYGR_DATA_ID:''
  will be retrieved by pygr.Data in the usual way.

  \var{data} should have
  been generated by a previous call to pygr.Data.dumps().

  \var{cursor} if not None, must be a Python DB API 2.0 compliant
  cursor object, that will be used to load any objects that require
  a database connection.
\end{funcdesc}


\subsubsection{pygr.Data Layers}
To provide an intuitive way to refer to different resource databases,
pygr.Data associates ``layer names'' with them.  For example, the layer
name for the first resource database whose path is given relative to 
your home directory is \code{my}, and the first one whose path is given
relative to current directory is \code{here}.  Remote resource databases
(XMLRPC; MySQL) each store their own layer name.  For example, within the
Lee lab, we keep a MySQL resource database whose layer name is ``leelab''.

\begin{itemize}
\item You can specify precisely which layer you want to access by prefixing
your pygr.Data resource name with the desired layer name, e.g.
\begin{verbatim}
nlmsa = pygr.Data.leelab.Bio.MSA.UCSC.hg17_multiz17way()
\end{verbatim}

\item Similarly, you can specify which layer you want to store a resource
or schema, in the same way:
\begin{verbatim}
pygr.Data.leelab.schema.Bio.Annotation.ASAP2.hg17.splicegraph = \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}

\item If you do not specify a layer, pygr.Data uses the first resource
database in its list that returns the desired resource.

\item You can delete a resource and its schema rules from a specific resource
database by specifying its layer name:
\begin{verbatim}
del pygr.Data.leelab.Bio.MSA.UCSC.hg17_multiz17way
\end{verbatim}

\item pygr.Data provides a set of default layer names:
the first resource database whose path is given relative to 
your home directory is \code{my}; the first one whose path is given
relative to current directory is \code{here};the first one whose path is given
relative to the root directory / is \code{system};
the first entry that begins with a relative path 
(ie. a local file path that does not fit any of the preceding
definitions) is \code{subdir};
the first one whose path begins ``http://'' is \code{remote};
the first one whose path begins ``mysql:'' is \code{MySQL}.

\end{itemize}

\subsubsection{pygr.Data Schema Concepts}
Parallel to the pygr.Data namespace, pygr.Data maintains a schema namespace 
that records schema information for pygr.Data resources.  Broadly speaking,
{\em schema} is any relationship that holds true over a set of data in a given
collection (e.g. in the human genome, ``genes have exons'', a one-to-many relation).
In traditional (relational) databases, this schema information is usually
represented by {\em entity-relationship diagrams} showing foreign-key
relationships between tables.  A pygr.Data resource is a collection
of objects (referred to in these docs as a ``container'' or ``database'');
thus in pygr, schema is a relation between pygr.Data resources, i.e.
a relationship that holds true between the items of one pygr.Data resource
and the items of another.  For examples, items in a ``genes'' resource
might each have a mapping to a subset of items in an ``exons'' resource.
This is achieved in pygr.Data by adding the mapping object itself as a pygr.Data
resource, and then specifying its schema to pygr.Data (in this example,
its schema would be a one-to-many relation between the ``genes''
resource and the ``exons'' resource).  Adding the mapping object 
as a pygr.Data resource, and adding its schema information, are
two separate steps.
\begin{verbatim}
pygr.Data.Bio.Genomics.ASAP2.hg17.geneExons = geneToExons # SAVE MAPPING
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.geneExons = \
  pygr.Data.OneToManyRelation(genes,exons,bindAttrs=('exons','gene'))
pygr.Data.save() # SAVE ALL PENDING DATA AND SCHEMA TO RESOURCE DATABASE
\end{verbatim}
assuming that \code{genes} and \code{exons} are the pygr.Data resources
that are being mapped.  This would allow a user to obtain the mapping
from pygr.Data and use it just as you'd expect, e.g. assuming that
\code{gene} is an item from \code{genes}:
\begin{verbatim}
geneToExons = pygr.Data.Bio.Genomics.ASAP2.hg17.geneExons()
myexons = geneToExons[gene] # GET THE SET OF EXONS FOR THIS GENE
\end{verbatim}
In practice, pygr.Data accomplishes this by automatically setting
\code{geneToExon}'s \code{sourceDB} and \code{targetDB} attributes
to point to the \code{genes} and \code{exons} resources, respectively.

Since most users find it easier to remember object-oriented behavior
(e.g. ``a gene has an exons attribute'', rather than ``there exists a 
mapping between gene objects and exon objects, called geneToExons''),
pygr.Data provides an option to bind attributes of the mapped
resource items.  In the example above, we bound an \member{exons} attribute
to each item of \code{genes}, which automatically performs this mapping,
e.g. we can iterate over all exons in a given gene as easily as
\begin{verbatim}
for exon in gene.exons: # gene.exons IS EQUIVALENT TO geneToExons[gene]
  # DO SOMETHING...
\end{verbatim}
Note: in this usage, the user does not even need to know about the 
existence of the \code{geneToExons} resource; pygr.Data will load it
automatically when the user attempts to access the \code{gene.exons}
attribute.  It can do this because it knows the schema of the pygr.Data
resources!

One additional aspect of pygr.Data schema relations goes a bit beyond
ordinary mapping: a mapping between one object (source) and another
(target) can have {\em edge information} that describes this specific
relationship.  For example, the connection
between one exon and another in the alternative splicing of an mRNA
isoform, is a {\em splice}.  For alternative splicing analysis, it is
actually crucial to have detailed information about the splice (e.g.
what experimental evidence exists for that splice; what tissues it was
observed, in what fraction of isoforms etc.) in addition to the exons.
Therefore, pygr.Data allows us to save edge information also as part 
of the schema, e.g. for a \code{splicegraph} representing the set of
all splices (edges) between pairs of exons (nodes), we can
store the schema as follows:
\begin{verbatim}
pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph = splicegraph # ADD A NEW RESOURCE
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.splicegraph = \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
pygr.Data.save() # SAVE ALL PENDING DATA AND SCHEMA TO RESOURCE DATABASE
\end{verbatim}
This type of mapping (``edge'' relations between pairs of ``nodes'')
is referred to in mathematics as a {\em graph}, and has very general
utility for many applications.  For further information on graphs in 
pygr, see the tutorial or the \module{mapping} module reference below.

What information does pygr.Data schema actually store?  In practice,
the primary information stored is {\em attribute} relations: 
i.e. for a specified resource ID, a specified attribute name
should be added to the resource object (or to items obtained
from it), which in turn maps to some specified target resource
(or items of that resource).  

Although users do not need to know
how this information is saved, I will outline the methodology
as a reference for developers who want to work directly with this
internal data (skip this section otherwise).
\begin{itemize}
\item In a given resource database (dictionary), information for constructing a 
given resource \code{id} is stored with its resource ID as the key.  
i.e. if \code{rdb} is a resource database, \code{rdb[id]} gives
the string to unpickle to construct the resource.  Schema information
for that resource is stored as \code{rdb['SCHEMA.'+id]}.

\item This schema information (for a given resource) is itself
a dictionary, whose keys are attribute names to bind to this 
resource, and whose associated values are themselves dictionaries
specifying the rules for what to bind to this attribute and how.
See below for further details.

\item Attributes are added as ``shadow attributes'' provided by 
descriptors added to the class object for the resource or to
its \member{itemClass} or \member{itemSliceClass} object if the
attribute is to be bound to {\em items of the resource}.  Descriptors
(also referred to in the Python documentation as ``properties'')
are the major mechanism by which Python new-style classes
(i.e. subclasses of \class{object} in Python 2.2 and later)
can execute code in response to a user attempt to get an
object attribute, and are definitely preferable over writing
\method{__getattr__} method code if all that's desired
is an attribute with a specified name.  For more information
on descriptors, see the Python Reference Manual.

\item The basic principles of these ``shadow attributes'' are that
1. they are bound to the class object, not the instance object;
2. they are only invoked if the specified attribute name is 
missing from the instance object's \member{__dict__}; 
3. once invoked, they save their
result on the instance object (in its \member{__dict__})
as the same-named attribute; 4. thus, the descriptor method
will only be called once; thereafter the attribute will be 
obtained directly from the value cached on the instance object;
5. the descriptor only loads its target resource(s) when the user
attempts to read the value of the attribute.  Thus no extra
resources are loaded until the user actually demands information
that requires them.

\item Currently, these shadow attributes are implemented by
three different descriptor classes in pygr.Data: 
\class{OneTimeDescriptor}, for binding attributes directly on a resource
object (container);
 \class{ItemDescriptor}, for binding attributes on items (or slices of
items) obtained from a resource object (via its __getitem__ method);
\class{SpecialMethodDescriptor}, for binding special Python methods like
\method{__invert__}.

\item The rule information for a given attribute is itself a dictionary,
with the following string keys governing the behavior of the shadow attribute.
\var{targetID}: the pygr.Data resource ID of the resource that this
attribute links to.
\var{itemRule}: True if the attribute should be bound to {\em items}
(and slices of items, if defined) of the source resource, rather than
directly to the source resource object itself (if itemRule=False).
\var{invert}: True if the target resource should first be inverted
(i.e. query its reverse-mapping rather than its forward-mapping), False otherwise.
\var{getEdges}: True if the attribute should query the target resource's
\member{edges} mapping (i.e. the mapping provided by its \member{edges} attribute)
rather than its forward mapping, False otherwise.
\var{mapAttr}: if not None, use this named attribute of our source object,
instead of the source object itself, as the key for search the target resource
mapping.
\var{targetAttr}: if not None, return this named attribute of the result of
the search, rather than the result of the search itself.
\end{itemize}

\subsubsection{Collection}
Provides a \class{dict}-like container that can be directly saved as a
container in pygr.Data.  Ordinary \class{dict} instances cannot be 
conveniently saved as pygr.Data resources, because they do not allow
attributes to be saved (which is required for storing pygr.Data information
like _persistent_id and itemClass), and because older versions of Python
have a bug that affects pickling of dicts with cyclic references (i.e. contents
that refer to the container).  \class{pygr.Data.Collection} provides a drop-in
substitute that uses \class{dict} or a Python \class{shelve} 
as its internal storage, and provides
a full dict-like interface externally.  It takes several arguments:

\var{saveDict}, if not None, is the internal mapping to use as our storage.

\var{filename}: if provided, is a file path to a shelve (BerkeleyDB) file to
  store the data in.  NOTE: if you add data to a Collection stored in such a file,
you {\em must} call the Collection's \method{close}() method to ensure
that all the data will be saved to the Python shelve.  Otherwise, the 
Python shelve file might be left in an incomplete state.
NOTE: opening a collection with the \var{filename} option will cause
it to use the PicklableShelve or IntShelve class for the Collection.

\var{mode=None} is passed to the Python \method{shelve.open}() function
to control whether \var{filename} is opened in read, write or create mode;
see the Python \module{shelve} module documentation for details.  If \var{mode}
is None, it will first try to open the shelve in mode 'r' (read-only),
but if the file is missing, will open it in mode 'c' (create).

\var{writeback=True} is passed to the Python \method{shelve.open}() function
to control the saving of data to the shelve.  
See the Python \module{shelve} module documentation for details.
The default \var{writeback=True} setting can consume large amounts of
memory if you are writing a lot of data to the shelve.  To avoid
this problem, use \var{writeback=False}; note that this means updates
to the shelve will only be saved when you explicitly set an item
in the Collection (e.g. \code{collection[k] = v}; specifically, if
\code{v} is a mutable object, subsequently changing the contents of
\code{v} will not automatically update the \module{shelve}, whereas
it would be with \var{writeback=True}).
   
\var{dictClass}: if provided, is the class to use for storage of the dict data.

For example,
\begin{verbatim}
ens_genes = pygr.Data.Collection(itemClass=Transcript) # DICTIONARY OF GENES
ens_genes[gene_id] = gene
\end{verbatim}
pygr.Data generally needs to know the \class{itemClass} of items stored
inside a resource, so that it can add shadow attributes (by adding properties,
directly to the itemClass).

\begin{funcdesc}{close}{}
  You must call this method to ensure that any data added to the Collection
  will be written to its Python shelve file on disk.
  This method is irrelevant, but harmless,
  if you are instead using an in-memory dictionary as storage.
\end{funcdesc}

\subsubsection{Mapping}
This class provides dict-like class suitable for persistent usages.  
It extracts ID values from
keys and values passed to it, and saves these IDs into its internal dictionary
instead of the actual objects.  Thus, the external interface is objects,
but the internal storage is ID values.  This allows the mapping to be stored
persistently (i.e. pickled) separately from the objects which it maps, 
because only IDs are stored in the \class{Mapping}.

You can use any object that obeys the
Python mapping protocol (e.g. \class{dict}, or Python \module{shelve}) 
as the internal storage.  \class{Mapping} behaves exactly like a standard
Python dictionary, providing all the standard methods of the Mapping Protocol.

\begin{funcdesc}{Mapping}{sourceDB, targetDB, saveDict=None, IDAttr='id', targetIDAttr='id', itemAttr=None, multiValue=False, inverseAttr=None,filename=None,dictClass=None,mode=None}
  Initializes a mapping between items of \var{sourceDB} and items of \var{targetDB}.

  \var{sourceDB}: container whose items will serve as keys for this Mapping.
   i.e. \var{sourceDB} must be a dictionary that maps key ID values to key objects.

  \var{targetDB}: container whose items will serve as values of this Mapping.
  i.e. \var{targetDB} must be a dictionary that maps value IDs to value objects.

  \var{saveDict}, if not None, is the internal mapping to use as our storage.
  If None, attempts to open or create a suitable storage for you.
  See also the \var{filename}, \var{dictClass} and \var{mode} arguments.
  If none of these arguments are provided, a standard Python dictionary will be used.

  \var{IDAttr}: attribute name to obtain an ID from a key object.

  \var{targetIDAttr}: attribute name to obtain an ID from a value object.

  \var{itemAttr}, if not None, the attribute to obtain target (value) ID
  from an internal storage value

  \var{multiValue}: if True, treat each value as a list of values, i.e. this
  Mapping will serve as a one-to-many mapping from \var{sourceDB} to \var{targetDB}.

  \var{inverseAttr}, if not None, attribute name to obtain a source ID from
  a value object.

  \var{filename}: if not None, is a file path to a shelve (BerkeleyDB) file to
  store the data in.

  NOTE: if you add data to a Mapping stored in such a disk file,
  you {\em must} call the Mapping's \method{close}() method to ensure
  that all the data will be saved to the Python shelve.  Otherwise, the 
  Python shelve file might be left in an incomplete state.

  \var{mode}: if not None, specifies how the shelve file should be opened:
  'r' (read-only), 'c' (create), 'w' (read/write).  For more details see the
  Python Library \module{shelve} documentation.
   
  \var{dictClass}: if not None, is the class to use for storage of the dict data.
\end{funcdesc}

\begin{funcdesc}{close}{}
  You must call this method to ensure that any data added to the Mapping
  will be written to its Python shelve file on disk.
  This method is irrelevant, but harmless,
  if you are instead using an in-memory dictionary as storage.
\end{funcdesc}



Here's an example usage:
\begin{verbatim}
gene_exons = Mapping(ens_genes, exon_db, multiValue=True, inverseAttr='transcript_id')
for exon in exon_db:
    gene = ens_genes[exon.transcript_id]
    exons = gene_exons.get(gene, [])
    exons.append(exon)
    gene_exons[gene] = exons # SAVE EXPANDED EXON MAPPING LIST
# SAVE TO PYGR DATA, AND CREATE GENES -> EXONS SCHEMA RELATION
pygr.Data.Bio.Titus.Test1.GeneExons = gene_exons
pygr.Data.schema.Bio.Titus.Test1.GeneExons = \
     pygr.Data.OneToManyRelation(ens_genes,exon_db,bindAttrs=('exons','gene'))
pygr.Data.save() # SAVE ALL PENDING DATA AND SCHEMA TO RESOURCE DATABASE
\end{verbatim}

\subsubsection{ResourceFinder}
The core functionality of the pygr.Data module is provided by the
\class{ResourceFinder} class, an instance of which is created at the
top-level of the module as \code{pygr.Data.getResource}.  It 
provides methods for adding, deleting and controlling pygr.Data
resources and schema.

\begin{funcdesc}{getResource}{id, layer=None, debug=None, download=False, *args, **kwargs}
  Look up pygr.Data resource \var{id}, using the specified abstract
  resource \var{layer} if provided.  Searches the resouce database(s)
  for \var{id}, constructs it from the saved resource rule (e.g. from
  a local resource database, by unpickling the object).  Saves the 
  object in its cache so that subsequent calls for the same resource
  ID will return the same object.  Applies the stored pygr.Data schema
  rules to it using \method{applySchema}().  Marks the object with
  its \member{_persistent_id} attribute, whose value is just \var{id}.

  The \var{download=True} option forces pygr.Data to restrict the
  search to downloadable resources.  If a downloadable resource
  matching the requested ID is found, it will be downloaded to a local
  file, uncompressed, and any necessary initialization steps 
  performed automatically.  The returned object will be a fully
  initialized local copy of the requested resource.

  Passing the option \var{debug=True} will cause it to raise any 
  exception that occurs during resource loading immediately, rather
  than continuing to search its resource database list.  This is
  helpful for debugging purposes.
\end{funcdesc}

\begin{funcdesc}{getResource.addResource}{id,obj,layer=None}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.addSchema}{name,schemaObj,layer=None}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.dir}{prefix,layer=None,asDict=False}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.deleteResource}{id,layer=None}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.dumps}{obj}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.list_pending}{}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.loads}{data,cursor=None}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.newServer}{name,serverClasses=None,clientHost=None,withIndex=False, host=None, port=5000, **kwargs}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.rollback}{}
  Same as the top-level module function of the same name.
\end{funcdesc}

\begin{funcdesc}{getResource.save_pending}{layer=None}
  Same as the top-level module function \code{pygr.Data.save()}.
\end{funcdesc}

The following methods are mainly for internal use, and are unlikely to be
needed by users of pygr.Data.  In general, you should not use them unless
you have a very good reason to be working with the interal pygr.Data 
methods, and really know what you are doing!
\begin{funcdesc}{update}{}
  Update \code{getResource}'s list of resource databases, by parsing the environment
  variable PYGRDATAPATH and attempting to connect to the resource databases
  listed there.  Does not return anything.
\end{funcdesc}

\begin{funcdesc}{addLayer}{layerName,rdb}
  Add the resource database \var{rdb} to the current resource database list,
  as a named layer given by the string \var{layerName}.  Over-writing an 
  existing layer name is not allowed, for security reasons; 
  the previous layer entry must first be deleted.
\end{funcdesc}

\begin{funcdesc}{getLayer}{layerName}
  Get the specified resource database, by its layer name.  If \var{layerName}
  is None, returns the default (first) resource database in its list.
\end{funcdesc}

\begin{funcdesc}{resourceDBiter}{}
  Generates all the resource databases currently listed by \code{getResource}.
\end{funcdesc}

\begin{funcdesc}{registerServer}{locationKey,serviceDict}
  Registers the set of resources specified by \var{serviceDict} to the
  first resource database index in PYGRDATAPATH that will accept them.
  \var{serviceDict} must be a dictionary whose keys are resource IDs and
  whose associated values are pickled resource objects (encoded as strings).
  \var{locationKey} should be a string name chosen to represent the ``location''
  where the data are stored.  This can be anything you wish, and is mainly used
  to let the user know where the data will come from.  This might be used
  in future versions of pygr.Data to allow preferential screening of where
  to get data from (local disk is better than NFS mounted disk, which in turn
  might be preferable over remote XMLRPC data access).
\end{funcdesc}

\begin{funcdesc}{findSchema}{id}
  Returns a dictionary for the schema (if any) found for the pygr.Data resource 
  specified by \var{id}.  The dictionary keys are attribute names (representing
  attributes of the specified resource or its contents that should have 
  schema relations with other pygr.Data resources), and whose values are
  themselves dictionaries specifying the precise schema rules for constructing
  this specific attribute relation.
\end{funcdesc}

\begin{funcdesc}{schemaAttr}{id,attr}
  Return the target data linked to by attribute \var{attr} of pygr.Data
  resource \var{id}, based on the stored pygr.Data schema.  The target resource
  object will be obtained by pygr.Data.getResource as usual.
\end{funcdesc}

\begin{funcdesc}{applySchema}{id,obj}
  Apply the pygr.Data schema for resource \var{id} to the actual data
  object representing it (\var{obj}), by decorating it (and / or its itemClass
  and itemSliceClass) with properties representing its schema attributes.
  These properties are implemented by adding descriptor attributes to the
  associated class, such as \class{OneTimeDescriptor} or \class{ItemDescriptor}.
\end{funcdesc}

\begin{funcdesc}{saveResource}{resID,obj,layer=None}
  Raw interface to actually save a specific resource to the specified
  (or default) resource database.
  DO NOT use this internal interface unless you know what you are doing!
\end{funcdesc}

\begin{funcdesc}{saveSchema}{id,attr,bindingDict,layer=None}
  Save a schema attribute relation for attribute \var{attr} of pygr.Data
  resource \var{id}, to the specified resource database \var{layer} (or the default,
  first resource database in the list, if no layer specified).
  \var{bindingDict} must be a dictionary specifying the rules for
  binding the attribute to a pygr.Data resource target; see below for details.
  DO NOT use this internal interface unless you know what you are doing!
\end{funcdesc}

\begin{funcdesc}{delSchema}{id,layer=None}
  Delete schema bindings for all attributes of the resource \var{id}, in
  the specified resource database \var{layer}, as well as all schema relations
  on other resources that are targeted to resource \var{id}.
\end{funcdesc}

\subsubsection{ResourceDBMySQL}
Implements an interface to storage of a resource database in a MySQL
database table.
\begin{funcdesc}{__init__}{tablename,finder=None,createLayer=None}
  \var{tablename} is the table to use in the database, in the format
  ``\var{DBNAME.TABLENAME} \var{dbinfo}'', where \var{DBNAME} is the name of the
  database in the MySQL server, and \var{TABLENAME} is the name of
  the table in that database that you wish to use to store the
  resource database.  \var{dbinfo} is optional.
  If provided, it must be a whitespace separated 
  list of arguments for connecting to the MySQL server, of the form
  \var{host} \var{user} \var{passwd}.  You can provide one, two
  or three of these optional arguments.
  If no \var{dbinfo} is provided, host, port, user and password info are obtained
  from your .my.cnf config file as usual for the mysql client.

  \var{finder}, if specified gives the \class{ResourceFinder} instance
  in which the new resource DB should be registered.  If None provided,
  defaults to pygr.Data.getResource.

  \var{createLayer}, if specified forces it to create a new table
  in the MySQL database (instead of assuming that it already exists),
  and saves \var{createLayer} as the layer name of this resource database.

  Example: create a new resource database, give it the layer name ``leelab'',
  and register it in our list of resource databases.
\begin{verbatim}
rdb = pygr.Data.ResourceDBMySQL('pygrdata.index',createLayer='leelab')
\end{verbatim}
  Note that you must provide the \var{createLayer} argument, in order to 
  create a new resource database table.  \class{ResourceDBMySQL} will not
  automatically create a new table without this argument, simply because the
  \var{tablename} you provided does not exist.  In that case, it will
  raise an exception to alert you to the fact that either the correct table name
  was not given, or the table does not exist.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  Get resource \var{id} from this resource database, or \code{KeyError} 
  if not found.
\end{funcdesc}

\begin{funcdesc}{__delitem__}{id}
  Delete resource \var{id} from this resource database, or \code{KeyError} 
  if not found.
\end{funcdesc}

\begin{funcdesc}{__setitem__}{id,obj}
  Save resource \var{id} to this resource database, by pickling it
  with \code{self.finder.dumps(obj)}.
\end{funcdesc}

\begin{funcdesc}{registerServer}{locationKey,serviceDict}
  Saves the set of resources specified by \var{serviceDict} to the
  database.
  \var{serviceDict} must be a dictionary whose keys are resource IDs and
  whose associated values are pickled resource objects (encoded as strings).
  \var{locationKey} should be a string name chosen to represent the ``location''
  where the data are stored.  This can be anything you wish, and is mainly used
  to let the user know where the data will come from.  This might be used
  in future versions of pygr.Data to allow preferential screening of where
  to get data from (local disk is better than NFS mounted disk, which in turn
  might be preferable over remote XMLRPC data access).
\end{funcdesc}

\begin{funcdesc}{setschema}{id,attr,ruleDict}
  Save schema information for attribute \var{attr} on resource \var{id}
  by pickling the \var{ruleDict}.
\end{funcdesc}

\begin{funcdesc}{delschema}{id,attr}
  Delete schema information for attribute \var{attr} on resource \var{id}.
\end{funcdesc}

\begin{funcdesc}{getschema}{id}
  Get schema information for resource \var{id}, in the form of a dictionary
  whose keys are attribute names, and whose values are the associated
  schema \var{ruleDict} for each bound attribute.
\end{funcdesc}

\subsubsection{ResourceDBShelve}
Implements an interface to storage of a resource database in a Python
\module{shelve} (i.e. BerkeleyDB file) stored on local disk.
Provides the same interface as \class{ResourceDBMySQL}, except for
no \method{registerServer} method.  Note: any method call that would
save information to the database temporarily re-opens the database
file in write mode, saves the required information, and immediately
closes and re-opens
the databae in read-only mode.  Thus, unless two clients try
to save information to the same file at exactly the same time,
successive writes by multiple clients will not interfere with each
other.
\begin{funcdesc}{__init__}{dbpath,finder,mode='r'}
  \var{dbpath} is the path to the directory in which the shelve
  file is found (or should be created, if none present).
\end{funcdesc}

\subsubsection{ResourceDBClient}
Implements a client interface to storage of a resource database in an XMLRPC
server.  For security reasons, only provides the \method{__getitem__},
and \method{registerServer} methods.

\subsubsection{ResourceDBServer}
Implements a server interface for storage of a resource database in 
a standard Python dict, served to clients via an XMLRPC
server (use \class{coordinator.XMLRPCServerBase} as the XMLRPC
server to serve this object).  

\begin{funcdesc}{__init__}{layerName,readOnly=True}
  \var{layerName} is the layer name that this server will provide
  to pygr.Data clients.  \var{readOnly} if True, makes the server reject
  any requests to add new database rules received via XMLRPC, i.e.
  only allows \method{getName} and \method{getResource} calls via XMLRPC.
  If False, also allows calls to \method{registerServer} and \method{delResource}.
\end{funcdesc}

\subsubsection{ResourcePath}
Used for providing the dynamically extensible pygr.Data namespace
that provides the normal interface for users to access pygr.Data resources.
\begin{funcdesc}{__init__}{namepath,layerName=None}
  \var{namepath} specifies the ID string to use for this resourcePath.
  \var{layerName} if specified, gives the layer name that should be used
  for finding this resource and any subattributes of it.

  For example, \code{Bio} is added at the top-level of the pygr.Data module
  by the following code:
\begin{verbatim}
Bio = ResourcePath('Bio')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{__getattr__}{attr}
  extends the resource path by one step, returning a
  \class{ResourcePath} object representing the requested attribute.
\end{funcdesc}

\begin{funcdesc}{__setattr__}{attr,obj}
  saves \var{obj} as the specified resource ID, by calling
  \method{getResource.addResource}, with our layer name (if any).
\end{funcdesc}

\begin{funcdesc}{__delattr__}{attr}
  deletes the specified resource ID, by calling
  \method{getResource.deleteResource}, with our layer name (if any).
\end{funcdesc}

\begin{funcdesc}{__call__}{*args,**kwargs}
  Construct the specified resource ID, by calling \method{getResource},
  with our layer name (if any), and the specified arguments (if any).
\end{funcdesc}

\subsubsection{SchemaPath}
Class for top-level object representing a schema namespace.  e.g. in the pygr.Data
module,
\begin{verbatim}
schema = SchemaPath() # CREATE ROOT OF THE schema NAMESPACE
\end{verbatim}

\subsubsection{ResourceLayer}
Class for top-level object representing a pygr.Data layer.  e.g. in the pygr.Data
module,
\begin{verbatim}
here = ResourceLayer('here') # CREATE TOP-LEVEL INTERFACE TO here LAYER
\end{verbatim}

\subsubsection{ForeignKeyMap}
Provides a mapping between two containers, assuming that items of the target
container have a foreign key attribute that gives the ID of an item in the source
container.
\begin{funcdesc}{ForeignKeyMap}{foreignKey,sourceDB=None,targetDB=None}
  \var{foreignKey} must be a string attribute name for the foreign key on
  items of the \var{targetDB}.  Furthermore, \var{targetDB} must provide
  a \method{foreignKey} method that takes two arguments: the \var{foreignKey} attribute name,
  and an identifier that will be used to search its items for those whose attribute
  matches this identifier.  It must return an iterator or list of the matching items.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  get a list of items in \var{targetDB} whose attribute matches this \var{id}.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  get an interface to the reverse mapping, i.e. mapping object that takes an
  item of \var{targetDB}, and returns its corresponding item from \var{sourceDB},
  based on the input item's foreign key attribute value.
\end{funcdesc}

For example, given a container of clusters, and a container of exons (that each
have a \member{cluster_id} attribute), we create a mapping between them as follows:
\begin{verbatim}
m = ForeignKeyMap('cluster_id',clusters,exons)
for exon0 in m[cluster0]: # GET EXONS IN THIS CLUSTER
    do something...
cluster1 = (~m)[exon1]  # GET CLUSTER OBJECT FOR THIS EXON
\end{verbatim}


\subsubsection{ManyToManyRelation, OneToManyRelation, ManyToOneRelation, OneToOneRelation}
Convenience class for constructing schema relations for
a general graph mapping from a sourceDB to targetDB with edge info.
\begin{funcdesc}{__init__}{sourceDB,targetDB,edgeDB=None,bindAttrs=None}
  \var{sourceDB},\var{targetDB}, and \var{edgeDB} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
  \var{bindAttrs}, if provided, must give a list of string attribute names to be
  bound, in order, to items of \var{sourceDB}, \var{targetDB},
  and \var{edgeDB}, in that order.  A None value in this list simply
  means that no attribute binding will be made to the corresponding
  pygr.Data resource.
\end{funcdesc}
Note: this class simply records the information necessary for this
schema relation.  The information is not actually saved to the resource
database until its \method{saveSchema} method is called by 
the \class{SchemaPath} object.  In addition to saving attribute
bindings given by \var{bindAttrs}, this will also create bindings
on the mapping resource object itself (i.e. the resource whose
schema is being set; see an example in the tutorial).  Specifically,
it will save bindings for its \member{sourceDB},\member{targetDB},
and \member{edgeDB} attributes to the corresponding resources
given by the \var{sourceDB},\var{targetDB},
and \var{edgeDB} arguments.

\class{OneToOneRelation}, \class{OneToManyRelation}, \class{ManyToOneRelation}
and \class{ManyToManyRelation} differ only in the uniqueness vs. multiplicity
of the mapping indicated.
E.g.  \textasciitilde\code{m1[v] --> k} vs. 
\textasciitilde\code{mMany[v] --> [k1,k2,...]}

\subsubsection{DirectRelation, ItemRelation, InverseRelation}
Users are unlikely to have any reason to work directly with these
internal interfaces.  Instead, use \class{ManyToManyRelation, OneToManyRelation, ManyToOneRelation, OneToOneRelation}
as these cover the normal schema relationships.
You should only use internal interfaces like 
\class{DirectRelation, ItemRelation, InverseRelation} if you
have a real need to do so, and really know what you are doing!
This documentation is only provided for developers directly working
on pygr internals.

\class{DirectRelation} is a convenience class for constructing 
a single schema attribute relation on a pygr.Data resource,
linking it to another pygr.Data resource.
\begin{funcdesc}{__init__}{target}
  \var{target} gives a reference to a pygr.Data resource, which will
  be the target of a bound schema attribute.  \var{target} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
\end{funcdesc}

\begin{funcdesc}{schemaDict}{}
  returns a basic \var{ruleDict} dictionary for saving this schema binding.
  Can be over-ridden by subclasses to customize schema binding behavior.
\end{funcdesc}

\begin{funcdesc}{saveSchema}{source,attr,layer=None,**ruleDict}
  Saves a schema binding for attribute \var{attr} on pygr.Data resource
  \var{source} to the specified resource database \var{layer} (or
  to the default resource database if not specified).  \var{ruleDict}
  if specified provides additional binding rules (which can add to or
  over-ride those returned by the \method{schemaDict} method).
  \var{source} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
\end{funcdesc}

\class{ItemRelation} provides a subclass of \class{DirectRelation}
that binds to the {\em items} of resource \var{source} rather than to the
\var{source} object itself.

\class{InverseRelation} provides a subclass of \class{DirectRelation},
that binds \var{source} and \var{target} as each other's inverse mappings.
That is, it binds an \member{inverseDB} attribute to each resource
that points to the other resource.  When either resource is loaded,
a special \method{__invert__} method will be added, that simply
loads and returns the resource pointed to by the \member{inverseDB}
binding.

\subsubsection{nonPortableClasses,SourceFileName}
The variable \var{pygr.Data.nonPortableClasses} specifies a list of
classes which have local data dependencies (e.g. requires reading a file
that is on your local disk),
and therefore cannot be transferred over XMLRPC to a remote client
by simple pickling / unpickling.  \method{pygr.Data.newServer} will
automatically cull any data that has such dependencies from the list
of resources it loads into the XMLRPC server it constructs, so that
the server will not attempt to serve data that actually will not work
on remote clients.  You can add your own classes to this list if
needed.

By default, the \var{pygr.Data.nonPortableClasses} list consists of simply a single
class, \class{pygr.Data.SourceFileName}, which is a subclass of str
that marks a string as representing a path to a file.  It behaves
just like a string, but allows pygr.Data to be smart about checking
whether the required file actually exists and is readable before returning
a resource to the user.  If you save filenames on your own objects using
this class, pygr.Data will therefore be able to handle them properly for
many issues such as XMLRPC portability to remote clients.  You do this simply
as follows:
\begin{verbatim}
class Foo(object):
  def __init__(self,filename):
    self.filename = SourceFileName(str(filename)) # MARK THIS A BEING A FILE NAME
    ifile = file(self.filename) # OPEN THIS FILE NOW IF YOU WANT...
\end{verbatim}

\subsection{downloader Module}
The downloader module supports automatic download of resources,
primarily for pygr.Data.  It consists of several pieces:
\begin{itemize}
\item \class{SourceURL}: use this class to create a picklable reference
to a resource that can be downloaded from a specific URL.  

\item download_unpickler: Unpickling the
SourceURL object (e.g. when it is retrieved from pygr.Data) will trigger
downloading of the resource from this URL, as a local file.  That means
that the unpickled form of the SourceURL is simply the local filename
where the resource is now stored.

\item unzip, gunzip, untar support: If the downloaded resource
is a zip archive, a gzip compressed file, or a tar archive (possibly
with compression), it will by default be uncompressed / extracted.
Currently, the following file suffixes are recognized automatically:
\code{.gz} (gunzip); \code{.zip} (unzip); \code{.tar}, 
\code{.tar.gz}, \code{.tar.bz2} or \code{.tgz} (extract tar
archive, with uncompression if appropriate).
\end{itemize}

\subsubsection{SourceURL}
This class exists solely to be saved in pygr.Data (or any other
pickle-based persistence storage) so that unpickling the object
will trigger automatic download of a desired URL.

\begin{funcdesc}{SourceURL}{path, filename=None, **kwargs}
  \var{path} must be a URL to a downloadable file.

  \var{filename}, if given, will be used as the local filename
  to save the file as.  A resource should be given a local
  filename that uniquely identifies this resource (e.g. "hg17"
  instead of "chromFa" as is often used by UCSC genome packages).
  Without a unique filename, there is no way to guarantee that
  name collisions will not occur, potentially over-writing other
  local resource files.

  Additional \var{kwargs} are simply passed on to the download
  unpickler.  Currently, the only useful option is \var{singleFile=True},
  which forces extraction of a multi-file archive (zip or tar) to
  be concatenated into a single file (useful for a genome sequence
  database stored as a zip archive of multiple FASTA files, one for
  each chromosome).
\end{funcdesc}

Here is an example of saving a downloadable file reference to
pygr.Data:
\begin{verbatim}
dfile = pygr.Data.SourceURL('http://biodb.bioinformatics.ucla.edu/PYGRDATA/dm2_multiz9way.txt.gz')
dfile.__doc__ = 'DM2 based nine genome alignment from UCSC in textfile dump format'
pygr.Data.Bio.MSA.UCSC.dm2_multiz9way.txt = dfile # SAVE AS RESOURCE NAME
nbuilder = NLMSABuilder(dfile) # will make NLMSA from this when unpickled
nbuilder.__doc__ = 'DM2 based nine genome alignment from UCSC'
pygr.Data.Bio.MSA.UCSC.dm2_multiz9way = nbuilder
pygr.Data.save() 
\end{verbatim}

When this object is retrieved from pygr.Data, that will trigger  
construction of the binary indexes (in a location that can be  
controlled by the PYGRDATABUILDDIR environment variable).  The product  
of the unpickling will simply be a properly initialized NLMSA object  
using these index files. i.e.
\begin{verbatim}
nlmsa = pygr.Data.Bio.MSA.UCSC.dm2_multiz9way(download=True) # get the download
\end{verbatim}

\subsubsection{NLMSABuilder}
This class is defined in the module \module{nlmsa_utils}, but its
sole purpose is to support auto-download of NLMSA alignments.
When unpickled, it triggers unpacking of NLMSA binary index files
from a NLMSA textdump file (usually provided using a \class{SourceURL}
object).  For a usage example, see the previous section immediately
above.
 
\begin{funcdesc}{NLMSABuilder}{filepath, **kwargs}
  \var{filepath} should be a \class{SourceURL} object.  When an NLMSABuilder
  object is pickled, it saves its filepath in the pickle.  When 
  the NLMSABuilder is unpickled, the filepath object is also unpickled,
  which will trigger downloading of the \class{SourceURL} file, 
  which in turn will be used by the nlmsa_textdump_unpickler
  to extract the binary index files for the NLMSA.  The final product
  of unpickling an NLMSABuilder object is simply the fully initialized
  NLMSA object that it constructed.

  \var{kwargs} can be any keyword arguments understood by the
  textdump_to_binaries() function (see \module{cnestedlist} module
  documentation for details).
\end{funcdesc}

\subsubsection{GenericBuilder}
\begin{funcdesc}{GenericBuilder}{classname, *args, **kwargs}
  \var{classname} should be a string specifying the name of
  the class to be used for building the resource when this
  object is unpickled by pygr.Data.  As a security precaution,
  this class name is checked against the unpickler's list of
  allowed target classes.  Currently, the only 
  allowed target class is 'BlastDB'.

  To build the target resource upon unpickling, the
  target class is simply called with the exact same list
  of arguments and keyword arguments (less the initial \var{classname}
  argument) as originally supplied to \class{GenericBuilder}.
\end{funcdesc}

\subsubsection{Environment Variables Controlling Downloads}
Two environment variables control where downloaded files will be
stored:
\begin{itemize}
\item \code{PYGRDATADOWNLOAD}: sets the directory where files will
be downloaded to.  If it is not set, files are downloaded to the current
directory.

\item \code{PYGRDATABUILDDIR}: sets the directory where indexes will
be saved in subsequent steps that may occur after download of
a resource, e.g. NLMSA index files.  If not set, indexes will be
saved in the current directory.
\end{itemize}

\subsubsection{Performance and Platform Independence Issues}
Uncompression and archive extraction depend on tools such gunzip,
which create performance vs. platform-independence issues, as
summarized here:
\begin{itemize}
\item Python provides platform-independent modules \module{tarfile},
\module{gzip}, \module{zipfile}, so Pygr uses these, with the following
caveats.

\item The Python module \module{gzip} appears to be about half the 
speed of the command line program \code{gunzip} on UNIX.  Therefore,
Pygr attempts first to run the \code{gunzip} program if available; if 
not, it uses the \module{gzip} module.

\item The Python module \module{zipfile} only provides an interface
to read an entire file from the archive into memory (!), which is
impractical for very large datasets.  Instead, we just want to extract
each archive file directly to disk.  We therefore use the UNIX 
program \code{unzip} to do this.  If that fails, we try using 
the \module{zipfile} module.
\end{itemize}

\subsection{sqlgraph Module}
\label{sqlgraph-module}
This module provides back-end database access.

\subsubsection{SQLTableBase}
The base class for \class{SQLTable} and other variants below.
This class is derived from
the Python builtin \class{dict} class, so all standard methods of \class{dict}
can be used.

\subsubsection{SQLTable}
Provides a \class{dict}-like interface to an SQL table.  It accepts
an identifier as a key, and returns a Python object representing
the corresponding row in the database.  Typically, these ``row''
objects have an \member{id} attribute that represents the
primary key, and all column names in the SQL table can be
used as attribute names on the row object.

This class assumes that the database table has a primary key,
which is used as the key value for the dictionary.  For tables
with no primary key see other variants below.

\begin{funcdesc}{SQLTable}{name, cursor=None, itemClass=None, attrAlias=None, clusterKey=None,maxCache=None, arraysize=1024}
  Open a connection to the existing SQL table specified by \var{name}.
  You can supply a Python DB API \var{cursor} providing a connection
  to the database server.  If \var{cursor} is None, it will attempt
  to connect to a MySQL server using authentication information either
  from your the \var{name} string (treated as a whitespace separated
  list in the form \var{tablename} \var{host} \var{user} \var{passwd};
  at least \var{tablename} and \var{host} must be present), or from your 
  .my.cnf configuration file in the usual MySQL way (in which case only
  \var{tablename} needs to be specified).

  \var{itemClass} indicates 
  the class that should be used for constructing item objects (representing
  individual rows in the database).

  \var{attrAlias}, if provided, must be a dictionary whose keys are
  attribute names that should be bound to items from your database,
  and whose values are an SQL column name or SQL expression that should
  be used to obtain the value of the bound attribute.

  \var{clusterKey}, if provided, is a caching hint for speeding up
  database access by ``clustering'' queries to load an entire block
  of rows that share the same value of the specified \var{clusterKey} column.
  This caching hint is only used by the \class{Clustered} SQLTable variants
  described in detail below.

  \var{maxCache}, if not None, specifies the maximum number of database
  objects to keep in the cache.  For large databases, this is an important
  parameter for ensuring that \class{SQLTable} will not consume too much
  memory (e.g. if you iterate over all or a large fraction of the items
  in the database).

  \var{arraysize}: specifies the number of rows to be transfered from the
  database server in each cursor.fetchmany() operation.  This can be important
  for speeding up data transfer from the database server.

\end{funcdesc}

This class and its variants follow a simple rule for controlling
how data is loaded into memory.  For the most common usage,
iterating over the objects in the database, you should use the
iterator methods \method{iteritems}() (which yields tuples of (\var{id,obj})),
or \method{itervalues}() (which just yields each object).  These methods
use the parameters \var{maxCache} and \var{arraysize} to control the
size of caching and data transfer from the database server (see details above).
This allows you to keep tight control over the total memory usage of \class{SQLTable}
when iterating over all the items in a very large database, and also to ensure
efficient data transfer using the Python DB API 2.0 \method{fetchmany}() method.
\begin{funcdesc}{iteritems}{}
\end{funcdesc}
\begin{funcdesc}{itervalues}{}
\end{funcdesc}

By contrast, if you call the table's
\method{items}() or \method{values}() method, it will load data for the entire table into
memory, since these methods actually require creating a list object
containing every object in the database.
These methods ensure very efficient data transfer from the database server
(using the \method{fetchall}() method), but can consume large amounts of
memory limited only by the size of your database!
\begin{funcdesc}{items}{}
  return a list of all (id,obj) pairs representing all data in the table,
  after first loading the entire table into memory.
\end{funcdesc}
\begin{funcdesc}{values}{}
  return a list of all obj representing each row in the table,
  after first loading the entire table into memory.
\end{funcdesc}

Finally, if you iterate over IDs using \method{__iter__}() or \method{keys}()
(i.e. \code{for id in mytable}), data is not pre-loaded into memory;
each object will be fetched individually when you try to access it
(e.g. \code{obj=mytable[id]}).  

\begin{funcdesc}{__iter__}{}
  Iterate over all IDs (primary key values) in the table,
  without loading the entire table into memory.
\end{funcdesc}

Accessing individual objects by \var{id} also obeys the \var{maxCache}
caching limits:
\begin{funcdesc}{__getitem__}{id}
  get the object whose primary key is \var{id}, and cache it in
  our local dictionary (so that subsequent requests will return the
  same Python object, immediately, with no need to re-run an SQL query).
  For non-caching versions of \class{SQLTable}, see below.
\end{funcdesc}

You can also force loading of the entire database directly:
\begin{funcdesc}{load}{oclass=None}
  Load all data from the table, using \var{oclass} as the row object
  class if specified (otherwise use the oclass for this table).
  All rows are loaded from the database and saved as row objects
  in the Python dictionary of this class.
\end{funcdesc}

\begin{funcdesc}{objclass}{itemClass}
  Specify a object class to use for creating new ``row'' objects.
  \var{itemClass} must accept a single argument, a tuple object representing
  a row in the database.  

  Otherwise, the default \var{oclass} for SQLTable is
  the \class{TupleO} class, which provides a named attribute interface
  to the tuple values representing the row.
\end{funcdesc}

\begin{funcdesc}{select}{whereClause,params=None,oclass=None,selectCols='t1.*'}
  Generate the list of objects that satisfy the \var{whereClause}
  via a SQL SELECT query.  This function is a generator, so you
  use it as an iterator.  \var{params} is passed to the
  cursor execute statement to allow additional control over
  the query.  \var{selectCols} allows you to control what subset of
  columns should actually be retrieved.
\end{funcdesc}

\begin{funcdesc}{_attrSQL}{attr}
  Get a string expression for accessing attribute \var{attr} in SQL.
  This might either simply be an alias to the corresponding column
  name in the SQL table, or possibly an SQL expression that computes
  the desired value, executed on the database server.
\end{funcdesc}



There are several variants of this class:
\subsubsection{SQLTableClustered}
A subclass of \class{SQLTable} that groups its retrieval
of data from the table (into its local dictionary, where it
is cached), into ``clusters'' of rows that share the same value of
a column specified by the \var{clusterKey} argument to the \class{SQLTableBase}
constructor.  For data that naturally subdivide into large clusters,
this can speed up performance considerably.  If the clustering
closely mirrors how users are likely to access the data, this
performance gain will have relatively little cost in terms
of memory wasted on loading rows that the user will not need.

Also provides a few convenience methods:
\begin{funcdesc}{clusterkeys}{}
  Return list of all cluster IDs (distinct values in the \var{clusterKey}
  field of the database).
\end{funcdesc}
\begin{funcdesc}{itercluster}{cluster_id}
  Return list of all objects in the database that have a \var{clusterKey}
  value equal to \var{cluster_id}.
\end{funcdesc}


\subsubsection{SQLTableNoCache}
Provide on-the-fly access to rows in the database, 
but never cache results.  Use this when memory constraints or other 
considerations (for example, if the data in the database may change
during program execution, and you want to make sure your program
is always working with the latest version of the data) 
make it undesirable to cache recently used row objects, as the
standard \class{SQLTable} does.  Instead it returns (by default)
\class{SQLRow} objects that simply provide an interface
to obtain desired data attributes via database SQL queries.
Of course this reduces performance; every attribute access
requires an SQL query.  You can customize the class used for
providing this interface by specifying a different \var{itemClass}
to the constructor.

\subsubsection{SQLTableMultiNoCache}
Drops the assumption of a one-to-one
mapping between each key and a row object (i.e. removes the
assertion that the key is unique, a ``primary key''), allowing
multiple row objects to be returned for a given key.  Therefore,
the standard \method{__getitem__} must act as a generator, returning
an iterator for one or more row object.  You must set a 
\member{_distinct_key} attribute to inform it of which 
column to use as the key for searching the database;
this defaults to ``id''.

\subsubsection{SQLGraph}
Provides a graph interface to data stored in a table
in a relational database.  It follows the standard pygr 
graph interface, i.e. it behaves like a dictionary whose
keys are {\em source nodes}, and whose associated
values are dictionaries whose keys are {\em target nodes},
and whose associated values are {\em edges} between
a pair of nodes.  This class is a subclass of 
\class{SQLTableMultiNoCache}.  By default, it assumes that
the column names for source, target and edge IDs are simply
``source_id'', ``target_id'', and ``edge_id'' respectively.
To use different column names, simply provide an \var{attrAlias}
dictionary to the constructor, e.g.
\begin{verbatim}
g = SQLGraph('YOURDB.YOURTABLE',attrAlias=dict(source_id='left_exon_form_id',
                                               target_id='right_exon_form_id',
                                               edge_id='splice_id'))
\end{verbatim}
For good performance, the columns storing the source_id, target_id,
and edge_id should each be indexed.

\begin{funcdesc}{__init__}{name,cursor=None,itemClass=None,attrAlias=None,sourceDB=None,targetDB=None,edgeDB=None,simpleKeys=False,unpack_edge=None,**kwargs}
  \var{name} provides the name of the database table to use.

  \var{cursor}, if provided, should be a Python DB API 2.0 compliant cursor
  for connecting to the database.  If not provided, the constructor will attempt
  to connect automatically to the database using the MySQLdb module and
  your .my.cnf configuration file.

  \var{attrAlias}, if provided, must be a dictionary that maps desired
  attribute names to actual column names in the SQL database.

  \var{simpleKeys}, if True, indicates that the nodes and edge objects saved to
  the graph by the user should themselves be used as the internal representation
  to store in the SQL database table.  This usually makes sense only for strings
  and integers, which can be directly stored as columns in a relational database,
  whereas complex Python objects generally cannot be.  To use complex Python objects
  as nodes / edges for a SQLGraph, use the \var{sourceDB,targetDB,edgeDB} options below.

  \var{sourceDB}, if provided, must be a database container (dictionary interface) whose
  keys are source node IDs, and whose values are the associated node objects.
  If no \var{sourceDB} is provided, that implies \var{simpleKeys}=True.

  \var{targetDB}, if provided, must be a database container (dictionary interface) whose
  keys are target node IDs, and whose values are the associated node objects.

  \var{edgeDB}, if provided, must be a database container (dictionary interface) whose
  keys are edge IDs, and whose values are the associated edge objects.

  \var{unpack_edge}, if not None, must be a callable function that takes a ``packed''
  edge value and returns the corresponding edge object.
\end{funcdesc}

\begin{funcdesc}{__iadd__}{node}
  Add \var{node} to the graph, with no edges.  \var{node} must be 
  an item of \var{sourceDB}, if that option was provided.
\end{funcdesc}

\begin{funcdesc}{__delitem__}{node}
  Delete \var{node} from the graph, and its edges.  \var{node} must be a
  source node in the graph.  \method{__isub__} does exactly the same thing.
\end{funcdesc}

\begin{funcdesc}{__contains__}{id}
  Test whether \var{id} exists as a source node in this graph.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  Return an \class{SQLGraph} instance representing the reverse 
  directed graph (i.e. swap target nodes for source nodes).
\end{funcdesc}

\subsubsection{SQLGraphClustered}
Provides a read-only graph interface with improved performance based on
using \class{SQLTableClustered} as the interface to the database
table.  This has several implications: 1. the table should have
a primary key; 2. the table should have a \var{clusterKey}
column that provides the value for clustering rows in the table.
This class can offer much better performance than \class{SQLGraph}
for several reasons: 1. it caches data so that subsequent requests
for the same node or edge will be immediate, with no need to query
the SQL database; 2. it employs clustering to group together 
data retrieval of many rows at a time sharing the same cluster key
value, instead of one by one; 3. it provides a \method{load}
method for loading the entire graph into cache (local dictionary);
4. use of the \method{items} method and other ``value iterator'' methods
will automatically perform a load of the entire graph, so that 
only a single database query is used for the entire dataset, 
rather than a separate query for each row or cluster.

As for \class{SQLTable}, getting a list of node IDs using
\method{__iter__} or \method{keys} does not force an automatic load of 
the entire table into memory, but calling \method{items} or
other ``value'' list / iterator methods will.

\begin{funcdesc}{__init__}{table,source_id='source_id',target_id='target_id',edge_id='edge_id',clusterKey=None,sourceDB=None,targetDB=None,edgeDB=None,simpleKeys=False,unpack_edge=None,**kwargs}
  Similar to the \class{SQLTableBase}, but not exactly the same format.
  \var{table} can either be a string table name, or an actual 
  \class{SQLTableClustered} object.  You must provide a \var{clusterKey}
  value.  The \var{sourceDB,targetDB,edgeDB,simpleKeys,unpack_edges} optional
  arguments have the same meanings as for \class{SQLGraph} (see above).
\end{funcdesc}

\begin{funcdesc}{load}{l=None}
  Load all data from the table, and store in our local cache (a
  Python dictionary).  If \var{l} is not None, it provides a
  list of tuples obtained via the \method{select} method that
  should be added to the cache, instead of loading the entire
  database table.
\end{funcdesc}

\begin{funcdesc}{__contains__}{id}
  Test whether \var{id} exists as a source node in this graph.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  Return an \class{SQLGraphClustered} instance representing the reverse 
  directed graph (i.e. swap target nodes for source nodes).
\end{funcdesc}

\subsubsection{TupleO}
Default class for ``row objects'' returned by \class{SQLTable}.
Provide attribute interface to a tuple.  To subclass this,
add an \member{_attrcol} attribute
that maps attribute names to tuple index values (integers).
Constructor takes a single tuple argument representing a
row in the database.

\subsubsection{SQLRow}
Default class for row objects from NoCache variants of SQLTable.
Provides transparent interface to a row in the database: attribute access
will be mapped to SELECT of the appropriate column, but data is not cached
on this object.  Constructor takes two arguments: a database table
object, and an identifier for this row.  Actual data requests will
be relayed by \class{SQLRow} to the database table object.


\subsection{mapping module: graphs and graph query}
\label{graphs-query}

\subsubsection{The Pygr graph model}
The basic idea of Pygr is that all Python data can be viewed as a graph whose nodes are objects and whose edges are object relations (in Python, references from one object to another).  This has a number of advantages. 

   1. All data in a Python program become a database  that can be queried through simple but general graph query tools.  In many cases the need to write new code for some task can be replaced by a database query. 

   2. Graph databases are more general and flexible in terms of what they can represent and query than relational databases, which is very important for complex bioinformatics data.

   3. Indeed, in Pygr, a query is itself just a graph that can be stored and queried in a database, opening paths to automated query construction.

   4. Pygr graphs are fully indexed, making queries about edge relationships (which are often unacceptably slow in relational databases) fast.

   5. The interface can be very simple and pythonic: it's just a Mapping.  In Python "everything is a dictionary", also known as "the Mapping protocol": a dictionary maps some set of inputs to some set of outputs. e.g. m[a]=b maps a onto b, as a unique relation.  In Pygr, if we want to be able to map a node to multiple target nodes (i.e. allow it to have multiple edges), we simply add another layer of mapping: m[a][b]=edgeInfo (where edgeInfo is optional edge info.)

Examples of the Pygr syntax:

\begin{verbatim}
graph += node1 # ADD node1 TO graph
graph[node1] += node2 # ADD AN EDGE FROM node1 TO node2
graph[node1][node2] = edge_info # ADD AN EDGE WITH ASSOCIATED edge_info
# ADD SCHEMA BINDING WITH graph[node] BOUND AS node.attr
setschema(node,attr,graph) 
# SEARCH graph FOR SUBGRAPH {1->2; 1->3; 2->3}, 
# I.E. EXONSKIP, WHERE THE SPLICE FROM 2 -> 3 HAS ATTRIBUTE type 'U11/U12' 
for m in GraphQuery(graph,{1:{2:None,3:None},\
                   2:{3:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},\
                   3:{}}):
    print m[1].id,m[2].id,m[1,2].id
\end{verbatim}

Let's examine these examples one by one:
\begin{itemize}

\item
adding a node to a graph is distinct from creating edges between it and other nodes.  The graph+=node notation simply adds node to the graph, initially with no edges to other nodes.

\item 
A similar syntax (graph[node1]+=node2) can be used to add an edge between two nodes, but with no edge information.  In this case the edge information stored for this relation is simply the Python None value.  Note that in Pygr the default type of graph has directed edges; that is a->b does not imply b->a.  In the default dictGraph graph class, these are two distinct edges that would have to be added separately if you truly want to have an edge going both from a to b and from b to a.

\item 
To add an edge between two nodes with edge information, use the graph[node1][node2]=edge_info syntax. 

\item 
You can bind an object attribute to a graph, using setschema(obj,attr,graph).  This acts like Python's built-in setattr(obj,attr,value), but instead of obj.attr simply storing the specified value, it is bound to the graph so that obj.attr is equivalent to graph[obj].  Both syntaxes are interchangeable and can be mixed in different pieces of code accessing the same object.

\item 
Since Pygr adopts the Mapping protocol as its model for storing graphs, you can create graphs simply by creating Python dict objects e.g. {foo:bar}.  In this example we construct a query graph whose "nodes" are just the integers 1, 2, and 3.  Since any kind of object is a valid key in Python mappings, they can therefore also be used as "nodes" in a Pygr graph.  This query graph illustrates a few simple principles:

\item 
a Pygr graph is just a two-level Python mapping.  For example, {1:{2,None}} is a graph with a single edge from 1 to 2, with no edge information.  Pygr graphs can have multiple edges from or to a given node. 

\item    
edge information in a query graph can be used to specify extra query arguments, again in the form of a Python dictionary.  This dictionary is interpreted as a set of "named arguments" to be used by the GraphQuery search method.  For example, a filter argument is interpreted as a callable function that is passed a set of named arguments describing the current edge / node matching being tested, and whose return value (True or False) will determine whether this edge "matches" our query graph.  In this example, we used it to check whether the edge.type attribute is "U11/U12" (an unusual type of splicing in gene structure graphs).

\item         
Graph query in Pygr simply means finding a subgraph of the datagraph that has node-to-node match to the edge structure given in the query graph.  In this example it is a simple exon-skip structure (3 exons, one of which can either be included or skipped).  The GraphQuery class provides a general mechanism for performing graph queries on any Python data (see below for full details).  It can be used as an iterator that will return all matches to the query (if any). 

\item          
Matches are themselves returned as a mapping of nodes and edges of the query graph (in this example, its nodes are the integers 1, 2 and 3) onto nodes and edges of the data graph.  In this example the match is returned as m, so m[1] is the node in the data graph corresponding to node 1 in the query graph.  This example assumes that object has an id attribute, which is printed out.  To refer to an edge, just use a tuple corresponding to a pair of nodes in the query graph.  In this example, 1,2 refers to the edge from node 1 to node 2 in the query graph, so m[1,2] is the edge in data graph between nodes m[1] and m[2].  This example also attempts to print an id attribute from that edge object.

\item         
Note on current behavior: currently, GraphQuery will throw a KeyError exception if it tries to search for a query node in the query graph and does not find it.  That's why we have to add the "node with no edges" entry 3:{} for node 3.  This will probably be addressed in the future, since this seems like a potential source of annoying unexpected behaviors.

\item
\code{for node in graph}: iterator method returns all nodes in the graph; you could also use graph.items() to get node,dictEdge pairs, etc.

\item
\code{for node in graph[node]}:  iterator method returns all nodes that are targets of edges originating at node.  Again, you could use graph[node].items() to get node,edgeInfo pairs.  Note: if node is not in graph, this will throw a KeyError exception just like any regular Python dict.

\item
\code{if node in graph}:  \method{__contains__} method checks whether node is present in the graph, using dict indexing.

\item
\code{if node2 in graph[node1]}:  test whether node1 has an edge to node2.  Again, if node1 isn't in graph, this will throw a KeyError exception.

\end{itemize}

\subsubsection{Directionality and Reverse Traversal}

Note that dictGraph stores directed edges, that is, a->b does not imply b->a; those are two distinct edges that would have to be added separately if you want an edge going both directions.  Moreover, the current implementation of dictGraph does not provide a mechanism for traveling an edge backwards.  To do so with algorithmic efficiency requires storing each edge twice: once in a forward index and once in a reverse index.  Since that doubles the memory requirements for storing a graph, the default dictGraph class does not do this.  If you want such a "forward-backwards" graph, use the dictGraphFB subclass that stores both forwad and reverse indexes, and supports the inverse operator ($\sim$).  $\sim$ graph gets the reverse mapping, e.g. ($\sim$ graph)[node2] corresponds to the set of nodes that have edges to node2.  This area of the code hasn't been tested much yet.

\subsubsection{Graph}
This class provides a graph interface that can work with external storage
typically, a BerkeleyDB file, based on storing node ID and
edgeID values in the external storage instead of the python objects themselves.
\begin{funcdesc}{__init__}{saveDict=None,dictClass=dict,writeNow=False,filename=None,sourceDB=None,targetDB=None,edgeDB=None,intKeys=False,simpleKeys=False,unpack_edge=None,**kwargs}
  \var{filename}, if provided, gives a path to a BerkeleyDB file to use as the 
  storage for the graph.  If the file does not exist, it will be created automatically.
  If the \var{intKeys}=True option is provided, this will be an \class{IntShelve},
  which allows the use of \class{int} values as keys.  Otherwise a regular Python 
  \class{shelve} will be used (via the \class{PicklableShelve} class),
  which only allows string keys.  Note that in this case you {\em must} 
  call the Graph's \method{close}() method when you are done adding nodes / edges,
  to ensure that all the data is written to disk (unless you are using the
  \var{writeNow}=True option, see below).

  The \var{writeNow}=True option makes all
  writing operations atomic; i.e. the shelve file is opened read-only, and
  any attempt to write a single edge will re-open in write mode, save the data,
  and immediately close it, then re-open it in read-only mode.  This minimizes
  the probability that multiple processes simultaneously accessing the graph
  database will over-write each others' data.  Note: if you leave this option False,
  and write data to the graph, you {\em must} call the \method{close}() method
  once you have finished writing data to the graph, as described below.

  \var{saveDict}, if provided, must be a graph-style interface that stores the graph
  purely in terms of node ID and edge ID values.  This could be an \class{IntShelve},
  \class{PicklableShelve} or dict instance, for example.  If None provided,
  the constructor will create storage for you using the \var{dictClass} class, passing
  on \var{kwargs} to its constructor.

  \var{simpleKeys}, if True, indicates that the nodes and edge objects saved to
  the graph by the user should themselves be used as the internal representation
  to store in the graph database file.  This usually makes sense only for strings
  and integers, which can be directly stored as keys in a BerkeleyDB (Python shelve),
  whereas complex Python objects generally cannot be.  To use complex Python objects
  as nodes / edges for a Graph, use the \var{sourceDB,targetDB,edgeDB} options below.

  \var{sourceDB}, if provided, must be a database container (dictionary interface) whose
  keys are source node IDs, and whose values are the associated node objects.
  If no \var{sourceDB} is provided, that implies \var{simpleKey}=True.

  \var{targetDB}, if provided, must be a database container (dictionary interface) whose
  keys are target node IDs, and whose values are the associated node objects.

  \var{edgeDB}, if provided, must be a database container (dictionary interface) whose
  keys are edge IDs, and whose values are the associated edge objects.
\end{funcdesc}

\begin{funcdesc}{__iadd__}{node}
  Add \var{node} to the graph, with no edges.  \var{node} must be an
  item of \var{sourceDB}.
\end{funcdesc}

\begin{funcdesc}{__delitem__}{node}
  Delete \var{node} from the graph, and its edges.  \var{node} must be a
  source node in the graph.  \method{__isub__} does exactly the same thing.
\end{funcdesc}

\begin{funcdesc}{close}{}
  If you chose to use a Python \module{shelve} as the actual storage, you used
  the default setting of \var{writeNow}\code{=False}, and you
  wrote data to the graph, then you {\em must} call the \class{Graph} object's
  \method{close}() method to finalize writing to the disk of any data that may
  be pending, once you have finished writing data to the graph.  Failure to do
  so may leave the shelve index file in an incomplete and corrupted state.
\end{funcdesc}

%\begin{funcdesc}{__invert__}{}
%  Returns an \class{IDGraph} interface to the inverse mapping
%  (i.e. where source and target nodes are swapped).
%\end{funcdesc}

The object's \member{edges} attribute provides an interface to iterating
over or querying its edge dictionary.

\subsubsection{dictGraph}

\class{dictGraph} is Pygr's in-memory graph class.  For persistent
graph storage and query (e.g. stored in a relational database table
or BerkeleyDB file), see the \class{Graph} class above.

This class provides all the standard behaviors described above.  The current reference implementation uses standard Python dict objects to store the graph.  All the usual Mapping protocol methods can be used on dictGraph objects (top-level interface, in the examples above graph) and dictEdge objects (second-level interface; in the examples above graph[node]).

\subsubsection{Collection}
Provides a generic holder for a collection of data objects, that can be
pickled, stored in \module{pygr.Data} etc.  It is a generic dictionary-like interface
for flexible storage mapping ID --> OBJECT, and depending on its 
initial arguments will store the data in memory (e.g. using a Python \class{dict})
or on-disk (e.g. using \class{PicklableShelve} or \class{IntShelve}, see details below).

\begin{funcdesc}{Collection}{saveDict=None,dictClass=dict,filename=None,intKeys=False,**kwargs}
  \var{saveDict}, if not None, is the internal mapping to use as our storage.
        
  \var{filename}: if provided, is a file path to a shelve (BerkeleyDB) file to
  store the data in, using \class{PicklableShelve}, or \class{IntShelve} if \var{intKeys}
  is True.  In this case \var{kwargs} are passed to the constructor for that class,
  allowing you to specify \var{mode}, \var{writeback} and other \module{shelve} parameters.

  \var{dictClass}: if provided, is the class to use for storage of the dict data.
\end{funcdesc}

\begin{funcdesc}{close}{}
  After saving data into a \class{Collection} you must ``commit'' the transaction
  by calling its \method{close}() method, which will ensure that all pending data
  will be written to associated files (e.g. for \class{PicklableShelve} or \class{IntShelve}).
\end{funcdesc}


\subsubsection{PicklableShelve}
Subclass of \class{Collection} that 
provides an interface to the Python \module{shelve} persistent dictionary
storage, as an object that can be pickled; unpickling the object will 
correctly re-open the associated \module{shelve} file.  One important
difference is that it allows you to specify both the mode flag for opening
the shelve {\em now} and the mode flag for re-opening the shelve in the
future whenever this object is unpickled.  

Note also that since \class{PicklableShelve} is designed to be pickled
and potentially shared among users, it automatically supports re-opening in
read-only mode.  That is, if re-opening in read/write mode fails, it will 
automatically re-open in read-only mode, and prints a warning message to the
user.  This feature avoids permissions problems that commonly occur, e.g.
if one user builds a PicklableShelve, and shares that to other users, they 
typically will not have write-permission to the file, and could only access
it in read-only mode.
\begin{funcdesc}{PicklableShelve}{filename,mode=None,writeback=False,unpicklingMode=False,verbose=True,**kwargs}
  Ideally, you
  should specify a TWO letter mode string: the first letter to
  indicate what mode the shelve should be initially opened in, and
  the second to indicate the mode to open the shelve during unpickling.
  e.g. \var{mode}='nr': to create an empty shelve (writable),
  which in future will be re-opened read-only.

  Single letter \var{mode} values such as 'n' (create empty file), 'c'
  (open read-write, but create if missing), and 'w' (open read-write)
  are permitted, but will default to read-only for re-opening the file
  in {\em future} unpickling operations.  Use a two-letter \var{mode}
  if you want the file re-opened in read-write mode; e.g. \var{mode}='nw'
  to create an empty file now and re-open it in read-write mode in future
  unpickling operations.

  \var{mode=None} makes it first attempt to open read-only, but if the file
  does not exist will create it using mode 'c'.  Note that it will also 
  follow this behavior pattern in future unpickling operations (i.e. if
  the file is missing, it will be silently re-created, empty, in read-write mode).
  This is appropriate if you want to be able to ``empty the database'' by
  simply deleting the shelve file manually.  This behavior is different from
  the 'nr' mode, which will create the shelve file empty {\em now}, but 
  will raise an exception if it is missing when future unpickling operations
  attempt to re-open it read-only.
\end{funcdesc}

\begin{funcdesc}{reopen}{mode='r'}
  Re-open the shelve file in the specified \var{mode} and also save this
  \var{mode} as the mode for re-opening the shelve file in future unpickling
  operations.
\end{funcdesc}

\begin{funcdesc}{close}{}
  After saving data into a \class{PicklableShelve} you must ``commit'' the transaction
  by calling its \method{close}() method, which will ensure that all pending data
  will be written to its shelve file.
\end{funcdesc}

\subsubsection{IntShelve}
Subclass of PicklableShelve,
provides an interface to the Python \module{shelve} persistent dictionary
storage, that can accept \class{int} values as keys.
\begin{funcdesc}{IntShelve}{filename,mode=None,writeback=False,unpicklingMode=False,verbose=True,**kwargs}
  Open the specified \module{shelve} BerkeleyDB file, using the specified
  mode.
\end{funcdesc}

\begin{funcdesc}{close}{}
  After saving data into a \class{IntShelve} you must ``commit'' the transaction
  by calling its \method{close}() method, which will ensure that all pending data
  will be written to its shelve file.
\end{funcdesc}
In other respects the \class{IntShelve} behaves like a regular shelve
(dictionary interface).




%\subsubsection{Schema: binding object attributes to graphs}

%The goal of Pygr is to provide a single consistent model for working with data explicitly modeled as graphs (i.e. dictGraph-like objects) and standard Python objects that were not originally designed to be queried (or thought of) as a "graph".  Since Python uses the Mapping concept throughout the language and object model, and provides introspection, there is no reason why Pygr can't work with both kinds of data transparently.  One mechanism for making this idea explicit is the idea of binding an object attribute to a graph, via the new method we've called setschema(obj,attr,graph).  The idea here is that once you bind an object attribute to a graph, the two different data models obj.attr (object model) or graph[obj] (graph model) are made equivalent and interchangeable.  Operating on one affects the other and vice versa; they are two ways of referring to the same relation.  This concept can be applied at several different levels

%\begin{itemize}
%\item
%individual objects: just like getattr() and setattr(), you can apply schema methods to individual objects: getschema(obj,attr) (returns the bound graph) or setschema(obj,attr,graph) (binds the object attribute to the graph). 

%\item
%all instances of a class: you can bind specific attributes of a given class to a graph using the following class attribute syntax:

%\end{itemize}
%\begin{verbatim}
%class ExonForm(object): # ADD ATTRIBUTES STORING SCHEMA INFO
%    __class_schema__=SchemaDict(((spliceGraph,'next'),(alt5Graph,'alt5'),(alt3Graph,'alt3')))
%\end{verbatim}
%
%In this class we bound the next attribute to spliceGraph, alt5 attribute to alt5Graph, and alt3 attribute to alt3Graph.  That means, every instance obj of this class will have an attribute obj.next that is equivalent to spliceGraph[obj], etc.  Note that this is schema, not the actual operation of adding the object as a node to the graph.  Indeed, when obj is first created, it is not automatically added to spliceGraph; that is up to the user.  Unless your code has added the node to the graph (e.g. spliceGraph+=obj), obj.next should throw a KeyError exception.
%
%The general method getschema(obj,attr) works regardless of whether the schema was stored on an individual object or at the class level.

\subsubsection{GraphQuery}

The GraphQuery class implements simple node-to-node matching, in which each new node-set is generated by an iterator associated with a specific node in the query graph.  This iterator model is general: since indexes (mappings) support the iterator protocol, a given iterator may actually be an index lookup (or other clever search algorithm).  The GraphQuery constructor takes two arguments: the default data graph being queried, and the query graph.  The query graph is just a graph; its nodes can be any object that can be a graph node (i.e. any object that is indexible, e.g. by adding a __hash__() method).  Its node objects will not be modified in any way by the GraphQuery.  Its edges are expected to be dictionaries that can be checked for specific keyword arguments:

\begin{itemize}

\item
filter: must be a callable function that accepts keyword arguments and returns True (accept this edge as a match to the queryGraph) or False (do not accept this edge as a match).  This function will be called with the following keyword arguments:
       \begin{itemize}
          \item
          toNode: the target node of this edge, in the data graph
          \item
          fromNode: the origin node of this edge, in the data graph
          \item
           edge: the edge information for this edge in the data graph
          \item 
           queryMatch: a mapping of the query graph to the data graph, based on the partial matchings made so far
           \item
           gqi: the GraphQueryIterator instance associated with this matching operation.  Much more data is available from specific attributes of this object.
	\end{itemize}

\item 
dataGraph: graph in which the current edge should be search for.  This allows a query to traverse multiple graphs.  In other words, when searching for edges from the current node, look up dataGraph[node] instead of defaultGraph[node].

\item
attr: object attribute name to use as the iterator, instead of the defaultGraph.In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a mapping; specifically, it must provide an items() method that returns zero or more pairs of targetNode,edgeInfo, just like a standard Pygr dictEdge object. 

\item
attrN: object attribute name to use as the iterator, instead of the defaultGraph. In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a sequence; specifically, it must provide an iterator that returns zero or more targetNode.  The edgeInfo for any edges generated this way will be None.

\item
f: a callable function that must return an iterator producing zero or more pairs of targetNode,edgeInfo.  Typically f is a Python generator function containing a statement like yield targetNode,edgeInfo.

\item
fN: a callable function that must return an iterator producing zero or more targetNode.  Typically fN is a Python generator function containing a statement like yield targetNode.  The edgeInfo for any edges generated this way will be None.

\item 
subqueries: a tuple of query graphs to be performed.  Since GraphQuery traversalcorresponds to logical AND (i.e. all the query graph nodes must be successfully matched to return a match), the subqueries are currently treated as a union (logical OR), by simply returning every match from each subquery as a match (at least for this node).  Each subquery is itself just another query graph.  Moreover, since query graphs can share nodes (i.e. the same object can appear as a node in multiple query graphs), subqueries can make reference to nodes that are already matched by the higher query.  This is an area that has not been explored much yet, but provides a pretty general model for powerful queries.

\end{itemize}
The attr - subqueries options are all implemented as extremely simple subclasses of GraphQuery.  If you want to see just how easy it is to write new subclasses of GraphQuery functionality, look at the graphquery.py module (the entire graph query module is only 237 lines long).

Note: an easy way to pass keyword dictionaries (e.g. as edge information) is simply using the dict() constructor, e.g. dict(dataGraph=myGraph,filter=my_filter).  I think this is a little more readable than {'dataGraph':myGraph, 'filter':my_filter}.

Note on current behavior: currently, the GraphQuery iterator returns the same mapping object for each iteration (simply changing its contents).  So to save these multiple values safely in a list comprehension we have to copy each one into a new dict object via dict(m).

\subsubsection{What is GraphQuery actually doing?}

A GraphQuery is basically an iterator that returns all possible mappings of the query graph onto the datagraph that match all of the nodes and edges of the query graph onto nodes and edges of the data graph.  As an iterator, it does not instantiate a list of the matches, but simply returns the matches one by one.  The current design is very simple.  The GraphQuery constructor builds an "iterator stack" of GraphQueryIterators, each representing one node in the query graph; they are enumerated in order by a breadth-first-search of the query graph.  The GraphQuery iterator processes the stack of GraphQueryIterators: any match simply pushes the stack to the next level; any match at the deepest level of the stack is a complete match (yield the queryMatch mapping); the end of any GraphQueryIterator simply pops the stack.  One obvious idea for improving all this is to replace this "interpreter" with a "compiler" that compiles Python for loops that are equivalent to this stack, and run that... likely to be many fold faster.




\subsection{classutil Module}
\label{classutil-module}
This module provides basic support functions for pickling and unpickling,
etc.

\subsubsection{open_shelve}
Alternative to Python standard library function \class{shelve.open} with several benefits:
\begin{funcdesc}{open_shelve}{filename,mode=None,writeback=False,allowReadOnly=False,useHash=False,verbose=True}
\end{funcdesc}
\begin{itemize}
\item uses bsddb btree by default instead of bsddb hash, which is very slow
for large databases.  Will automatically fall back to using bsddb hash
for existing hash-based shelve files.  Set \var{useHash=True} to force it to use bsddb hash.
In our experience, the Python standard library \class{shelve} object using
bsddb hash file by default, becomes very slow and produces unreasonably large
files, when the number of records exceeds several million.  Fortunately, the
bsddb btree file seems to solve this problem, so \method{open_shelve}() uses
it by default.
      
\item \var{allowReadOnly=True} will automatically suppress permissions errors so
user can at least get read-only access to the desired shelve, if no write permission.

\item \var{mode=None} first attempts to open file in read-only mode, but if the file
does not exist, opens it in create mode.

\item raises standard exceptions defined in dbfile: \class{WrongFormatError}, 
\class{PermissionsError}, \class{ReadOnlyError}, \class{NoSuchFileError}
\end{itemize}

\subsection{coordinator Module}
\label{coord-module}

{\em Framework for running subtasks distributed over many computers, in a pythonic way, using SSH for secure process invocation and XMLRPC for message passing. Also provides simple interface for queuing and managing any number of such "batch jobs".}

I will first describe some classes for simple XMLRPC services, then proceed
to the job control classes.

\subsubsection{XMLRPCServerBase}
Base class for creating an XMLRPC server to serve data from multiple objects.
On the server-side, this object can be treated as a dictionary whose
keys are object names, and whose associated values are the server
objects that will serve functionality to XMLRPC clients.
It provides an XMLRPC method \method{methodCall} that takes an object name,
method name, and arguments, and if the call is permitted by its security
rules, calls the designated method on that object.
\begin{funcdesc}{__init__}{name,host=None,port=5000,logRequests=False}
  \var{name} is an arbitrary string identifier for the XMLRPC server.

  \var{host} allows you to override the default hostname to use for this
  server (which defaults to the fully-qualified domain name of this computer).
  Setting it to 'localhost' will typically make the XMLRPC server only accessible
  to processes running on this computer.

  \var{port} specifies the port number on which this server should run.

  \var{logRequests} is passed on to \class{SimpleXMLRPCServer} as
  a flag determining whether it outputs verbose log information.
\end{funcdesc}

\begin{funcdesc}{__setitem__}{name,obj}
  Save \var{obj} as the service called \var{name} in this XMLRPC server.
  \var{obj} must have an \member{xmlrpc_methods} dictionary whose
  keys are the names of its methods that XMLRPC clients are allowed
  to call.
\end{funcdesc}
\begin{funcdesc}{__delitem__}{name}
  Delete the service called \var{name} in this XMLRPC server.
\end{funcdesc}
\begin{funcdesc}{register}{url=None,name='index',server=None}
  Send information describing the services in this XMLRPC server,
  stored by the user on its \member{registrationData} attribute,
  to the resource database server, which can be specified in 
  several ways.  If \var{url} is not None, it will make an XMLRPC
  connection to the resource database server using \var{url} (as the
  URL for the XMLRPC server) and \var{name} (as the name of the server
  object that stores the resource database dictionary).  Otherwise,
  if \var{server} is not None, it is assumed to be a resource database
  object (or XMLRPC connection to such a database) providing a
  \method{registerServer} method that takes two arguments,
  a \var{locationKey} and the registration data.  Otherwise,
  it tries to connect to pygr.Data's default resource database
  by calling its \code{getResource.registerServer} method with the
  same arguments.
\end{funcdesc}
\begin{funcdesc}{serve_forever}{}
  Start the XMLRPC server, after detaching it from
  stdin, stdout and stderr; this call will never exit.
\end{funcdesc}

This XMLRPC server provides several interface methods to
XMLRPC clients contacting it:
\begin{funcdesc}{objectList}{}
  Returns a dictionary of its server objects, whose keys are their
  names, and whose values are in turn dictionaries whose keys are
  their allowed method names.
\end{funcdesc}
\begin{funcdesc}{objectInfo}{objname}
  Returns a dictionary whose keys are the allowed method names for
  the server object named \var{objname}.
\end{funcdesc}
\begin{funcdesc}{methodCall}{objname,methodname,args}
  Calls the designated method on the named server object, with the
  provided \var{args}, and returns its result to the XMLRPC client.
\end{funcdesc}

Example server objects that can be added to a \class{XMLRPCServerBase}
include \class{seqdb.BlastDBXMLRPC}, \class{xnestedlist.NLMSAServer}.


\subsubsection{XMLRPCClient, get_connection}
Client for accessing a \class{XMLRPCServerBase} server.  Provides 
a dictionary interface whose keys are names of available server objects,
and whose associated values are client objects that provide a transparent
interface to the server objects (i.e. calling a method on the client
object returns the value of the result of calling the same named method
on the server object).
\begin{funcdesc}{__init__}{url}
  Makes a connection to the XMLRPC server running on the specified \var{url},
  typically consisting of both a host name and port number.
\end{funcdesc}
\begin{funcdesc}{__getitem__}{name}
  Obtain a client object for the server object specified by \var{name}.
  It will be decorated with the set of methods on the server object
  that are allowed to be accessed by XMLRPC.
\end{funcdesc}

As a convenience, the \module{coordinator} module provides a function
\code{get_connection} that provides an efficient connection to XMLRPC
server objects.  Specifically, it caches past requests, so that multiple
requests for the same server object will re-use the same client object,
and requests for different server objects on the same XMLRPC server will
share the same \class{XMLRPCClient} connection.  It is simply used as follows:
\code{get_connection(url,name)}, where \code{url} is the URL of the XMLRPC
server, and \code{name} is the name of the server object you wish to access.
For example:
\begin{verbatim}
myclient = coordinator.get_connection('http://leelab.mbi.ucla.edu:5000','ucsc17')
\end{verbatim}

\subsubsection{coordinator Module Functionality Overview}

The \module{coordinator} module provides a simple system for running a large collection of tasks on a set of cluster nodes.  It assumes:

\begin{itemize}

\item
authentication is handled using ssh-agent.  The coordinator module does no authentication itself; it simply tries to spawn jobs to remote nodes using ssh, assuming that you have previously authenticated yourself to ssh-agent. 

\item
the client nodes can access your scripts using the same path as on the initiating system.  In other words, if you launch a coordinator job /home/bob/mydir/myscript.py, your client nodes must also be able to access /home/bob/mydir/myscript.py (e.g. via NFS).

\item
your job consists of a large set of task IDs, and a computation to be performed on each ID.  To run this job, you provide an iterator that generates the list of task IDs for the Coordinator to distribute to your client nodes.  You start your script to run a Coordinator that serves your list of task IDs to the client nodes.  You also provide  a function that performs your desired computation on each task ID it receives from the Coordinator.  Typically, you provide both the server function (i.e. the iterator that generates the list of task IDs) and the client function (that runs your desired computation for each ID) within a single Python script file.  Running this script without extra flags starts the Coordinator, which in turn launches your script as a Processor on one or more client nodes.  The Processors andCoordinator work together to complete all the task IDs.

\item
a ResourceController performs load balancing and resource allocation functions, including: dividing up loads from one or more Coordinators over a set of hosts (each with one or more CPUs); serving a Resource database to Processors requesting specific resources; resource-locking on a per node basis for preventing Processors from using a Resource that is under construction by another Processor.  For very large files that are used repeatedly by your computation, it is preferable to first copy them to local disk on each cluster node (fast), rather than reading them over and over again from NFS (slow).  Resources provide a simple mechanism for doing this.

\end{itemize}
To see how to use this, let's look at an example script, mapclusters5.py:

\begin{verbatim}

from pygr.apps.leelabdb import *
from pygr import coordinator

def map_clusters(server,genome_rsrc='hg17',dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome = BlastDB(ifile=server.open_resource(genome_rsrc,'r'))
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, \
     protein,clusterExons,clusterSplices) = getSpliceGraphFromDB(spliceCalcs[dbname])

    for cluster_id in server:
        g = genomic_seq[cluster_id]
        m = genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE test.mytable USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "serve up cluster_id one by one"
    cursor = getUserCursor(dbname)
    t = SQLTable(source_table,cursor)
    for id in t:
        yield id

if __name__=='__main__':
    coordinator.start_client_or_server(map_clusters,serve_clusters,['hg17'],__file__)
\end{verbatim}

Let's analyze the script line by line:

\begin{itemize}

\item
mapclusters() is a client generator function to be run in a Processor on a client node.  It takes one argument representing its connection to the server (a Processor object), and optional keyword arguments read from the command line.  It first does some initial setup (opens a BLAST database and loads a schema from a MySQL database), then iterates over task IDs returned to it from the server.  A few key points:

\item
server.open_resource(genome_rsrc,'r') requests a resource given by the genome_rsrc argument from the ResourceController, does whatever is necessary to copy this resource to local disk, and then opens it for reading, returning a file-like object.  This can then be used however you like, but you MUST call its close() method (just as you should always do for any file object) to indicate that you're done using it.  Failure to close() the file object will leave the Resource "hg17" permanently locked on this specific node.  (You would then have to unlock it by hand using the ResourceController.release_rule() method).

\item
yield cluster_id: the client function must be a Python generator function (i.e. it must use the yield statement), and it must yield the list of IDs that it has processed.  Python's generator construct is extremely convenient for many purposes: here it lets us perform both our initializations and iteration over IDs within a single function, while at the same time wrapping each iteration within the Processor's error trapping code (to prevent a single error in your code from causing the entire Processor to shut down).  The Processor will trap any errors in your code and and send tracebacks to your Coordinator, which will report them in its logfile.  The Processor will tolerate occasional errors and continue processing more IDs.  However, if more than a certain number of IDs in a row fail with errors (controlled by the Processor.max_errors_in_a_row attribute), the Processor will exit, on the assumption that either your code or this specific client node don't work correctly.

\item
serve_clusters() is the server generating function to be run in the Coordinator.  It returns an iterator that generates all the task IDs that we want to run.  Again, the Python generator construct provides a very clean way of doing this: we simply yield each ID that we want to process in our client Processors.

\item
if __name__=="__main__": this final clause automatically launches our script as either a Coordinator or Processor depending on the command line options (which are automatically parsed by start_client_or_server()).  All we have to do is pass the client generator function, the server generator function, a list of the resources this job will use, and the name of the script file to be run on client nodes.  Since that is just this script itself, we use the Python builtin symbol __file__ (which just evaluates to the name of the current script).

\item     
Command-line arguments are parsed (GNU-style, ie. --foo=bar) by start_client_or_server() and passed to your client and server functions as Python named parameters.  Because the same list of arguments is passed to your client and server functions, and each of these functions won't necessarily want to get all the named arguments, you should include the **kwargs at the end of the argument list.  Any unmatched arguments will be stored in kwargs as a Python mapping (dictionary).  If you fail to do this, your client or server function will crash if called with any named parameters other than the ones it expects.
\end{itemize}

\subsubsection{Log and Error Information}

Process logging and error information go to three different types of logs:

\begin{itemize}

\item
Processor logfile(s): every individual Processor (and all subprocesses run by it) send stdout and stderr to a logfile on local disk of the host on which it is running.  Currently the filename is /usr/tmp/NAME_N.log, where NAME is the name you assigned to the job when you started the Coordinator, and N is the numeric ID of the Processor assigned by the coordinator (just an auto-increment integer beginning at 0, and increasing by one for each Processor the Coordinator starts).  This logfile is the place to look if your job is failing mysteriously--look in the logfile and see its last words before its demise.  You can get a complete list of the logfiles for all the Coordinator's Processors by inspecting the logfile attribute of the CoordinatorMonitor (see below).

\item
Coordinator logfile: all XMLRPC requests from client Processors, as well as error messages from them, are logged here.  All Python errors (tracebacks) in your client (Processor) code are reported here.  Also, the actual SSH commands used to invoke your Processors on cluster nodes, are logged here.  This is usually the place to start, to see whether things are going well (you should see a long stream of next requests as Processors finish a task and request the next one), or failing with errors.

\item
ResourceController logfile: all XMLRPC requests from Processors and Coordinatorsare logged here, including register() and unregister(), resource requests, and load reporting from cluster nodes.  If things are working well, you should see a stream of regular report_load() messages showing steady, full utilization of all the host processors.  Excessive register/unregister churning (jobs that start and immediately exit) is a common sign of trouble with your jobs.

\end{itemize}
\subsubsection{Coordinator}

To start a job coordinator (which in turn will the run the whole job by starting Processors on cluster nodes using SSH):

\begin{verbatim}
python mapclusters5.py mm5_jan02 --errlog=/usr/tmp/leec/mm5_jan02.log \ 
  --dbname=MOUSE_SPLICE --source_table=genomic_cluster_jan02 \
  --genome_rsrc=mm5 --result_table=GENOME_ALIGNMENT.mm5_cluster_jan02_all \ 
  --rmOpts=-rodent \
\end{verbatim}

Here we have told the Coordinator to name itself "mm5_jan02" in all its communications with the ResourceController.  Since we gave no command-line flags, the Coordinator will assume that a ResourceController is already running on port 5000 of the current host.    You must have an ssh-agent running BEFORE you start the Coordinator, since the Coordinator will attempt to spawn jobs using SSH.  The Coordinator will exit with an error message if it is unable to connect to ssh-agent.  A few notes:

\begin{itemize}

\item
The Coordinator will run as a demon process (i.e. in the background, and detached from your terminal session), and redirect its  output into a file (here, given by the --errlog option). If you don't specify an --errlog filename, it will create a filename determined by the name we told it to run as, in this case "mm_jan02.log".  

\item
You must ensure that SSH can launch processes on your client nodes "unattended" i.e. without a connection to a controlling terminal.  If SSH has to ask for userconfirmations when connecting to a given host (e.g. if it asks whether you want to accept the host key), the Coordinator will not be able to use that host.

\item
Python errors (tracebacks) in your will be GNU-style command-line options (e.g. --port=8889) are automatically parsed by start_client_or_server() and passed to the Coordinator.__init__() as keyword arguments.  This constructor takes the following optional arguments: 
    \begin{itemize}
    \item
    port: the port number on which this Coordinator should run
    
    \item
    priority: a floating point number specifying the priority level at which this Coordinator should be run by the ResourceController.  The default value is 1.0.  A value of 2.0 will give it twice as many Processors as a competing Coordinator of priority 1.0.

    \item
    rc_url: the URL for the ResourceController.  Defaults to http://THISHOST:5000
    \item
    errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this Coordinator. Can be an absolute path.

    \item
    immediate: if True, make the job run immediately, without waiting for previous jobs to finish.  Default: False.
    
    \item
    demand_ncpu: if set to a non-zero value, specifies the exact number of Processors you want to run your job.

    \item
    NB: command line arguments are also passed to your server function, and to your client function, as Python named parameters.  See the mapclusters5.py example above.
    \end{itemize}
\end{itemize}

\subsubsection{ResourceController}

Whereas you start a separate Coordinator for each set of jobs you want to run, you only need a single ResourceController running. To start the ResourceController, run:

\begin{verbatim}
python coordinator.py --rc=bigcheese
\end{verbatim}

This starts the ResourceController (running as a demon process in the background) and names it "bigcheese"; a name argument (given by the --rc flag) is REQUIRED.  Since you didn't specify command-line flags, it will run on the default port 5000.  It will use several files based on the name you gave it:
\begin{itemize}
 
\item
bigcheese.hosts: a list of cluster nodes and associated maximum load (separated by whitespace, one pair per line).  It will attempt to fill these nodes with jobs, up to the maximum load level specified for each, sharing the load between whatever set of Coordinators contact it.

\item
bigcheese.log: all output from the ResourceController (showing requests made to it by Coordinators and Processors) is logged to this file.

\item
bigcheese.rules: this file is a Python shelve created by the ResourceController as its rules database.

\item
bigcheese.rsrc: this file is a Python shelve created by the ResourceController as its resource database.GNU-style command-line options (e.g. --port=5001) are automatically parsed by start_client_or_server() and passed to the ResourceController.__init__() as keyword arguments.  This constructor takes the following optional arguments:

\item
port: the port number on which this ResourceController should run

\item
overload_margin: how much "extra" load above the standard level is allowable.  This prevents temporary load spikes from causing Processors to exit.  Set by default to 0.6.  I.e. if the maxload for a host was set to 2.0, any load above 2.6 would cause the ResourceController to start shutting down Processor(s) on that host.

\item
rebalance_frequency: the time interval, in seconds, for rerunning the ResourceController.load_balance() method.  Defaults to 1200 sec.

\item
errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this ResourceController. Can be an absolute path.

\end{itemize}

\subsubsection{RCMonitor}

The coordinator module also provides a convenience interface for interrogating and controlling jobs.  In an interactive Python shell, import the coordinator module, and create an RCMonitor object::

\begin{verbatim}
from pygr import coordinator
m = coordinator.RCMonitor()
\end{verbatim}

Since you did not specify any arguments, it will default to searching for the ResourceController on the current host, port 5000.  You can specify a host and or port as additional arguments.  It also loads an index of coordinators currently registered with this ResourceController, accessible on its coordinators attribute:

\begin{verbatim}
for name,c in m.coordinators.items():
  print name,len(c.client_report)
\end{verbatim}

will print a list of the coordinators and how many Processors each is currently running.  Each coordinator is represented by a CoordinatorMonitor object in this coordinators index.

Both RCMonitor and CoordinatorMonitor objects give you access to the XMLRPC methods of the ResourceController and Coordinators they represent.  That is, running a method on the RCMonitor actually runs the identically-named method on the ResourceController.  Some of the most useful ResourceController methods are:

\begin{itemize}

\item
report_load(host,pid,load): inform RC that the current load on host is load.

\item
load_balance(): make the RC rebalance load, using all available nodes and coordinators

\item
setrule(rsrc,rule): set a production rule for the resource named rsrc.  rule must be a tuple consisting of the local filepath to be used for the resource, and a shell command that will construct it, with a %s where you want the filename to be filled in.

\item
delrule(rsrc): deletes the rule for rsrc from the rules database.

\item
set_hostinfo(host,attr,val) set an attribute for host.  For example, to set the maximum load for this host: rcm.set_hostinfo(host,'maxload',2.0).  This should usually be the number of CPUs on this host.  NB: these settings will apply only to the current ResourceController, and are not saved back to its NAME.hosts file.  If you want to make these settings permanent (i.e. to apply to ResourceControllers you start anew in the future), then edit the NAME.hosts file.

\item
retry_unused_hosts(): make the RC search its hosts database for hosts that are not currently in use (e.g. jobs may have died) and try to reallocate them to the existing coordinators.

\end{itemize}
Both RCMonitor and CoordinatorMonitor objects have a get_status() method that updates them with the latest information from their associated ResourceController or Coordinator.

Here are some typical monitor usages:

\begin{verbatim}
c = m.coordinators['mapclusters3'] # GET MY COORDINATOR
c.client_report.sort() # MAKE IT SORT CLIENTS BY HOSTNAME
c.client_report # PRINT THE SORTED LIST, SHOWING HOST, PID, #TASKS DONE
c.pending_report # PRINT LIST OF TASK IDS CURRENTLY RUNNING
c.nsuccess # PRINT TOTAL #TASKS DONE
c.nerrors  # PRINT TOTAL #TASKS FAILED
c.logfile # PRINT LIST OF ALL PROCESSOR LOGFILES

m.rules # PRINT THE CURRENT RULES DATABASE
m.resources # PRINT THE CURRENT RESOURCES DATABASE
m.setrule('hg17',
('/usr/tmp/ucsc_msa/hg17',
'gunzip -c /data/yxing/databases/ucsc_msa/human_assembly_HG17/*.fa.gz
>%s'))
m.get_status() # UPDATE OUR RC INFO
m.set_hostinfo('llc22','maxload',2.0) # ADD A NEW HOST TO OUR DATABASE
m.setload('llc1','maxload',0.0) # STOP USING llc1 FOR THE MOMENT
m.load_balance() # MAKE IT ALLOCATE ANY FREE CPUS NOW...
m.locks # SHOW LIST OF RESOURCES CURRENTLY LOCKED, UNDER CONSTRUCTION
\end{verbatim}

\subsubsection{Security}

Internal communication between Processors, Coordinators and ResourceController is performed using XMLRPC and thus is not secure. However, since no authentication information or actual commands are transmitted by XMLRPC, and the coordinator module does not enable the processes that use it to do anything that they are not ALREADY capable of doing on their own (i.e. spawn ssh processes), the main security vulnerabilty is Denial Of Service (i.e. an attacker listening to the XMLRPC traffic could send messages causing Processors to shutdown, or Coordinators to be blocked from running any Processors).  In other words the security philosophy of this module is to avoid compromising your security, by leaving the security of process invocation entirely to your existing security mechanisms (i.e. ssh and ssh-agent).  Commands are only sent using SSH, not XMLRPC, and the XMLRPC components are designed to prevent known ways that an XMLRPC caller might be able to run a command on an XMLRPC server or client. (I blocked known security vulnerabilities in Python's SimpleXMLRPCServer module).

In the same spirit, the current implementation does not seek to block users from issuing commands that could let them "hog" resources, for the simple reason that in an SSH-enabled environment, they would be able to do so regardless of this module's policy.  I.e. the user can simply not use this module, and spawn lots of processes directly using SSH.  In the current implementation, every user can send directives to the ResourceController that affect resource allocation to other users' jobs.  This means everybody has to "play nice", only giving their Coordinator(s) higher priority if it is really appropriate and agreed by other users.  Unless a different process invocation mechanism (other than SSH by each user) were adopted, it doesn't really make sense to me to try to enforce a policy that is stricter than the policy of the underlying process invocation mechanism (i.e. SSH).  Since every user can use SSH to spawn as many jobs as they want, without regard for sharing with others, making this module's policy "strict" doesn't really secure anything.


\section{Testing}
\label{testing-doc}

The following subsections provide details about how the testing of different
modules of Pygr functionality. 

\subsection{Running Tests}
First, you must have \code{nose} installed.  Then simply go to 
the testing directory and run the tests:
\begin{verbatim}
cd pygr/tests
python protest.py
\end{verbatim}
One can also run individual test files:
\begin{verbatim}
python protest.py pygrdata_test.py
\end{verbatim}

To run ALL tests, including very time-consuming and resource-intensive
NLMSA build tests, tell it to include our ``megatests'':
\begin{verbatim}
python protest.py *_megatest.py
\end{verbatim}
Note: depending on what modules you have installed, and what data resources
you have available locally, some tests may be skipped; it will indicate
which tests were skipped.  Note that you must have the necessary input 
datasets to run some of the megatests.  For obvious reasons, we do not include
these massive datasets in the Pygr source code repository or install packages.
To run a specific megatest, first look at its source code to determine what
input data it requires; often this is obvious from the megatest file name.

\subsubsection{What is protest.py?}
For testing pygr.Data, it is helpful to use a testing framework
that enables each test to run in a separate process.  Since nose does
not allow this, we use a small script compatible with nose, that
runs the each test in a separate process (i.e. separate Python interpreter
session).

\subsection{Testing Approach}
\label{test-utils}
Given Pygr's focus on working with large datasets, testing has been a bit of a puzzle.  
We have always included an automated test suite with the source code package.  
However, we obviously can't include big datasets in the package, so how can 
we code our tests so that they will automatically run only those tests that 
are possible in the user's local environment and resources?  

Here's a summary of the testing approach we've developed to solve these challenges:
\begin{itemize}
\item Based on advice from Titus Brown, we've adopted \code{nose} as the testing
framework for testing Pygr.
\item Tests are included with the source code package
in \code{pygr/tests} in files with names like ``foo_test.py''.
\item We moved our existing unit tests to be run by nose.
\item We are adding our extensive tutorial examples as a wide-ranging set of functional tests.  The main goal is for the complete set of tests to be sensitive to failures in just about any area of pygr functionality.  So, for example, checking that the calculated percent identity for each result in a multigenome alignment query matches the stored correct result is very sensitive to many functions working right (correct construction of the alignment; correct query results; correct sequence interval retrieval; etc.)
\item Major data dependencies for individual tests will be managed with pygr.Data.  That is, each test setup() will simply try to obtain the data it needs by requesting named pygr.Data resources.  If required data are missing, the test will be skipped.  In my lab, we will keep a nightly test platform where ALL tests must pass (no skipping!).
\item Tests that take a long time or a lot of resources (e.g. 28 vertebrate genome alignment NLMSA build) will have the suffix "_megatest" (instead of the default suffix "_test"), and thus will be omitted by a normal nosetests run.  We will include all megatests on our nightly test platform.  
\end{itemize}

\subsubsection{How To Automatically Skip Tests?}
When resources necessary for running a specific test are missing
(such as a data file, or network access), the test should be automatically
skipped.  The pattern mandated by nose, which we follow, is that the 
\method{setup}() method should raise a nose.SkipTest exception to 
indicate that the test should be skipped.  E.g.
\begin{verbatim}
class PygrDownload_Test(object):
    def setup(self,**kwargs):
        try:
            s = SourceURL('http://www.doe-mbi.ucla.edu/~leec/test.gz')
        except socket.gaierror:
            raise nose.SkipTest
\end{verbatim}
The \class{SourceURL} class raises an exception if the network connection
fails, which in turn causes the test to be skipped (rather than being 
reported as "test failed".

\subsection{Testing Utilities}
\subsubsection{nosebase Module}
This module is included in the pygr/tests directory and provides the following
convenience classes and functions:
\begin{funcdesc}{TempDir}{}
  Creates a temporary directory, which will be automatically deleted
  when the \class{TempDir} instance is released.  Provides convenience methods
  \method{subfile}() and \method{copyFile}() used as follows:
\begin{verbatim}
tmp = TempDir()
path = str(tmp) # GET THE PATH TO THIS TEMPORARY DIRECTORY
targetPath = tmp.subfile('foo.test') # APPEND foo.test TO THE DIRECTORY PATH
newPath = tmp.copyFile('/some/path/to/some/file') # COPY TO TMP DIR AND RETURN ITS PATH
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{TempPygrData}{}
  Subclass of \class{TempDir}; creates a temporary directory,
  and forces pygr.Data to use it as an (initially
  empty) resource database.  Useful for testing pygr.Data functionality.  
\end{funcdesc}

\begin{funcdesc}{TempPygrDataMySQL}{dbname='test',args=''}
  Subclass of \class{TempPygrData}; creates a temporary table in MySQL,
  and forces pygr.Data to use it as an (initially
  empty) resource database.  Useful for testing pygr.Data functionality. 

  \var{dbname} should be the name of the MySQL database in which the temp
  table should be created.  \var{args}, if provided, should be a whitespace
  separated list of one or more of {\em host, user, password}.  NOTE: \var{args}
  MUST begin with a space, because it is simply appended to the tablename for
  calling the standard pygr.Data creation mechanism.
\end{funcdesc}

\begin{funcdesc}{skip_errors}{errors...}
  Decorator for making a setup function skip a specified list of errors
  (i.e. if one of those errors occurs, cause the associated test(s) to be skipped).
  For example, to protect against the user either lacking the necessary 
  database module, or access to a database server that can run the test:
\begin{verbatim}
    @skip_errors(ImportError)
    def setup(self):
        import MySQLdb
        try:
            Seq_Test.setup(self,**self.mysqlArgs)
        except MySQLdb.MySQLError:
            raise ImportError
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{PygrDataTextFile}{path,mode='r'}
  dict-like interface to a text file storage that is pygr.Data-smart,
  i.e. it uses pygr.Data.getResource.loads(), so data will be saved
  and loaded in terms of pygr.Data resource IDs, which will be loaded
  from pygr.Data in the usual way.  Intended for storing test results
  in a platform-independent text format.  \var{mode} can be ``r'' (read),
  ``w'' (write) or ``a'' (append).
  
  For example, to save a correct test result, just give it a unique
  name:
\begin{verbatim}
store = nosebase.PygrDataTextFile('tryme.pickle','w')
store['hbb1 fragment'] = ival # SAVE AS PICKLE IN TEXT FILE
\end{verbatim}
  The data is saved to the disk file immediately, each time you
  execute such a ``key assignment'' statement.
  You can now write a testcase that runs the appropriate steps to 
  construct \var{ival}, and check whether your test result matches the correct 
  answer you previously stored:
\begin{verbatim}
store = nosebase.PygrDataTextFile('tryme.pickle')
saved = store['hbb1 fragment']
assert ival == saved, 'seq ival should match stored result'
\end{verbatim}
  Note: when you open a \class{PygrDataTextFile} in read or append mode, the
  file is read into memory as text.  Only when you request a specific key name
  will it attempt to obtain the pygr.Data resource associated with that key name.

  By convention, our correct test results are stored in \code{pygr/tests/results}.
  So a convenient way to save a new test result (which you have verified is
  correct, and now wish to store), is to append it to an existing 
  \class{PygrDataTextFile}:
\begin{verbatim}
store = nosebase.PygrDataTextFile('results/seqdb1.pickle','a')
store['hbb1 fragment'] = ival # SAVE AS PICKLE IN TEXT FILE
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{TestXMLRPCServer}{*pygrDataNames}
  create an XMLRPC server loaded with the pygr.Data resources specified
  by the arguments provided as \var{pygrDataNames}.  Note: the XMLRPC server
  runs in a separate process (see the script \code{pygrdata_server.py})
  launched using \method{os.system}, to simulate
  real client-server usage patterns.  Note: this class will first attempt to
  load the specified pygr.Data resources (using your PYGRDATAPATH as usual)
  to ensure that they exist, before launching the XMLRPC server.  If a pygr.Data
  resource cannot be found, \class{KeyError} will be raised.

  To access the XMLRPC server
  using pygr.Data, call the object's \method{access_server} method to receive a reference
  to pygr.Data that will load data only from the XMLRPC server.  To shut down the
  XMLRPC server, call the object's \method{close} method.
\begin{verbatim}
self.server = nosebase.TestXMLRPCServer('Bio.Seq.Swissprot.sp42')
pygrData = self.server.access_server()
sp = pygrData.Bio.Seq.Swissprot.sp42() # TRY TO ACCESS FROM XMLRPC SERVER
self.server.close() # SHUT DOWN THE XMLRPC SERVER
\end{verbatim}
\end{funcdesc}


\subsubsection{protest: single-test per process}
We found that trying to use nose to test pygr.Data was not ideal.  
The problem is that what we really want to test is that pygr.Data works persistently 
between separate python interpreter sessions, whereas nose forces ALL of the tests 
to be performed within a single session.  I found myself wasting a lot of time 
trying to figure out why a particular nose test did not work, 
rather than trying to find / figure out potential pygr bugs.  
I therefore wrote a short script (protest.py) that performs the 
same function as nose, but runs each test in a separate Python 
interpreter session.   

To run all tests:

\begin{verbatim}
python protest.py
\end{verbatim}
To tell it to run tests in one particular file:
\begin{verbatim}
python protest.py pygrdata_test.py
\end{verbatim}

\section{C Library for NestedList: libnclist}
\label{intervaldb-doc}

The C library (libnclist) for NestedList storage and query
is actually included in the pygr source code package,
however we have not yet added documentation on its C interfaces.
It is straightforward to compile intervaldb.c as a library, 
and link that to your own C program, following the examples in 
our Python interface code (cnestedlist.pyx) to see how to call the C functions.
You can see extensive examples of how to build or query using these intervaldb functions, by looking at the \class{IntervalDBIterator} and \class{IntervalDB} classes (in-memory nested list) or \class{IntervalFileDBIterator} and \class{IntervalFileDB} classes (on disk) in the cnestedlist.pyx file.  This code directly calls the intervaldb C functions so you can see exactly how everything is done (the pyx suffix signifies that this is Pyrex code, i.e. for providing a python interface to C functionality).  It is not a very complicated process: basically, a few steps:

\subsubsection{To build the C library}
To build libnclist, just cd to the pygr/pygr source directory and type \code{make}.
The current version of the Makefile builds a statically linked library (libnclist.a).  
To build a shared library on your platform, just modify the compilation flags
in the Makefile.

\subsubsection{To build a nested list database}
\begin{itemize}
\item 1. load a bunch of 1:1 interval:interval pairs into an array of IntervalMap data structures, whose start,end members are your source coordinate system, and target_id gives an integer ID of the aligned sequence, and target_start,target_end are coordinates in the aligned sequence.  As you can see in the IntervalDB constructor, there is a read_intervals() convenience function that will load this for you from a text file.  The save_tuples() function shows a trivial example of how to store the data yourself...

\item 2. call build_nested_list() or build_nested_list_inplace() on the array.  This actually builds the nested list in memory.  The _inplace variant uses less memory (algorithm described in detail in the paper).

\item 3. if you wish to store the nested list to on-disk index files (for querying from disk rather than in-memory), call write_binary_files() with the desired filename.  Note that multiple files will be saved by adding different suffixes to this filename.
\end{itemize}

\subsubsection{To query a nested list database stored in-memory}
\begin{itemize}
\item 1. allocate an iterator using interval_iterator_alloc() and call find_intervals() to do the query.  See IntervalDB.find_overlap_list() for a detailed example.
\item 2. call free_interval_iterator() to free the iterator.
\end{itemize}

\subsubsection{To query a nested list database stored on-disk}
\begin{itemize}
\item 1. call read_binary_files() to get a data structure describing the size values, sublist structure etc.  Note that this does not load the nested list database into memory, it just loads a small amount of information for efficiently accessing its indexes.
\item 2. allocate an iterator as usual, and call find_file_intervals() to do the query.  See IntervalFileDB.find_overlap_list() for detailed example.
\item 3. call free_interval_iterator() as usual.
\end{itemize}

I also suggest you start by looking at intervaldb.c, which has build_nested_list() functions, query functions for both in-memory and on-disk nested list databases (find_intervals() and find_file_intervals() respectively), and reading / writing functions for the binary index (on-disk nested list), read_binary_files() and write_binary_files().

\subsubsection{Important Caveats}
Note that the Python alignment class (NLMSA) built on top of intervaldb can handle much larger alignments than can be built in memory, because it knows how to split up an alignment into separate coordinate systems that can each be built separately.  At this time, intervaldb.c is limited in the size of nested list it can build, by the total amount of memory you can allocate.  This only affects the build phase, obviously, not the on-disk query phase.  We haven't gotten around to implementing a pure on-disk build algorithm, which would obviously be slower, but would eliminate this memory size build limitation.

\section{Pygr Developer's Guide}
\label{api-doc}

Pygr attempts to provide a highly Pythonic interface to bioinformatics datatypes
such as sequence, alignments and databases.  This document explains the interfaces
for how different parts of Pygr fit together, for developers seeking to extend
Pygr's capabilities.  Overall, Pygr follows several principles:
\begin{itemize}
\item Pygr is Python + Databases: I want to unify the power and generality of
Python's data models with the scalable principles of schema and data integrity
that the database field has developed.  I will explain the database part of
this in a moment; first, let's examine the Python data models that are central
to Pygr:

\item dictionary interfaces: Dictionaries (i.e. the Mapping Protocol) are
the core of the Python language.  In Pygr, we use dictionary interfaces to 
represent database tables, mappings and graphs.  In relational databases
(and indeed any kind of federated database architecture where data from one
dataset need to be able to refer to specific items in another dataset),
each item in a database ordinarily has a "primary key" that has a unique
value for each item.  Python dictionaries mirror this behavior by associating
each (unique) key value with a specific object.  Thus dictionaries are a
natural interface to external databases.  However, the dictionary interface
is also useful for representing entity-relationships, which can be one-to-one
(i.e. \code{y = m[x]}), many-to-many (i.e. \code{for y in m[x]}), etc.  Pygr
graph interfaces are simply two-level mappings (i.e. \code{m[x][y] = edge}).

\item sequence interfaces: One of the main features of sequences is that they
are sliceable using integer number ranges (e.g. \code{s[5:10]}).  Pygr uses
sequence interfaces for both bio-sequence objects, intervals (slices) and 
annotations.

\item pickling: Python included a powerful, elegant and general method for
object persistence from very early in the language's history.  pygr.Data simply
leverages this strong foundation to provide convenient data sharing of 
any picklable data.
\end{itemize}

\subsection{Dictionary Interfaces}
Dictionary interfaces play a key role in Pygr modularization, because they
provide a clean, general interface that Python programmers intuitively
understand, hiding the complexities of interacting with a backend database,
so that user code can work transparently with {\em any} database that
follows this interface.
\begin{itemize}
\item identifier: one of the key principles of database
design is that every item should have a unique identifier ("primary key").
Identifiers are always basic data types such as integer or string, allowing
them to act as a portable system of reference for different datasets to keep
persistent references to each other (e.g. the identifier can be transmitted  
or stored by a relational database server or XMLRPC service)
The most common usage is \code{obj = m[k]} which returns the object associated
with key value \code{k}.

\item reverse mapping: one crucial operation is to obtain the identifier for
a given object from the database.  The database therefore provides an
inverse operator that returns the inverse mapping, i.e. 
\code{k = (}\textasciitilde\code{m)[obj]}.

\item Each database should have a \member{itemClass} attribute, that 
gives the class to be used for constructing an object to represent a 
database row (e.g. a sequence from a sequence database).  
This is used by pygr.Data to bind schema rules automatically
to items from a given database.  Similarly, the \member{itemSliceClass} attribute
gives the class to be used for constructing subslices of database row 
objects (e.g. sequence intervals).  Again, this is used for binding automatic
schema rules.

\item Caching: Pygr persistent database objects perform local caching of objects
from the database that have been requested by the user.  This guarantees that
different requests for the same identifier will return the same object.

\item iteration: Pygr follows a simple logic for allowing users to control
how data (objects) are loaded into memory.  Iterating over identifiers
(e.g. using __iter__() or keys()) simply retrieves the list of identifiers,
but does not load the set of objects from the database.  Thus, the object
associated with a specific identifier will only be loaded if the user
explicitly requests it, e.g via \code{m[k]}.  By contrast, iterating over
items or values from the database (e.g. using items() or values()) signals
to Pygr that the user intends to examine all objects in the database, so
the entire dataset is automatically retrieved using a single query (to 
maximize performance), and kept in local cache as usual.  In this case
a subsequent user request \code{m[k]} will simply be returned immediately
from local cache.  Note that it is possible to customize exactly what
columns from the database get actually loaded into memory
by writing row object subclasses.

\subsection{Databases and Schema}
First, let's define exactly what I mean by the word "database": a
set of data that share a common schema; in other words that each item
in the set participates in the same pattern of relations with other collections
of data.  Example: an {\em exon} always has the following attributes: 
\begin{itemize}
\item it is part of a {\em gene}; this is a many-to-one relation.
\item it is part of one or more {\em transcripts}; this is a many-to-many relation.
\item it maps to a specific {\em interval on a genomic sequence};
this is a one-to-one relation.
\item it can be connected to other exons by one or more directed edges, called
{\em splices} or {\em introns}.  This is a many-to-many graph relation
(because it also involves edge-relation objects).
\end{itemize}
Thus we can think of a database as a set of things that are functionally
equivalent in terms of having the same relations.

Second, let's define what I mean by the word "schema": first, the
list of {\em mappings} that items in a database participate in; second,
the {\em bindings} that attach these relations to items in this database.
A mapping is characterized by its {\em source} database, its {\em target}
database, and optionally its {\em edge} database whose items represent the
actual individual relations between a source-target pair of items.  In the example
above, the {\em splice} objects functioned as such "edge relation" objects.
A binding is typically an attribute name: i.e. for each item in the database,
that attribute name will yield the target that this item maps to according
to the mapping that is bound as this attribute.  For example, an exon object
might have a "gene" attribute that would yield the gene object that this 
exon "is part of".

\subsubsection{Subclass Binding}
Python has one "old-style" mechanism for customizing attribute access
(\code{__getattr__()}), and a "new-style" mechanism, called "descriptors".
Descriptors are modular -- each attribute is handled by a separate object
bound to the class -- whereas __getattr__() is not (a single super-function
handles all custom attribute requests... even worse, __setattr__() intercepts
ALL write requests for the object).  Pygr uses descriptors to implement
schema binding.  This has several pieces
\begin{itemize}
\item Subclassing: descriptors are bound to the {\em class}, not the instance
object.  So if we want to bind descriptors for a specific object, we need to 
create a subclass.  Pygr.classutil.get_bound_subclass() does this for you.
The current class is just subclassed, and any desired descriptors are bound
to the subclass.
\item _init_subclass: if provided by the parent class, this classmethod will be called when
the subclass is created, to let it do whatever it needs to initialize itself
and its relation with its target database, such as...
\item Binding: descriptors for all the relations we want are bound as
attributes to the subclass.  This makes them appear on all instances of this
subclass.
\item Pygr.Data uses this for automatic schema binding.  SQLTable uses this 
for efficient attribute access on TupleO (values stored locally as a tuple)
and SQLRow (all requests relayed as queries to the back-end database).
\item The subclass pickles as its parent class.
\end{itemize}

\subsection{Persistence}
Python built in a clean, modular system for persistence from very early in its
history: pickling.  Pygr.Data is built on pickling.  To make your classes
picklable, you need to follow some simple guidelines.  Your class will fall
in one of several categories:
\begin{itemize}
\item Simple: if the information needed to "resurrect" your object from storage
is nothing more than its attributes, and those attributes are picklable, you
don't need to do anything.  Pickling will by default just pickle all the 
attributes, and restore them during unpickling.

\item State: if your object needs control over what gets saved during pickling,
it should define a \method{__getstate__()} method that returns just the
data you want saved as the object's "state".  E.g. a database object might have a
cursor object as an attribute, which can't be pickled.  Your db object must
save "state information" sufficient for it to re-connect to the database server
upon unpickling.  Pygr.classutil.standard_getstate() and standard_setstate()
provide default Pygr behaviors (_pickleAttrs attribute controls list of attributes
to pickle).

\item Total control: if your object needs to determine what class it should
become at the moment of unpickling, you need to provide a \method{__reduce__}()
method.  NB: this is always needed if you subclass a built-in class like \class{dict}.
\end{itemize}

\subsection{Examples}
\subsubsection{Example: SQLTable}
A very common usage is to employ a dictionary interface to a relational
database table.  In this case the key value must be a valid identifier
in the database (primary key); a Python object representing that row in the database
will be returned.  The class to be used for constructing the "row object"
is controlled by setting the \member{itemClass} attribute.  The default
row class (TupleO) simply provides attributes that mirror the column names
in the database.
\begin{verbatim}
seq_region = sqlgraph.SQLTable('homo_sapiens_core_47_36i.seq_region',
                               cursor) 
\end{verbatim}
We can then request information about a specific sequence region, e.g.
\begin{verbatim}
sr = seq_region[143909]
print sr.name, sr.coord_system_id
\end{verbatim}

As a more sophisticated example, we can force rows from a specific table
to be interpreted as sequence objects:
\begin{verbatim}
class EnsemblDNA(seqdb.DNASQLSequence):
    def __len__(self): # just speed optimization
        return self._select('length(sequence)') # SQL SELECT expression
dna = sqlgraph.SQLTable('homo_sapiens_core_47_36i.dna', cursor, 
                        itemClass=EnsemblDNA, attrAlias=dict(seq='sequence'))
s = dna [143909] # get this sequence object
print len(s) # 41877
print str(s[:10]) # CACCCTGCCC
\end{verbatim}
Note the use of the \var{attrAlias} to provide a dictionary for remapping
the actual column names used in the Ensembl database ("sequence") to the
canonical name expected by seqdb.DNASQLSequence ("seq").  Note also how
we introduce a custom method for calculating the sequence length entirely on
the server side, to avoid Pygr having to retrieve the sequence string just to 
calculate its length.


\subsubsection{Example: PrefixUnionDict}
Multigenome alignments pose a problem: instead of making references to 
a set of sequences from a single database, they combine references to many
different databases each representing one genome.  How can this be handled
within the dictionary interface?  Simple: UCSC adds a prefix (representing the
"name" of the genome database to each sequence identifier, e.g. "hg18.chr1" is
sequence identifier "chr1" in database "hg18".  This can be considered an
identifier in a new "database" that is itself just a union of all the databases
that are included in the alignment.  Its job is to accept strings like "hg18.chr1"
as keys, then request the right identifier ("chr1") from the right database (hg18)
and return the resulting sequence object.
We construct it by supplying a dictionary of string prefixes to associate
with each sequence database as follows:
\begin{verbatim}
db = PrefixUnionDict({'hg18':hg18, 'mm7':mm7})
\end{verbatim}
where \code{hg18} is itself a sequence database that accepts string keys
(like "chr1") and returns the correspond sequence object.  Then we can
do things like:
\begin{verbatim}
s = db['hg18.chr1']
\end{verbatim}
Note that we will get different identifiers for s depending on whether
we ask db or hg18: \textasciitilde db[s] gives "hg18.chr1" whereas
\textasciitilde hg18[s] just gives "chr1", as it should.
\end{itemize}

\subsubsection{Example: Ensembl SeqRegion Database}
Ensembl's annotation schema promulgates a single identifier space
(seq_region_id) that can refer to any database listed in the 
coord_systems table.  This is analogous to the UCSC prefix union,
except that it uses an intermediary table seq_region that joins
seq_region_id to coord_system_id.  

Once again, Pygr provides a simple interface as a dictionary, which
is itself initialized with a dictionary of {coord_system_id:seqDB}
pairs.
\begin{verbatim}
seq_region = sqlgraph.SQLTable('homo_sapiens_core_47_36i.seq_region',
                               cursor) 
hg18 = pygr.Data.Bio.Seq.Genome.HUMAN.hg18() # human genome
srdb = SeqRegion(seq_region, {17:hg18}) # trivial example, only 1 genome
\end{verbatim}
Now we can request seq_region_id values from \code{srdb}, e.g. 
\code{chr1 = srdb[226034]} gets human chromosome 1.
Note that we will get different identifiers for chr1 depending on whether
we ask srdb or hg18: \textasciitilde srdb[chr1] gives 226034 whereas
\textasciitilde hg18[chr1] just gives "chr1", as it should.


\subsubsection{Annotation Databases}
Pygr treats annotation as an intersection between two types of data:
\begin{itemize}
\item Slice database: a dictionary that takes annotation ID as a key,
and returns an object that provides "slice information" for that annotation,
consisting of sequenceID, start coordinate, stop coordinate, and orientation.

\item Sequence database: a dictionary that takes a sequence ID as a key,
and returns a sliceable sequence object.
\end{itemize}

It should be emphasized that you can use {\em any} dictionary-like object
as either the slice database or sequence database.  Examples include
\begin{itemize}
\item Python built-in \class{dict}.

\item Python persistent dictionary such as \module{shelve}, \module{anydbm} etc.

\item Pygr classes that wrap such persistent dictionaries with convenient
features, e.g. \class{PicklableShelve} (which, unlike \module{shelve} can be
pickled, allowing it to be stored in pygr.Data), \class{IntShelve} (can accept
integer keys, rather than just string keys like \module{shelve} etc.).

\item Pygr sequence database such as \class{BlastDB}.

\item "wrapper" or "union" dictionary interfaces like \class{PrefixUnionDict}
or \class{SeqRegion}.

\item Pygr wrapper for a relational database table such as \class{SQLTable}.
\end{itemize}

The AnnotationDB class supports
simple "aliasing" of attribute names from the database to the canonical
names expected by AnnotationDB, by supplying an \var{attrAliasDict} dictionary
to its constructor.  See the \class{AnnotationDB} reference documentation for
details.  If more sophisticated transformations need to be performed
on the sliceDB data (e.g. mathematical functions), the best solution is to
use a custom class for the sliceDB.itemClass (i.e. the row object class), 
with descriptors (also known as properties) to compute the desired attribute
values.

For example, to convert Ensembl annotations to standard Python zero-offset
coordinates (from the Ensembl coordinate system that starts at 1), we
can define a Python descriptor class, then bind it as the \member{start}
attribute for the row class, which is then supplied as the \member{itemClass}.
\begin{verbatim}
class SeqRegionStartDescr(object):
    'converts seq_region_start to Python zero-offset coordinate system'
    def __get__(self, obj, objtype):
        return obj.seq_region_start - 1

from pygr import sqlgraph

class EnsemblRow(sqlgraph.TupleO): # TupleO is generic tuple with named attrs
    'use this for all Ensembl tables with seq_region_start'
    start = SeqRegionStartDescr()

exonSliceDB = sqlgraph.SQLTable('homo_sapiens_core_47_36i.exon',
                                cursor, itemClass=EnsemblRow)
\end{verbatim}



\end{document}
